{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm.notebook import tqdm\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import vertexai\n",
    "import extraction_prompts as prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PROJECT_ID = \"sharp-airway-408502\"\n",
    "LOCATION = \"us-central1\"\n",
    "GUEST = \"Leopold Aschenbrenner\"\n",
    "HOST = \"Dwarkesh Patel\"\n",
    "DATE = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "AIR_DATE = \"2024-06-04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "model = GenerativeModel(model_name=\"gemini-1.5-flash-001\")\n",
    "\n",
    "# Load transcript\n",
    "def load_transcript(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "transcript = load_transcript(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract transcript entries\n",
    "def extract_transcript_entries(transcript):\n",
    "    pattern = re.compile(r'(\\w+\\s\\w+)\\s(\\d{2}:\\d{2}:\\d{2})\\n([\\s\\S]+?)(?=\\n\\w+\\s\\w+\\s\\d{2}:\\d{2}:\\d{2}|$)')\n",
    "    matches = pattern.findall(transcript)\n",
    "    return [{'speaker': match[0], 'start_time': match[1], 'text': match[2].strip()} for match in matches]\n",
    "\n",
    "transcript_entries = extract_transcript_entries(transcript)\n",
    "df = pd.DataFrame(transcript_entries)\n",
    "\n",
    "# Identify speaker chunks\n",
    "def identify_speaker_chunks(df, second_speaker):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if df.loc[i, 'speaker'] == second_speaker:\n",
    "            preceding_beliefs, second_speaker_beliefs, following_beliefs = [], [], []\n",
    "            j = i - 1\n",
    "            while j >= 0 and df.loc[j, 'speaker'] != second_speaker:\n",
    "                preceding_beliefs.insert(0, f\"{df.loc[j, 'speaker']}\\n{df.loc[j, 'text']}\")\n",
    "                j -= 1\n",
    "            \n",
    "            while i < len(df) and df.loc[i, 'speaker'] == second_speaker:\n",
    "                second_speaker_beliefs.append(f\"{df.loc[i, 'speaker']}\\n{df.loc[i, 'text']}\")\n",
    "                i += 1\n",
    "            \n",
    "            while i < len(df) and df.loc[i, 'speaker'] != second_speaker:\n",
    "                following_beliefs.append(f\"{df.loc[i, 'speaker']}\\n{df.loc[i, 'text']}\")\n",
    "                i += 1\n",
    "            \n",
    "            chunk = \"\\n\".join(preceding_beliefs + second_speaker_beliefs + following_beliefs)\n",
    "            chunks.append(chunk)\n",
    "        else:\n",
    "            i += 1\n",
    "    return chunks\n",
    "\n",
    "chunks = identify_speaker_chunks(df, GUEST)\n",
    "extraction_df = pd.DataFrame(chunks, columns=['chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meta chunks\n",
    "def create_meta_chunks(extraction_df):\n",
    "    meta_chunks = []\n",
    "    for index in range(len(extraction_df)):\n",
    "        if index < 3:\n",
    "            meta_chunk = \"\\n\\n\".join(extraction_df['chunk'].iloc[:5])\n",
    "        elif index >= len(extraction_df) - 3:\n",
    "            meta_chunk = \"\\n\\n\".join(extraction_df['chunk'].iloc[-5:])\n",
    "        else:\n",
    "            meta_chunk = \"\\n\\n\".join(extraction_df['chunk'].iloc[index-2:index+3])\n",
    "        meta_chunks.append(meta_chunk)\n",
    "    return meta_chunks\n",
    "\n",
    "extraction_df['meta_chunk'] = create_meta_chunks(extraction_df)\n",
    "extraction_df = extraction_df.reset_index().rename(columns={'index': 'chunk_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated process_row function with enhanced error handling and debugging details\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=4, max=60))\n",
    "def process_row(row, prompt_template, system_message):\n",
    "    prompt = (\n",
    "        system_message\n",
    "        + prompt_template.replace(\"<chunk>\", row['chunk'])\n",
    "        .replace(\"<meta_chunk>\", row['meta_chunk'])\n",
    "        .replace(\"<belief>\", row.get('belief', ''))\n",
    "        .replace(\"<type>\", row.get('type', ''))\n",
    "        .replace(\"<context>\", row.get('context', ''))\n",
    "        .replace(\"<justification>\", row.get('justification', ''))\n",
    "        .replace(\"<verification_focus>\", row.get('verification_focus', ''))\n",
    "        .replace(\"<guest>\", GUEST)\n",
    "        .replace(\"<host>\", HOST)\n",
    "    )\n",
    "    try:\n",
    "        response = model.generate_content(contents=prompt, generation_config={\"response_mime_type\": \"application/json\"})\n",
    "        return json.loads(response.text)\n",
    "    except json.JSONDecodeError:\n",
    "        display(Markdown(f\"JSON Decoding Error: Unable to decode the response as JSON. Response: {response.text}\\n\\nMeta Chunk:  \\n{row['meta_chunk']}\"))\n",
    "        raise ValueError(\"JSON decoding error, triggering retry\")\n",
    "    except ValueError as e:\n",
    "        display(Markdown(f\"Value Error: {e}\\n\\nResponse: {response.to_dict()}\\n\\nMeta Chunk:  \\n{row['meta_chunk']}\"))\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Unexpected Error: {e}\\n\\nMeta Chunk:  \\n{row['meta_chunk']}\"))\n",
    "        return None\n",
    "\n",
    "# Updated generate_responses function\n",
    "def generate_responses(df, prompt_template, system_message):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = list(tqdm(executor.map(lambda row: process_row(row, prompt_template, system_message), df.to_dict('records')), total=len(df)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format system message\n",
    "system_message = prompts.system_message.replace(\"<date>\", DATE).replace(\"<air_date>\", AIR_DATE).replace(\"<guest>\", GUEST).replace(\"<host>\", HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract beliefs\n",
    "# Process each chunk to extract beliefs using the belief extraction prompt\n",
    "extraction_df['extracted_beliefs'] = generate_responses(extraction_df, prompts.belief_extraction, system_message)\n",
    "# Explode the list of extracted beliefs into separate rows and reset the index to get a unique belief_id for each belief\n",
    "extraction_df = extraction_df.explode('extracted_beliefs').reset_index(drop=True).reset_index().rename(columns={'index': 'belief_id'})\n",
    "# Normalize the JSON structure of the extracted beliefs into a flat table\n",
    "extracted_df = pd.json_normalize(extraction_df['extracted_beliefs'])\n",
    "# Merge the normalized extracted beliefs back into the main DataFrame on belief_id\n",
    "extraction_df = pd.merge(extraction_df, extracted_df, left_on='belief_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(extraction_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process verification\n",
    "# Drop rows where 'belief' is missing and create a copy for verification processing\n",
    "temp_df = extraction_df.dropna(subset=['belief']).copy()\n",
    "# Process each belief to evaluate whether it needs verification using the verification evaluation prompt\n",
    "temp_df['verification_output'] = generate_responses(temp_df, prompts.verification_evaluation, system_message)\n",
    "# Normalize the JSON structure of the verification output into a flat table and drop the redundant 'belief' column\n",
    "verification_df = pd.json_normalize(temp_df['verification_output']).drop(columns=['belief'])\n",
    "# Merge the verification results back into the main DataFrame on belief_id\n",
    "extraction_df = pd.merge(temp_df, verification_df, left_on='belief_id', right_index=True)\n",
    "# Filter the DataFrame to keep only the beliefs that require verification and reset the index\n",
    "research_df = extraction_df[extraction_df['verify'] == True].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hypotheses\n",
    "# Process each belief that requires verification to generate hypotheses using the hypothesis generation prompt\n",
    "research_df['hypotheses_list'] = generate_responses(research_df, prompts.hypothesis_generation, system_message)\n",
    "# Explode the list of generated hypotheses into separate rows and reset the index to get a unique hypothesis_id for each hypothesis\n",
    "research_df = research_df.explode('hypotheses_list').reset_index(drop=True).reset_index().rename(columns={'index': 'hypothesis_id'})\n",
    "# Normalize the JSON structure of the generated hypotheses into a flat table\n",
    "hypotheses_df = pd.json_normalize(research_df['hypotheses_list'])\n",
    "# Merge the hypotheses back into the main DataFrame on hypothesis_id\n",
    "research_df = pd.merge(research_df, hypotheses_df, left_on='hypothesis_id', right_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
