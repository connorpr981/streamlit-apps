{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Markdown\n",
    "import concurrent.futures\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generate embedding for a given text using OpenAI API.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def safe_get_embedding(text):\n",
    "    \"\"\"Safely get embedding with error handling.\"\"\"\n",
    "    try:\n",
    "        return get_embedding(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding document: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_embeddings_multithreaded(documents):\n",
    "    \"\"\"Get embeddings for a list of documents using multithreading.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=25) as executor:\n",
    "        embeddings = list(tqdm(executor.map(safe_get_embedding, documents), total=len(documents)))\n",
    "    return [emb for emb in embeddings if emb is not None]\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_parquet('../extraction/outputs/dwarkesh_patel__leopold_aschenbrenner.parquet')\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "# Extract unique beliefs and hypotheses\n",
    "unique_beliefs_df = df[['belief', 'type', 'confidence']].drop_duplicates().reset_index(drop=True)\n",
    "unique_hypotheses_df = df[['hypothesis', 'explanation', 'potential_sources']].explode('potential_sources').drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Sample documents for BERTopic analysis\n",
    "documents = unique_hypotheses_df['potential_sources'].tolist()\n",
    "documents_sample = pd.Series(documents).tolist()\n",
    "\n",
    "# Embed the documents using multithreading\n",
    "embeddings = get_embeddings_multithreaded(documents_sample)\n",
    "# embeddings = pd.read_parquet('outputs/embeddings.parquet')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
