## Chunk 1

**Chunk:**

Dwarkesh Patel
Today I’m chatting with my friend Leopold Aschenbrenner. He grew up in Germany and graduated as valedictorian of Columbia when he was 19. After that, he had a very interesting gap year which we’ll talk about. Then, he was on the OpenAI superalignment team, may it rest in peace.

Now, with some anchor investments — from Patrick and John Collison, Daniel Gross, and Nat Friedman — he is launching an investment firm.

Leopold, you’re off to a slow start but life is long. I wouldn’t worry about it too much. You’ll make up for it in due time. Thanks for coming on the podcast.
Leopold Aschenbrenner
Thank you. I first discovered your podcast when your best episode had a couple of hundred views. It’s been amazing to follow your trajectory. It’s a delight to be on.
Dwarkesh Patel
In the Sholto and Trenton episode, I mentioned that a lot of the things I’ve learned about AI I’ve learned from talking with them. The third, and probably most significant, part of this triumvirate has been you. We’ll get all the stuff on the record now.

Here’s the first thing I want to get on the record. Tell me about the trillion-dollar cluster.

I should mention this for the context of the podcast. Today you’re releasing a series called Situational Awareness. We’re going to get into it. First question about that is, tell me about the trillion-dollar cluster.

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 2

**Chunk:**

Dwarkesh Patel
In the Sholto and Trenton episode, I mentioned that a lot of the things I’ve learned about AI I’ve learned from talking with them. The third, and probably most significant, part of this triumvirate has been you. We’ll get all the stuff on the record now.

Here’s the first thing I want to get on the record. Tell me about the trillion-dollar cluster.

I should mention this for the context of the podcast. Today you’re releasing a series called Situational Awareness. We’re going to get into it. First question about that is, tell me about the trillion-dollar cluster.
Leopold Aschenbrenner
Unlike most things that have recently come out of Silicon Valley, AI is an industrial process. The next model doesn’t just require some code. It’s building a giant new cluster. It’s building giant new power plants. Pretty soon, it’s going to involve building giant new fabs.

Since ChatGPT, this extraordinary techno-capital acceleration has been set into motion. Exactly a year ago today, Nvidia had their first blockbuster earnings call. It went up 25% after hours and everyone was like, "oh my God, AI is a thing." Within a year, Nvidia data center revenue has gone from a few billion a quarter to $25 billion a quarter and continues to go up. Big Tech capex is skyrocketing.

It’s funny. There’s this crazy scramble going on, but in some sense it’s just the continuation of straight lines on a graph. There’s this long-run trend of almost a decade of training compute for the largest AI systems growing by about half an order of magnitude, 0.5 OOMs a year.

Just play that forward. GPT-4 was reported to have finished pre-training in 2022. On SemiAnalysis, it was rumored to have a cluster size of about 25,000 A100s. That’s roughly a $500 million cluster. Very roughly, it’s 10 megawatts. 

Just play that forward half a year. By 2024, that’s a cluster that’s 100 MW and 100,000 H100 equivalents with costs in the billions.

Play it forward two more years. By 2026, that’s a gigawatt, the size of a large nuclear reactor. That’s like the power of the Hoover Dam. That costs tens of billions of dollars and requires a million H100 equivalents.

By 2028, that’s a cluster that’s ten GW. That’s more power than most US states. That’s 10 million H100 equivalents, costing hundreds of billions of dollars.

By 2030, you get the trillion-dollar cluster using 100 gigawatts, over 20% of US electricity production. That’s 100 million H100 equivalents.

That’s just the training cluster. There are more inference GPUs as well. Once there are products, most of them will be inference GPUs. US power production has barely grown for decades. Now we’re really in for a ride.
Dwarkesh Patel
When I had Zuck on the podcast, he was claiming not a plateau per se, but that AI progress would be bottlenecked by this constraint on energy. Specifically, he was like, "oh, gigawatt data centers, are we going to build another Three Gorges Dam or something?"

According to public reports, there are companies planning things on the scale of a 1 GW data center. With a 10 GW data center, who’s going to be able to build that? A 100 GW center is like a state project. Are you going to pump that into one physical data center? How is it going to be possible? What is Zuck missing?

**Extracted Belief:**

AI is an industrial process that requires more than just code. It necessitates the construction of new clusters, power plants, and potentially fabs.

**Context:**

Leopold Aschenbrenner distinguishes AI from other Silicon Valley innovations by emphasizing its industrial nature, requiring significant infrastructure development.

**Justification:**

The statement is based on the observed trend of AI requiring increasingly powerful computing resources, leading to the need for new clusters, power plants, and potentially fabrication facilities.

--------

## Chunk 3

**Chunk:**

Dwarkesh Patel
In the Sholto and Trenton episode, I mentioned that a lot of the things I’ve learned about AI I’ve learned from talking with them. The third, and probably most significant, part of this triumvirate has been you. We’ll get all the stuff on the record now.

Here’s the first thing I want to get on the record. Tell me about the trillion-dollar cluster.

I should mention this for the context of the podcast. Today you’re releasing a series called Situational Awareness. We’re going to get into it. First question about that is, tell me about the trillion-dollar cluster.
Leopold Aschenbrenner
Unlike most things that have recently come out of Silicon Valley, AI is an industrial process. The next model doesn’t just require some code. It’s building a giant new cluster. It’s building giant new power plants. Pretty soon, it’s going to involve building giant new fabs.

Since ChatGPT, this extraordinary techno-capital acceleration has been set into motion. Exactly a year ago today, Nvidia had their first blockbuster earnings call. It went up 25% after hours and everyone was like, "oh my God, AI is a thing." Within a year, Nvidia data center revenue has gone from a few billion a quarter to $25 billion a quarter and continues to go up. Big Tech capex is skyrocketing.

It’s funny. There’s this crazy scramble going on, but in some sense it’s just the continuation of straight lines on a graph. There’s this long-run trend of almost a decade of training compute for the largest AI systems growing by about half an order of magnitude, 0.5 OOMs a year.

Just play that forward. GPT-4 was reported to have finished pre-training in 2022. On SemiAnalysis, it was rumored to have a cluster size of about 25,000 A100s. That’s roughly a $500 million cluster. Very roughly, it’s 10 megawatts. 

Just play that forward half a year. By 2024, that’s a cluster that’s 100 MW and 100,000 H100 equivalents with costs in the billions.

Play it forward two more years. By 2026, that’s a gigawatt, the size of a large nuclear reactor. That’s like the power of the Hoover Dam. That costs tens of billions of dollars and requires a million H100 equivalents.

By 2028, that’s a cluster that’s ten GW. That’s more power than most US states. That’s 10 million H100 equivalents, costing hundreds of billions of dollars.

By 2030, you get the trillion-dollar cluster using 100 gigawatts, over 20% of US electricity production. That’s 100 million H100 equivalents.

That’s just the training cluster. There are more inference GPUs as well. Once there are products, most of them will be inference GPUs. US power production has barely grown for decades. Now we’re really in for a ride.
Dwarkesh Patel
When I had Zuck on the podcast, he was claiming not a plateau per se, but that AI progress would be bottlenecked by this constraint on energy. Specifically, he was like, "oh, gigawatt data centers, are we going to build another Three Gorges Dam or something?"

According to public reports, there are companies planning things on the scale of a 1 GW data center. With a 10 GW data center, who’s going to be able to build that? A 100 GW center is like a state project. Are you going to pump that into one physical data center? How is it going to be possible? What is Zuck missing?

**Extracted Belief:**

AI training compute for the largest systems has been growing at an exponential rate, approximately doubling every two years.

**Context:**

Leopold Aschenbrenner describes the rapid growth of AI computing resources by comparing it to a straight line on a graph, indicating a consistent doubling period.

**Justification:**

He refers to a decade-long trend of AI systems' training compute growing by 0.5 OOMs a year, which translates to a doubling every two years.

--------

## Chunk 4

**Chunk:**

Dwarkesh Patel
In the Sholto and Trenton episode, I mentioned that a lot of the things I’ve learned about AI I’ve learned from talking with them. The third, and probably most significant, part of this triumvirate has been you. We’ll get all the stuff on the record now.

Here’s the first thing I want to get on the record. Tell me about the trillion-dollar cluster.

I should mention this for the context of the podcast. Today you’re releasing a series called Situational Awareness. We’re going to get into it. First question about that is, tell me about the trillion-dollar cluster.
Leopold Aschenbrenner
Unlike most things that have recently come out of Silicon Valley, AI is an industrial process. The next model doesn’t just require some code. It’s building a giant new cluster. It’s building giant new power plants. Pretty soon, it’s going to involve building giant new fabs.

Since ChatGPT, this extraordinary techno-capital acceleration has been set into motion. Exactly a year ago today, Nvidia had their first blockbuster earnings call. It went up 25% after hours and everyone was like, "oh my God, AI is a thing." Within a year, Nvidia data center revenue has gone from a few billion a quarter to $25 billion a quarter and continues to go up. Big Tech capex is skyrocketing.

It’s funny. There’s this crazy scramble going on, but in some sense it’s just the continuation of straight lines on a graph. There’s this long-run trend of almost a decade of training compute for the largest AI systems growing by about half an order of magnitude, 0.5 OOMs a year.

Just play that forward. GPT-4 was reported to have finished pre-training in 2022. On SemiAnalysis, it was rumored to have a cluster size of about 25,000 A100s. That’s roughly a $500 million cluster. Very roughly, it’s 10 megawatts. 

Just play that forward half a year. By 2024, that’s a cluster that’s 100 MW and 100,000 H100 equivalents with costs in the billions.

Play it forward two more years. By 2026, that’s a gigawatt, the size of a large nuclear reactor. That’s like the power of the Hoover Dam. That costs tens of billions of dollars and requires a million H100 equivalents.

By 2028, that’s a cluster that’s ten GW. That’s more power than most US states. That’s 10 million H100 equivalents, costing hundreds of billions of dollars.

By 2030, you get the trillion-dollar cluster using 100 gigawatts, over 20% of US electricity production. That’s 100 million H100 equivalents.

That’s just the training cluster. There are more inference GPUs as well. Once there are products, most of them will be inference GPUs. US power production has barely grown for decades. Now we’re really in for a ride.
Dwarkesh Patel
When I had Zuck on the podcast, he was claiming not a plateau per se, but that AI progress would be bottlenecked by this constraint on energy. Specifically, he was like, "oh, gigawatt data centers, are we going to build another Three Gorges Dam or something?"

According to public reports, there are companies planning things on the scale of a 1 GW data center. With a 10 GW data center, who’s going to be able to build that? A 100 GW center is like a state project. Are you going to pump that into one physical data center? How is it going to be possible? What is Zuck missing?

**Extracted Belief:**

The training cluster for GPT-4 used approximately 25,000 A100 GPUs, consuming about 10 megawatts of power and costing around $500 million.

**Context:**

Leopold Aschenbrenner provides estimates for the GPT-4 training cluster based on information from SemiAnalysis.

**Justification:**

He cites a rumor from SemiAnalysis about the cluster size of 25,000 A100s, which he uses to calculate the power consumption and cost.

--------

## Chunk 5

**Chunk:**

Dwarkesh Patel
In the Sholto and Trenton episode, I mentioned that a lot of the things I’ve learned about AI I’ve learned from talking with them. The third, and probably most significant, part of this triumvirate has been you. We’ll get all the stuff on the record now.

Here’s the first thing I want to get on the record. Tell me about the trillion-dollar cluster.

I should mention this for the context of the podcast. Today you’re releasing a series called Situational Awareness. We’re going to get into it. First question about that is, tell me about the trillion-dollar cluster.
Leopold Aschenbrenner
Unlike most things that have recently come out of Silicon Valley, AI is an industrial process. The next model doesn’t just require some code. It’s building a giant new cluster. It’s building giant new power plants. Pretty soon, it’s going to involve building giant new fabs.

Since ChatGPT, this extraordinary techno-capital acceleration has been set into motion. Exactly a year ago today, Nvidia had their first blockbuster earnings call. It went up 25% after hours and everyone was like, "oh my God, AI is a thing." Within a year, Nvidia data center revenue has gone from a few billion a quarter to $25 billion a quarter and continues to go up. Big Tech capex is skyrocketing.

It’s funny. There’s this crazy scramble going on, but in some sense it’s just the continuation of straight lines on a graph. There’s this long-run trend of almost a decade of training compute for the largest AI systems growing by about half an order of magnitude, 0.5 OOMs a year.

Just play that forward. GPT-4 was reported to have finished pre-training in 2022. On SemiAnalysis, it was rumored to have a cluster size of about 25,000 A100s. That’s roughly a $500 million cluster. Very roughly, it’s 10 megawatts. 

Just play that forward half a year. By 2024, that’s a cluster that’s 100 MW and 100,000 H100 equivalents with costs in the billions.

Play it forward two more years. By 2026, that’s a gigawatt, the size of a large nuclear reactor. That’s like the power of the Hoover Dam. That costs tens of billions of dollars and requires a million H100 equivalents.

By 2028, that’s a cluster that’s ten GW. That’s more power than most US states. That’s 10 million H100 equivalents, costing hundreds of billions of dollars.

By 2030, you get the trillion-dollar cluster using 100 gigawatts, over 20% of US electricity production. That’s 100 million H100 equivalents.

That’s just the training cluster. There are more inference GPUs as well. Once there are products, most of them will be inference GPUs. US power production has barely grown for decades. Now we’re really in for a ride.
Dwarkesh Patel
When I had Zuck on the podcast, he was claiming not a plateau per se, but that AI progress would be bottlenecked by this constraint on energy. Specifically, he was like, "oh, gigawatt data centers, are we going to build another Three Gorges Dam or something?"

According to public reports, there are companies planning things on the scale of a 1 GW data center. With a 10 GW data center, who’s going to be able to build that? A 100 GW center is like a state project. Are you going to pump that into one physical data center? How is it going to be possible? What is Zuck missing?

**Extracted Belief:**

The size and power requirements of AI training clusters will continue to grow exponentially, leading to clusters requiring hundreds of billions of dollars by 2028 and over 20% of US electricity production by 2030.

**Context:**

Leopold Aschenbrenner projects the future size and power requirements of AI training clusters based on the observed exponential growth trend.

**Justification:**

He extrapolates from the GPT-4 cluster size, doubling the capacity every two years, resulting in a trillion-dollar cluster by 2030 that would consume over 20% of US electricity production.

--------

## Chunk 6

**Chunk:**

Dwarkesh Patel
When I had Zuck on the podcast, he was claiming not a plateau per se, but that AI progress would be bottlenecked by this constraint on energy. Specifically, he was like, "oh, gigawatt data centers, are we going to build another Three Gorges Dam or something?"

According to public reports, there are companies planning things on the scale of a 1 GW data center. With a 10 GW data center, who’s going to be able to build that? A 100 GW center is like a state project. Are you going to pump that into one physical data center? How is it going to be possible? What is Zuck missing?
Leopold Aschenbrenner
Six months ago, 10 GW was the talk of the town. Now, people have moved on. 10 GW is happening. There’s The Information report on OpenAI and Microsoft planning a $100 billion cluster.
Dwarkesh Patel
Is that 1 GW? Or is that 10 GW?

**Extracted Belief:**

There is currently a project underway by OpenAI and Microsoft involving the construction of a data center cluster with an estimated cost of $100 billion.

**Context:**

Leopold Aschenbrenner is responding to a question from Dwarkesh Patel about the possibility of building very large data centers, with the example of a hypothetical 100 GW data center being discussed.

**Justification:**

The belief is based on Leopold Aschenbrenner's reference to 'The Information report' which suggests that OpenAI and Microsoft are planning a cluster worth $100 billion. No further details are provided about the report or its credibility.

--------

## Chunk 7

**Chunk:**

Dwarkesh Patel
When I had Zuck on the podcast, he was claiming not a plateau per se, but that AI progress would be bottlenecked by this constraint on energy. Specifically, he was like, "oh, gigawatt data centers, are we going to build another Three Gorges Dam or something?"

According to public reports, there are companies planning things on the scale of a 1 GW data center. With a 10 GW data center, who’s going to be able to build that? A 100 GW center is like a state project. Are you going to pump that into one physical data center? How is it going to be possible? What is Zuck missing?
Leopold Aschenbrenner
Six months ago, 10 GW was the talk of the town. Now, people have moved on. 10 GW is happening. There’s The Information report on OpenAI and Microsoft planning a $100 billion cluster.
Dwarkesh Patel
Is that 1 GW? Or is that 10 GW?

**Extracted Belief:**

The construction of a 10 GW data center is currently underway or in the planning stages.

**Context:**

Leopold Aschenbrenner is responding to a question from Dwarkesh Patel about the possibility of building very large data centers, with the example of a hypothetical 100 GW data center being discussed.

**Justification:**

The statement is based on Leopold Aschenbrenner's assertion that "10 GW is happening."

--------

## Chunk 8

**Chunk:**

Dwarkesh Patel
Is that 1 GW? Or is that 10 GW?
Leopold Aschenbrenner
I don’t know but if you try to map out how expensive the 10 GW cluster would be, that’s a couple of hundred billion. It’s sort of on that scale and they’re planning it. It’s not just my crazy take. AMD forecasted a $400 billion AI accelerator market by 2027. AI accelerators are only part of the expenditures.

We’re very much on track for a $1 trillion of total AI investment by 2027. The $1 trillion cluster will take a bit more acceleration. We saw how much ChatGPT unleashed. Every generation, the models are going to be crazy and shift the Overton window.

Then the revenue comes in. These are forward-looking investments. The question is, do they pay off? Let’s estimate the GPT-4 cluster at around $500 million. There’s a common mistake people make, saying it was $100 million for GPT-4. That’s just the rental price. If you’re building the biggest cluster, you have to build and pay for the whole cluster. You can’t just rent it for three months.
Dwarkesh Patel
Can’t you?

**Extracted Belief:**

The cost of a 10 GW AI training cluster would be a couple of hundred billion dollars.

**Context:**

Leopold Aschenbrenner is responding to a question about the scale of an AI training cluster, specifically, whether it would be 1 GW or 10 GW.

**Justification:**

This belief is based on Leopold Aschenbrenner's estimation of the cost of a 10 GW cluster, which he states would be in the range of "a couple of hundred billion dollars."

--------

## Chunk 9

**Chunk:**

Dwarkesh Patel
Is that 1 GW? Or is that 10 GW?
Leopold Aschenbrenner
I don’t know but if you try to map out how expensive the 10 GW cluster would be, that’s a couple of hundred billion. It’s sort of on that scale and they’re planning it. It’s not just my crazy take. AMD forecasted a $400 billion AI accelerator market by 2027. AI accelerators are only part of the expenditures.

We’re very much on track for a $1 trillion of total AI investment by 2027. The $1 trillion cluster will take a bit more acceleration. We saw how much ChatGPT unleashed. Every generation, the models are going to be crazy and shift the Overton window.

Then the revenue comes in. These are forward-looking investments. The question is, do they pay off? Let’s estimate the GPT-4 cluster at around $500 million. There’s a common mistake people make, saying it was $100 million for GPT-4. That’s just the rental price. If you’re building the biggest cluster, you have to build and pay for the whole cluster. You can’t just rent it for three months.
Dwarkesh Patel
Can’t you?

**Extracted Belief:**

AMD forecasted a $400 billion AI accelerator market by 2027.

**Context:**

Leopold Aschenbrenner is providing evidence to support his belief about the scale of AI investment, citing a forecast made by AMD.

**Justification:**

Leopold Aschenbrenner states that AMD has forecasted a $400 billion AI accelerator market by 2027, suggesting that his belief is grounded in industry projections.

--------

## Chunk 10

**Chunk:**

Dwarkesh Patel
Is that 1 GW? Or is that 10 GW?
Leopold Aschenbrenner
I don’t know but if you try to map out how expensive the 10 GW cluster would be, that’s a couple of hundred billion. It’s sort of on that scale and they’re planning it. It’s not just my crazy take. AMD forecasted a $400 billion AI accelerator market by 2027. AI accelerators are only part of the expenditures.

We’re very much on track for a $1 trillion of total AI investment by 2027. The $1 trillion cluster will take a bit more acceleration. We saw how much ChatGPT unleashed. Every generation, the models are going to be crazy and shift the Overton window.

Then the revenue comes in. These are forward-looking investments. The question is, do they pay off? Let’s estimate the GPT-4 cluster at around $500 million. There’s a common mistake people make, saying it was $100 million for GPT-4. That’s just the rental price. If you’re building the biggest cluster, you have to build and pay for the whole cluster. You can’t just rent it for three months.
Dwarkesh Patel
Can’t you?

**Extracted Belief:**

Total AI investment will reach $1 trillion by 2027.

**Context:**

Leopold Aschenbrenner is discussing the trajectory of AI investment, making a prediction about the total investment amount.

**Justification:**

He states, "We’re very much on track for a $1 trillion of total AI investment by 2027."

--------

## Chunk 11

**Chunk:**

Dwarkesh Patel
Is that 1 GW? Or is that 10 GW?
Leopold Aschenbrenner
I don’t know but if you try to map out how expensive the 10 GW cluster would be, that’s a couple of hundred billion. It’s sort of on that scale and they’re planning it. It’s not just my crazy take. AMD forecasted a $400 billion AI accelerator market by 2027. AI accelerators are only part of the expenditures.

We’re very much on track for a $1 trillion of total AI investment by 2027. The $1 trillion cluster will take a bit more acceleration. We saw how much ChatGPT unleashed. Every generation, the models are going to be crazy and shift the Overton window.

Then the revenue comes in. These are forward-looking investments. The question is, do they pay off? Let’s estimate the GPT-4 cluster at around $500 million. There’s a common mistake people make, saying it was $100 million for GPT-4. That’s just the rental price. If you’re building the biggest cluster, you have to build and pay for the whole cluster. You can’t just rent it for three months.
Dwarkesh Patel
Can’t you?

**Extracted Belief:**

The cost of the GPT-4 training cluster was around $500 million.

**Context:**

Leopold Aschenbrenner is addressing a common misconception about the cost of the GPT-4 training cluster, emphasizing the distinction between rental costs and the actual cost of building the cluster.

**Justification:**

He states, "Let’s estimate the GPT-4 cluster at around $500 million." This suggests he is basing his belief on an estimation of the cost based on his knowledge of the cluster's infrastructure.

--------

## Chunk 12

**Chunk:**

Dwarkesh Patel
Is that 1 GW? Or is that 10 GW?
Leopold Aschenbrenner
I don’t know but if you try to map out how expensive the 10 GW cluster would be, that’s a couple of hundred billion. It’s sort of on that scale and they’re planning it. It’s not just my crazy take. AMD forecasted a $400 billion AI accelerator market by 2027. AI accelerators are only part of the expenditures.

We’re very much on track for a $1 trillion of total AI investment by 2027. The $1 trillion cluster will take a bit more acceleration. We saw how much ChatGPT unleashed. Every generation, the models are going to be crazy and shift the Overton window.

Then the revenue comes in. These are forward-looking investments. The question is, do they pay off? Let’s estimate the GPT-4 cluster at around $500 million. There’s a common mistake people make, saying it was $100 million for GPT-4. That’s just the rental price. If you’re building the biggest cluster, you have to build and pay for the whole cluster. You can’t just rent it for three months.
Dwarkesh Patel
Can’t you?

**Extracted Belief:**

The cost of renting an AI training cluster for three months is significantly less than the cost of building and owning the entire cluster.

**Context:**

Leopold Aschenbrenner is explaining why the common perception that GPT-4 cost $100 million is inaccurate.

**Justification:**

He states that the $100 million figure represents a rental price, not the actual cost of building the cluster. This suggests that he is aware of the different cost structures involved in renting and owning AI clusters.

--------

## Chunk 13

**Chunk:**

Dwarkesh Patel
Can’t you?
Leopold Aschenbrenner
Once you’re trying to get into the hundreds of billions, you have to get to like $100 billion a year in revenue. This is where it gets really interesting for the big tech companies because their revenues are on the order of hundreds of billions.

$10 billion is fine. It’ll pay off the 2024 size training cluster. It’ll really be gangbusters with Big Tech when it costs $100 billion a year. The question is how feasible is $100 billion a year from AI revenue? It’s a lot more than right now. If you believe in the trajectory of AI systems as I do, it’s not that crazy.

There are like 300 million Microsoft Office subscribers. They have Copilot now. I don’t know what they’re selling it for. Suppose you sold some AI add-on for $100/month to a third of Microsoft Office subscribers. That’d be $100 billion right there. $100/month is a lot.
Dwarkesh Patel
That’s a lot for a third of Office subscribers.

**Extracted Belief:**

The trajectory of AI systems will continue to advance, resulting in significant economic impact.

**Context:**

Leopold Aschenbrenner is discussing the feasibility of generating $100 billion per year in AI revenue, and he bases his belief on the trajectory of AI advancements.

**Justification:**

Leopold Aschenbrenner states that he believes in the trajectory of AI systems, indicating that his belief is based on his understanding of the field and its ongoing development.

--------

## Chunk 14

**Chunk:**

Dwarkesh Patel
Can’t you?
Leopold Aschenbrenner
Once you’re trying to get into the hundreds of billions, you have to get to like $100 billion a year in revenue. This is where it gets really interesting for the big tech companies because their revenues are on the order of hundreds of billions.

$10 billion is fine. It’ll pay off the 2024 size training cluster. It’ll really be gangbusters with Big Tech when it costs $100 billion a year. The question is how feasible is $100 billion a year from AI revenue? It’s a lot more than right now. If you believe in the trajectory of AI systems as I do, it’s not that crazy.

There are like 300 million Microsoft Office subscribers. They have Copilot now. I don’t know what they’re selling it for. Suppose you sold some AI add-on for $100/month to a third of Microsoft Office subscribers. That’d be $100 billion right there. $100/month is a lot.
Dwarkesh Patel
That’s a lot for a third of Office subscribers.

**Extracted Belief:**

A $100/month subscription fee for an AI add-on to Microsoft Office, purchased by one-third of the subscriber base, could generate $100 billion in revenue.

**Context:**

Leopold Aschenbrenner is making a hypothetical calculation to illustrate how AI revenue could reach $100 billion.

**Justification:**

Leopold Aschenbrenner's calculation is based on a simple multiplication: 300 million Microsoft Office subscribers x 1/3 x $100/month x 12 months = $100 billion.

--------

## Chunk 15

**Chunk:**

Dwarkesh Patel
That’s a lot for a third of Office subscribers.
Leopold Aschenbrenner
For the average knowledge worker, it’s a few hours of productivity a month. You have to be expecting pretty lame AI progress to not hit a few hours of productivity a month.
Dwarkesh Patel
Sure, let’s assume all this. What happens in the next few years? What can the AI trained on the 1 GW data center do? What about the one on the 10 GW data center? Just map out the next few years of AI progress for me.

**Extracted Belief:**

AI systems will improve to the point where they can boost the productivity of an average knowledge worker by a few hours per month.

**Context:**

Leopold Aschenbrenner was arguing for the potential economic viability of large AI investments by projecting the impact on knowledge workers' productivity.

**Justification:**

He stated that "For the average knowledge worker, it’s a few hours of productivity a month." This belief is based on the observed trajectory of AI progress and the potential for AI to automate tasks that currently require human effort.

--------

## Chunk 16

**Chunk:**

Dwarkesh Patel
Sure, let’s assume all this. What happens in the next few years? What can the AI trained on the 1 GW data center do? What about the one on the 10 GW data center? Just map out the next few years of AI progress for me.
Leopold Aschenbrenner
The 10 GW range is my best guess for when you get true AGI. Compute is actually overrated. We’ll talk about that.

By 2025-2026, we’re going to get models that are basically smarter than most college graduates. A lot of the economic usefulness depends on unhobbling. The models are smart but limited. There are chatbots and then there are things like being able to use a computer and doing agentic long-horizon tasks.

By 2027-2028, it’ll get as smart as the smartest experts. The unhobbling trajectory points to it becoming much more like an agent than a chatbot. It’ll almost be like a drop-in remote worker.

This is the question around the economic returns. Intermediate AI systems could be really useful, but it takes a lot of schlep to integrate them. There’s a lot you could do with GPT-4 or GPT-4.5 in a business use case, but you really have to change your workflows to make them useful. It’s a very Tyler Cowen-esque take. It just takes a long time to diffuse. We’re in SF and so we miss that.

But in some sense, the way these systems want to be integrated is where you get this kind of sonic boom. Intermediate systems could have done it, but it would have taken schlep. Before you do the schlep to integrate them, you’ll get much more powerful systems that are unhobbled.

They’re agents, drop-in remote workers. You’re interacting with them like coworkers. You can do Zoom calls and Slack with them. You can ask them to do a project and they go off and write a first draft, get feedback, run tests on their code, and come back. Then you can tell them more things. That’ll be much easier to integrate.

You might need a bit of overkill to make the transition easy and harvest the gains.
Dwarkesh Patel
What do you mean by overkill? Overkill on model capabilities?

**Extracted Belief:**

True artificial general intelligence (AGI) will be achieved when AI systems have access to around 10 gigawatts (GW) of compute power.

**Context:**

Leopold Aschenbrenner is discussing the timeline for AI progress and suggests that AGI will be achieved when AI systems have access to a certain level of compute power, which he estimates to be 10 GW.

**Justification:**

Aschenbrenner's belief is based on his personal experience and understanding of the relationship between computational power and AI capabilities.

--------

## Chunk 17

**Chunk:**

Dwarkesh Patel
Sure, let’s assume all this. What happens in the next few years? What can the AI trained on the 1 GW data center do? What about the one on the 10 GW data center? Just map out the next few years of AI progress for me.
Leopold Aschenbrenner
The 10 GW range is my best guess for when you get true AGI. Compute is actually overrated. We’ll talk about that.

By 2025-2026, we’re going to get models that are basically smarter than most college graduates. A lot of the economic usefulness depends on unhobbling. The models are smart but limited. There are chatbots and then there are things like being able to use a computer and doing agentic long-horizon tasks.

By 2027-2028, it’ll get as smart as the smartest experts. The unhobbling trajectory points to it becoming much more like an agent than a chatbot. It’ll almost be like a drop-in remote worker.

This is the question around the economic returns. Intermediate AI systems could be really useful, but it takes a lot of schlep to integrate them. There’s a lot you could do with GPT-4 or GPT-4.5 in a business use case, but you really have to change your workflows to make them useful. It’s a very Tyler Cowen-esque take. It just takes a long time to diffuse. We’re in SF and so we miss that.

But in some sense, the way these systems want to be integrated is where you get this kind of sonic boom. Intermediate systems could have done it, but it would have taken schlep. Before you do the schlep to integrate them, you’ll get much more powerful systems that are unhobbled.

They’re agents, drop-in remote workers. You’re interacting with them like coworkers. You can do Zoom calls and Slack with them. You can ask them to do a project and they go off and write a first draft, get feedback, run tests on their code, and come back. Then you can tell them more things. That’ll be much easier to integrate.

You might need a bit of overkill to make the transition easy and harvest the gains.
Dwarkesh Patel
What do you mean by overkill? Overkill on model capabilities?

**Extracted Belief:**

By 2025-2026, AI models will reach a level of intelligence comparable to or exceeding that of most college graduates.

**Context:**

Aschenbrenner is predicting the future capabilities of AI models, stating that they will surpass the intelligence of most college graduates within a few years.

**Justification:**

This belief is based on the observed rapid advancements in AI capabilities, particularly in areas like natural language processing and problem-solving.

--------

## Chunk 18

**Chunk:**

Dwarkesh Patel
Sure, let’s assume all this. What happens in the next few years? What can the AI trained on the 1 GW data center do? What about the one on the 10 GW data center? Just map out the next few years of AI progress for me.
Leopold Aschenbrenner
The 10 GW range is my best guess for when you get true AGI. Compute is actually overrated. We’ll talk about that.

By 2025-2026, we’re going to get models that are basically smarter than most college graduates. A lot of the economic usefulness depends on unhobbling. The models are smart but limited. There are chatbots and then there are things like being able to use a computer and doing agentic long-horizon tasks.

By 2027-2028, it’ll get as smart as the smartest experts. The unhobbling trajectory points to it becoming much more like an agent than a chatbot. It’ll almost be like a drop-in remote worker.

This is the question around the economic returns. Intermediate AI systems could be really useful, but it takes a lot of schlep to integrate them. There’s a lot you could do with GPT-4 or GPT-4.5 in a business use case, but you really have to change your workflows to make them useful. It’s a very Tyler Cowen-esque take. It just takes a long time to diffuse. We’re in SF and so we miss that.

But in some sense, the way these systems want to be integrated is where you get this kind of sonic boom. Intermediate systems could have done it, but it would have taken schlep. Before you do the schlep to integrate them, you’ll get much more powerful systems that are unhobbled.

They’re agents, drop-in remote workers. You’re interacting with them like coworkers. You can do Zoom calls and Slack with them. You can ask them to do a project and they go off and write a first draft, get feedback, run tests on their code, and come back. Then you can tell them more things. That’ll be much easier to integrate.

You might need a bit of overkill to make the transition easy and harvest the gains.
Dwarkesh Patel
What do you mean by overkill? Overkill on model capabilities?

**Extracted Belief:**

AI models are currently limited in their ability to perform tasks that require complex reasoning, planning, and long-term goals.

**Context:**

Aschenbrenner discusses the limitations of current AI models, highlighting their inability to act as agents or complete complex tasks requiring long-term planning and goal-oriented behavior.

**Justification:**

This belief is derived from Aschenbrenner's observation of the capabilities of existing AI systems, such as chatbots, which are often limited in their ability to perform complex tasks requiring long-term planning.

--------

## Chunk 19

**Chunk:**

Dwarkesh Patel
Sure, let’s assume all this. What happens in the next few years? What can the AI trained on the 1 GW data center do? What about the one on the 10 GW data center? Just map out the next few years of AI progress for me.
Leopold Aschenbrenner
The 10 GW range is my best guess for when you get true AGI. Compute is actually overrated. We’ll talk about that.

By 2025-2026, we’re going to get models that are basically smarter than most college graduates. A lot of the economic usefulness depends on unhobbling. The models are smart but limited. There are chatbots and then there are things like being able to use a computer and doing agentic long-horizon tasks.

By 2027-2028, it’ll get as smart as the smartest experts. The unhobbling trajectory points to it becoming much more like an agent than a chatbot. It’ll almost be like a drop-in remote worker.

This is the question around the economic returns. Intermediate AI systems could be really useful, but it takes a lot of schlep to integrate them. There’s a lot you could do with GPT-4 or GPT-4.5 in a business use case, but you really have to change your workflows to make them useful. It’s a very Tyler Cowen-esque take. It just takes a long time to diffuse. We’re in SF and so we miss that.

But in some sense, the way these systems want to be integrated is where you get this kind of sonic boom. Intermediate systems could have done it, but it would have taken schlep. Before you do the schlep to integrate them, you’ll get much more powerful systems that are unhobbled.

They’re agents, drop-in remote workers. You’re interacting with them like coworkers. You can do Zoom calls and Slack with them. You can ask them to do a project and they go off and write a first draft, get feedback, run tests on their code, and come back. Then you can tell them more things. That’ll be much easier to integrate.

You might need a bit of overkill to make the transition easy and harvest the gains.
Dwarkesh Patel
What do you mean by overkill? Overkill on model capabilities?

**Extracted Belief:**

By 2027-2028, AI models will reach a level of intelligence comparable to the smartest experts.

**Context:**

Aschenbrenner is predicting that AI models will continue to improve and reach a level of intelligence comparable to top experts in various fields within a few years.

**Justification:**

His belief is based on the rapid pace of AI development and his expectation that AI models will continue to surpass human intelligence in specific domains.

--------

## Chunk 20

**Chunk:**

Dwarkesh Patel
Sure, let’s assume all this. What happens in the next few years? What can the AI trained on the 1 GW data center do? What about the one on the 10 GW data center? Just map out the next few years of AI progress for me.
Leopold Aschenbrenner
The 10 GW range is my best guess for when you get true AGI. Compute is actually overrated. We’ll talk about that.

By 2025-2026, we’re going to get models that are basically smarter than most college graduates. A lot of the economic usefulness depends on unhobbling. The models are smart but limited. There are chatbots and then there are things like being able to use a computer and doing agentic long-horizon tasks.

By 2027-2028, it’ll get as smart as the smartest experts. The unhobbling trajectory points to it becoming much more like an agent than a chatbot. It’ll almost be like a drop-in remote worker.

This is the question around the economic returns. Intermediate AI systems could be really useful, but it takes a lot of schlep to integrate them. There’s a lot you could do with GPT-4 or GPT-4.5 in a business use case, but you really have to change your workflows to make them useful. It’s a very Tyler Cowen-esque take. It just takes a long time to diffuse. We’re in SF and so we miss that.

But in some sense, the way these systems want to be integrated is where you get this kind of sonic boom. Intermediate systems could have done it, but it would have taken schlep. Before you do the schlep to integrate them, you’ll get much more powerful systems that are unhobbled.

They’re agents, drop-in remote workers. You’re interacting with them like coworkers. You can do Zoom calls and Slack with them. You can ask them to do a project and they go off and write a first draft, get feedback, run tests on their code, and come back. Then you can tell them more things. That’ll be much easier to integrate.

You might need a bit of overkill to make the transition easy and harvest the gains.
Dwarkesh Patel
What do you mean by overkill? Overkill on model capabilities?

**Extracted Belief:**

The economic value of AI systems is heavily dependent on their ability to perform tasks that are currently done by humans, such as planning and executing complex projects.

**Context:**

Aschenbrenner is discussing the potential economic impact of AI and argues that its value will be determined by its ability to automate tasks that require human cognitive abilities.

**Justification:**

This belief is based on the logical assumption that AI systems will be most valuable when they can replace human workers in tasks requiring cognitive abilities.

--------

## Chunk 21

**Chunk:**

Dwarkesh Patel
Sure, let’s assume all this. What happens in the next few years? What can the AI trained on the 1 GW data center do? What about the one on the 10 GW data center? Just map out the next few years of AI progress for me.
Leopold Aschenbrenner
The 10 GW range is my best guess for when you get true AGI. Compute is actually overrated. We’ll talk about that.

By 2025-2026, we’re going to get models that are basically smarter than most college graduates. A lot of the economic usefulness depends on unhobbling. The models are smart but limited. There are chatbots and then there are things like being able to use a computer and doing agentic long-horizon tasks.

By 2027-2028, it’ll get as smart as the smartest experts. The unhobbling trajectory points to it becoming much more like an agent than a chatbot. It’ll almost be like a drop-in remote worker.

This is the question around the economic returns. Intermediate AI systems could be really useful, but it takes a lot of schlep to integrate them. There’s a lot you could do with GPT-4 or GPT-4.5 in a business use case, but you really have to change your workflows to make them useful. It’s a very Tyler Cowen-esque take. It just takes a long time to diffuse. We’re in SF and so we miss that.

But in some sense, the way these systems want to be integrated is where you get this kind of sonic boom. Intermediate systems could have done it, but it would have taken schlep. Before you do the schlep to integrate them, you’ll get much more powerful systems that are unhobbled.

They’re agents, drop-in remote workers. You’re interacting with them like coworkers. You can do Zoom calls and Slack with them. You can ask them to do a project and they go off and write a first draft, get feedback, run tests on their code, and come back. Then you can tell them more things. That’ll be much easier to integrate.

You might need a bit of overkill to make the transition easy and harvest the gains.
Dwarkesh Patel
What do you mean by overkill? Overkill on model capabilities?

**Extracted Belief:**

Integrating intermediate AI systems into existing workflows can be challenging and time-consuming.

**Context:**

Aschenbrenner points out the difficulties in integrating AI systems into existing workflows, highlighting the need for significant changes in work processes and a considerable time investment.

**Justification:**

His belief is based on his observation that the adoption of AI systems often requires significant changes in how tasks are performed, which can lead to resistance and delay.

--------

## Chunk 22

**Chunk:**

Dwarkesh Patel
Sure, let’s assume all this. What happens in the next few years? What can the AI trained on the 1 GW data center do? What about the one on the 10 GW data center? Just map out the next few years of AI progress for me.
Leopold Aschenbrenner
The 10 GW range is my best guess for when you get true AGI. Compute is actually overrated. We’ll talk about that.

By 2025-2026, we’re going to get models that are basically smarter than most college graduates. A lot of the economic usefulness depends on unhobbling. The models are smart but limited. There are chatbots and then there are things like being able to use a computer and doing agentic long-horizon tasks.

By 2027-2028, it’ll get as smart as the smartest experts. The unhobbling trajectory points to it becoming much more like an agent than a chatbot. It’ll almost be like a drop-in remote worker.

This is the question around the economic returns. Intermediate AI systems could be really useful, but it takes a lot of schlep to integrate them. There’s a lot you could do with GPT-4 or GPT-4.5 in a business use case, but you really have to change your workflows to make them useful. It’s a very Tyler Cowen-esque take. It just takes a long time to diffuse. We’re in SF and so we miss that.

But in some sense, the way these systems want to be integrated is where you get this kind of sonic boom. Intermediate systems could have done it, but it would have taken schlep. Before you do the schlep to integrate them, you’ll get much more powerful systems that are unhobbled.

They’re agents, drop-in remote workers. You’re interacting with them like coworkers. You can do Zoom calls and Slack with them. You can ask them to do a project and they go off and write a first draft, get feedback, run tests on their code, and come back. Then you can tell them more things. That’ll be much easier to integrate.

You might need a bit of overkill to make the transition easy and harvest the gains.
Dwarkesh Patel
What do you mean by overkill? Overkill on model capabilities?

**Extracted Belief:**

More powerful and unhobbled AI systems will be more easily integrated into existing workflows and lead to a more significant economic impact.

**Context:**

Aschenbrenner believes that future AI systems will be easier to integrate and have a greater impact than current AI systems due to their advanced capabilities and ability to work more autonomously.

**Justification:**

He reasons that the greater capabilities of these AI systems will make them more adaptable to existing workflows, reducing the need for major changes and allowing for more efficient integration.

--------

## Chunk 23

**Chunk:**

Dwarkesh Patel
What do you mean by overkill? Overkill on model capabilities?
Leopold Aschenbrenner
Yeah, the intermediate models could do it but it would take a lot of schlep. The drop-in remote worker AGI can automate cognitive tasks. The intermediate models would have made the software engineer more productive. But will the software engineer adopt it?

With the 2027 model, you just don’t need the software engineer. You can interact with it like a software engineer, and it’ll do the work of a software engineer.
Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?

**Extracted Belief:**

Intermediate AI models, like GPT-4 or GPT-4.5, could be useful in business applications, but they require significant effort to integrate into existing workflows.

**Context:**

Leopold Aschenbrenner is discussing the potential benefits of intermediate AI models and the challenges associated with their integration into existing workflows.

**Justification:**

He states that 'it takes a lot of schlep to integrate them,' indicating that intermediate AI models require significant effort to implement effectively in real-world scenarios.

--------

## Chunk 24

**Chunk:**

Dwarkesh Patel
What do you mean by overkill? Overkill on model capabilities?
Leopold Aschenbrenner
Yeah, the intermediate models could do it but it would take a lot of schlep. The drop-in remote worker AGI can automate cognitive tasks. The intermediate models would have made the software engineer more productive. But will the software engineer adopt it?

With the 2027 model, you just don’t need the software engineer. You can interact with it like a software engineer, and it’ll do the work of a software engineer.
Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?

**Extracted Belief:**

By 2027, AI models will become sufficiently advanced to automate the work of software engineers, effectively replacing them in certain tasks.

**Context:**

Leopold Aschenbrenner is discussing the future of AI and its potential to replace human workers in specific roles, particularly software engineering.

**Justification:**

He states that 'With the 2027 model, you just don’t need the software engineer. You can interact with it like a software engineer, and it’ll do the work of a software engineer,' indicating his belief that AI will reach a level of capability to automate such tasks by 2027.

--------

## Chunk 25

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

Current AI models, such as GPT-4, have a significant limitation in their ability to perform extended reasoning or chain-of-thought processes due to their restricted computational budget at inference time.

**Context:**

Leopold Aschenbrenner explains that while GPT-4 is capable of chain-of-thought processes, it is limited to a few hundred tokens, which he compares to a human thinking for a few minutes. He believes that unlocking the potential for millions of tokens would significantly improve reasoning abilities.

**Justification:**

He references GPT-4's current ability to process a few hundred tokens and compares it to human thinking, suggesting a limitation in reasoning capacity.

--------

## Chunk 26

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

Increasing the computational budget for inference time could dramatically enhance the reasoning and problem-solving capabilities of AI models.

**Context:**

Leopold Aschenbrenner argues that increasing the computational budget for inference time would allow AI models to perform longer chains of reasoning and potentially unlock new capabilities.

**Justification:**

He compares increasing computational budget to a human being able to think for several months instead of a few minutes, implying that significant advancements in reasoning ability would be possible.

--------

## Chunk 27

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

Improving error correction and planning capabilities in AI models is essential for unlocking their full potential for reasoning and problem-solving.

**Context:**

He suggests that AI models need to be able to learn and implement error correction and planning techniques to improve their reasoning abilities.

**Justification:**

He argues that these abilities, currently lacking in existing models, are crucial for improving reasoning and problem-solving.

--------

## Chunk 28

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

There are two primary paths for developing AI agents: scaling models for reliability and improving their reasoning abilities through System 2 processes.

**Context:**

He proposes two paths for achieving AI agents, one based on scaling models for greater reliability and the other on unlocking System 2 thinking capabilities.

**Justification:**

He presents these two paths as distinct strategies for achieving AI agency.

--------

## Chunk 29

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

Developing System 2 reasoning abilities in AI models is crucial for unlocking their full potential for agentic behavior and unlocking the test time compute overhang.

**Context:**

Leopold Aschenbrenner argues that enabling AI models to learn and utilize System 2 reasoning is crucial for unlocking their full potential for agentic behavior.

**Justification:**

He connects System 2 reasoning with the ability to unlock the test time compute overhang, suggesting that unlocking this type of thinking is critical for achieving agentic capabilities.

--------

## Chunk 30

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

The development of System 2 thinking in AI models is not inherently difficult due to the abundance of pre-training data and the potential for efficient learning through reinforcement learning (RL).

**Context:**

He suggests that unlocking System 2 thinking in AI models might be a relatively straightforward process due to the availability of pre-training data and the effectiveness of RL.

**Justification:**

He draws on his own expertise in the field of AI to suggest that unlocking System 2 thinking might not be a significant obstacle.

--------

## Chunk 31

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

Pre-training has provided AI models with a significant advantage in representation learning, enabling them to learn models of the world and generalize well.

**Context:**

He highlights the importance of pre-training in enabling AI models to learn rich representations of the world, leading to better generalization capabilities.

**Justification:**

He references his expertise in AI and deep learning to emphasize the importance of pre-training for representation learning.

--------

## Chunk 32

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

Reinforcement learning with human feedback (RLHF) has been successful in improving the performance of AI models, particularly in chatbot applications.

**Context:**

He acknowledges the success of RLHF in improving AI models, especially in chatbot applications, but argues that this approach might not be sufficient for unlocking the full potential of AI agents.

**Justification:**

He references the success of RLHF in chatbot applications, drawing on his understanding of AI training methods.

--------

## Chunk 33

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

AI models are increasingly able to learn autonomously, similar to how humans transition from learning from teachers to learning independently.

**Context:**

He suggests that AI models are moving towards a stage of autonomous learning, where they can learn independently without extensive pre-training.

**Justification:**

He draws an analogy between the learning process of humans and AI models, highlighting the transition from teacher-directed to self-directed learning.

--------

## Chunk 34

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

AI models are capable of learning through in-context learning, which is more sample-efficient than pre-training.

**Context:**

He contrasts in-context learning with pre-training, suggesting that in-context learning is more efficient for AI models.

**Justification:**

He draws on his expertise in AI to highlight the efficiency of in-context learning compared to traditional pre-training.

--------

## Chunk 35

**Chunk:**

Dwarkesh Patel
The last episode I did was with John Schulman.

I was asking about this. We have these models that have come out in the last year and none seem to have significantly surpassed GPT-4, certainly not in an agentic way where they interact with you as a coworker. They’ll brag about a few extra points on MMLU. Even with GPT-4o, it’s cool they can talk like Scarlett Johansson (I guess not anymore) but it’s not like a coworker.

It makes sense why they’d be good at answering questions. They have data on how to complete Wikipedia text. Where is the equivalent training data to understand a Zoom call? Referring back to your point about a Slack conversation, how can it use context to figure out the cohesive project you’re working on? Where is that training data coming from?
Leopold Aschenbrenner
A key question for AI progress in the next few years is how hard it is to unlock the test time compute overhang. Right now, GPT-4 can do a few hundred tokens with chain-of-thought. That’s already a huge improvement. Before, answering a math question was just shotgun. If you tried to answer a math question by saying the first thing that comes to mind, you wouldn’t be very good.

GPT-4 thinks for a few hundred tokens. If I think at 100 tokens a minute, that’s like what GPT-4 does. It’s equivalent to me thinking for three minutes. Suppose GPT-4 could think for millions of tokens. That’s +4 OOMs on test time compute on one problem. It can’t do it now. It gets stuck. It writes some code. It can do a little bit of iterative debugging, but eventually gets stuck and can’t correct its errors.

There’s a big overhang. In other areas of ML, there’s a great paper on AlphaGo, where you can trade off train time and test time compute. If you can use 4 OOMs more test time compute, that’s almost like a 3.5x OOM bigger model.

Again, if it’s 100 tokens a minute, a few million tokens is a few months of working time. There’s a lot more you can do in a few months of working time than just getting an answer right now. The question is how hard is it to unlock that?

In the short timelines AI world, it’s not that hard. The reason it might not be that hard is that there are only a few extra tokens to learn. You need to learn things like error correction tokens where you’re like “ah, I made a mistake, let me think about that again.” You need to learn planning tokens where it’s like “I’m going to start by making a plan. Here’s my plan of attack. I’m going to write a draft and now I’m going to critique my draft and think about it.” These aren’t things that models can do now, but the question is how hard it is.

There are two paths to agents. When Sholto was on your podcast, he talked about scaling leading to more nines of reliability. That’s one path. The other path is the unhobbling path. It needs to learn this System 2 process. If it can learn that, it can use millions of tokens and think coherently.

Here’s an analogy. When you drive, you’re on autopilot most of the time. Sometimes you hit a weird construction zone or intersection. Sometimes my girlfriend is in the passenger seat and I’m like “ah, be quiet for a moment, I need to figure out what’s going on.”

You go from autopilot to System 2 and you’re thinking about how to do it. Scaling improves that System 1 autopilot. The brute force way to get to agents is improving that system. If you can get System 2 working, you can quickly jump to something more agentified and test time compute overhang is unlocked.
Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?

**Extracted Belief:**

Humans learn through a process of in-context learning, involving reflection and practice, which can be replicated in AI models through reinforcement learning.

**Context:**

He draws a parallel between human learning through reflection and practice and the potential of AI models to learn through reinforcement learning.

**Justification:**

He suggests that RL can mimic the human learning process by providing feedback on attempts to solve problems.

--------

## Chunk 36

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

Pre-training is a crucial advantage for models of general intelligence because it enables the model to learn incredibly rich representations.

**Context:**

Leopold Aschenbrenner explains the advantages of pre-training for AI models, highlighting that it allows them to learn rich representations.

**Justification:**

Aschenbrenner states that predicting the next token in pre-training allows models to learn rich representations, which are essential for generalization. He also mentions that pre-training provides a huge advantage for models of general intelligence.

--------

## Chunk 37

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

The magic of deep learning lies in representation learning properties.

**Context:**

Leopold Aschenbrenner explains the advantages of pre-training for AI models, emphasizing the importance of representation learning.

**Justification:**

Aschenbrenner highlights that the ability of deep learning models to learn models of the world, rather than just statistical artifacts, is a key aspect of their effectiveness.

--------

## Chunk 38

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

Pre-training allows models to learn models of the world, leading to improved generalization capabilities.

**Context:**

Leopold Aschenbrenner explains the advantages of pre-training for AI models, focusing on their ability to generalize.

**Justification:**

Aschenbrenner explains that pre-trained models can generalize better than those without pre-training because they learn models of the world, rather than just statistical artifacts.

--------

## Chunk 39

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

The unhobbling process from GPT-2 to GPT-4 was a significant improvement in chatbot capabilities.

**Context:**

Leopold Aschenbrenner discusses the unhobbling process from GPT-2 to GPT-4, emphasizing its positive impact on chatbot capabilities.

**Justification:**

Aschenbrenner states that the unhobbling from GPT-2 to GPT-4 resulted in a significant improvement in chatbot capabilities, taking the model from a basic level to a more capable chatbot.

--------

## Chunk 40

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

RLHF (Reinforcement Learning from Human Feedback) is a key factor in enhancing model capabilities, particularly in the transition from GPT-2 to GPT-4.

**Context:**

Leopold Aschenbrenner discusses the role of RLHF in the unhobbling process from GPT-2 to GPT-4.

**Justification:**

Aschenbrenner highlights the importance of RLHF in improving model capabilities, noting that it resulted in a significant improvement in human preference rating.

--------

## Chunk 41

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

RLHF significantly improves human preference rating for AI models, leading to a 100x improvement in model size compared to models without RLHF.

**Context:**

Leopold Aschenbrenner discusses the impact of RLHF on AI models, providing a quantitative comparison.

**Justification:**

Aschenbrenner states that RLHF resulted in a 100x model size win on human preference rating compared to non-RLHF models.

--------

## Chunk 42

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

RLHF enables AI models to perform simple chain-of-thought reasoning.

**Context:**

Leopold Aschenbrenner discusses the capabilities enabled by RLHF in AI models.

**Justification:**

Aschenbrenner mentions that RLHF allows models to perform simple chain-of-thought tasks.

--------

## Chunk 43

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

Robotics research lacks the advantage of pre-training that is available for language models.

**Context:**

Leopold Aschenbrenner contrasts the pre-training advantage in language models with the challenges in robotics.

**Justification:**

Aschenbrenner highlights that robotics research does not have the advantage of pre-training, requiring a different approach based on reinforcement learning.

--------

## Chunk 44

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

Robotics research requires a greater reliance on reinforcement learning due to the lack of pre-training.

**Context:**

Leopold Aschenbrenner explains the difference in approach between language models and robotics.

**Justification:**

Aschenbrenner notes that the absence of pre-training in robotics necessitates starting with reinforcement learning from the beginning.

--------

## Chunk 45

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

Humans undergo a process of pre-training in their formative years, gradually transitioning to independent learning.

**Context:**

Leopold Aschenbrenner draws a parallel between human learning and AI model development.

**Justification:**

Aschenbrenner compares the pre-training phase of AI models to the formative years of human learning, where individuals acquire knowledge before transitioning to independent learning.

--------

## Chunk 46

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

Humans develop the ability to learn independently in their teenage years.

**Context:**

Leopold Aschenbrenner discusses the transition to independent learning in humans.

**Justification:**

Aschenbrenner notes that humans typically start learning independently around high school, with this ability becoming more pronounced in college.

--------

## Chunk 47

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

AI models are just beginning to enter a regime where they can learn independently.

**Context:**

Leopold Aschenbrenner discusses the progress of AI models towards independent learning.

**Justification:**

Aschenbrenner compares AI models to humans, suggesting that models are just starting to develop the ability to learn independently, similar to how humans learn independently after their formative years.

--------

## Chunk 48

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

Deep learning advancements are often obvious in retrospect.

**Context:**

Leopold Aschenbrenner discusses the nature of deep learning progress.

**Justification:**

Aschenbrenner states that many deep learning advancements seem obvious once they are realized, indicating that the field is constantly evolving.

--------

## Chunk 49

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

There are a multitude of details that need to be addressed to achieve significant progress in AI.

**Context:**

Leopold Aschenbrenner discusses the complexity of AI development.

**Justification:**

Aschenbrenner acknowledges the complexity of AI development, suggesting that numerous details need to be addressed to make significant advancements.

--------

## Chunk 50

**Chunk:**

Dwarkesh Patel
What’s the reason to think this is an easy win? Is there some loss function that easily enables System 2 thinking? There aren’t many animals with System 2 thinking. It took a long time for evolution to give us System 2 thinking.

Pre-training has trillions of tokens of Internet text, I get that. You match that and get all of these free training capabilities. What’s the reason to think this is an easy unhobbling?
Leopold Aschenbrenner
First of all, pre-training is magical. It gave us a huge advantage for models of general intelligence because you can predict the next token. But there’s a common misconception. Predicting the next token lets the model learn incredibly rich representations. Representation learning properties are the magic of deep learning. Rather than just learning statistical artifacts, the models learn models of the world. That’s why they can generalize, because it learned the right representations.

When you train a model, you have this raw bundle of capabilities that’s useful. The unhobbling from GPT-2 to GPT-4 took this raw mass and RLHF’d it into a good chatbot. That was a huge win.

In the original InstructGPT paper, comparing RLHF vs. non-RLHF models it’s like a 100x model size win on human preference rating. It started to be able to do simple chain-of-thought and so on. But you still have this advantage of all these raw capabilities, and there’s still a huge amount you’re not doing with them.

This pre-training advantage is also the difference to robotics. People used to say it was a hardware problem. The hardware is getting solved, but you don’t have this huge advantage of bootstrapping with pre-training. You don’t have all this unsupervised learning you can do. You have to start right away with RL self-play.

The question is why RL and unhobbling might work. Bootstrapping is an advantage. Your Twitter bio is being pre-trained. You’re not being pre-trained anymore. You were pre-trained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started and by college, if you’re smart, you can teach yourself. Models are just starting to enter that regime.

It’s a little bit more scaling and then you figure out what goes on top. It won’t be trivial. A lot of deep learning seems obvious in retrospect. There’s some obvious cluster of ideas. There are some ideas that seem a little dumb but work. There are a lot of details you have to get right. We’re not going to get this next month. It’ll take a while to figure out.
Dwarkesh Patel
A while for you is like half a year.

**Extracted Belief:**

Significant progress in AI won't happen overnight, but will require time and effort to achieve.

**Context:**

Leopold Aschenbrenner discusses the timeline for achieving significant progress in AI.

**Justification:**

Aschenbrenner emphasizes that achieving significant AI advancements will take time, indicating that it won't occur within a short timeframe.

--------

## Chunk 51

**Chunk:**

Dwarkesh Patel
A while for you is like half a year.
Leopold Aschenbrenner
I don’t know, between six months and three years. But it's possible. It’s also very related to the issue of the data wall. Here’s one intuition on learning by yourself. Pre-training is kind of like the teacher lecturing to you and the words are flying by. You’re just getting a little bit from it.

That's not what you do when you learn by yourself. When you learn by yourself, say you're reading a dense math textbook, you're not just skimming through it once. Some wordcels just skim through and reread and reread the math textbook and they memorize.

What you do is you read a page, think about it, have some internal monologue going on, and have a conversation with a study buddy. You try a practice problem and fail a bunch of times. At some point it clicks, and you're like, "this made sense." Then you read a few more pages.

We've kind of bootstrapped our way to just starting to be able to do that now with models. The question is, can you use all this sort of self-play, synthetic data, RL to make that thing work. Right now, there's in-context learning, which is super sample efficient. In the Gemini paper, it just learns a language in-context. Pre-training, on the other hand, is not at all sample efficient.

What humans do is a kind of in-context learning. You read a book, think about it, until eventually it clicks. Then you somehow distill that back into the weights. In some sense, that's what RL is trying to do. RL is super finicky, but when it works it's kind of magical.

It's the best possible data for the model. It’s when you try a practice problem, fail, and at some point figure it out in a way that makes sense to you. That's the best possible data for you because it's the way you would have solved the problem, rather than just reading how somebody else solved the problem, which doesn't initially click.
Dwarkesh Patel
By the way, if that take sounds familiar it's because it was part of the question I asked John Schulman. It goes to illustrate the thing I said in the intro. A bunch of the things I've learned about AI comes from these dinners we do before the interviews with me, you, Sholto, and a couple of others. We’re like, “what should I ask John Schulman, what I should ask Dario.” 

Suppose this is the way things go and we get these unhobblings—

**Extracted Belief:**

Pre-training is not sample-efficient in AI models, meaning that it requires a large amount of data to achieve desired results.

**Context:**

Leopold Aschenbrenner explains the difference between pre-training and in-context learning, suggesting that pre-training is not as efficient as in-context learning when it comes to sample efficiency.

**Justification:**

Aschenbrenner uses the example of the Gemini paper, which demonstrates the effectiveness of in-context learning in language acquisition, to contrast it with the less efficient pre-training approach.

--------

## Chunk 52

**Chunk:**

Dwarkesh Patel
A while for you is like half a year.
Leopold Aschenbrenner
I don’t know, between six months and three years. But it's possible. It’s also very related to the issue of the data wall. Here’s one intuition on learning by yourself. Pre-training is kind of like the teacher lecturing to you and the words are flying by. You’re just getting a little bit from it.

That's not what you do when you learn by yourself. When you learn by yourself, say you're reading a dense math textbook, you're not just skimming through it once. Some wordcels just skim through and reread and reread the math textbook and they memorize.

What you do is you read a page, think about it, have some internal monologue going on, and have a conversation with a study buddy. You try a practice problem and fail a bunch of times. At some point it clicks, and you're like, "this made sense." Then you read a few more pages.

We've kind of bootstrapped our way to just starting to be able to do that now with models. The question is, can you use all this sort of self-play, synthetic data, RL to make that thing work. Right now, there's in-context learning, which is super sample efficient. In the Gemini paper, it just learns a language in-context. Pre-training, on the other hand, is not at all sample efficient.

What humans do is a kind of in-context learning. You read a book, think about it, until eventually it clicks. Then you somehow distill that back into the weights. In some sense, that's what RL is trying to do. RL is super finicky, but when it works it's kind of magical.

It's the best possible data for the model. It’s when you try a practice problem, fail, and at some point figure it out in a way that makes sense to you. That's the best possible data for you because it's the way you would have solved the problem, rather than just reading how somebody else solved the problem, which doesn't initially click.
Dwarkesh Patel
By the way, if that take sounds familiar it's because it was part of the question I asked John Schulman. It goes to illustrate the thing I said in the intro. A bunch of the things I've learned about AI comes from these dinners we do before the interviews with me, you, Sholto, and a couple of others. We’re like, “what should I ask John Schulman, what I should ask Dario.” 

Suppose this is the way things go and we get these unhobblings—

**Extracted Belief:**

Reinforcement learning (RL) is a powerful and potentially magical technique in AI, but it can be finicky and difficult to implement.

**Context:**

Aschenbrenner compares RL to human learning, suggesting that RL is similar to the process of learning through trial and error, which can be challenging but ultimately very effective.

**Justification:**

He mentions that RL is "super finicky" but "kind of magical" when it works, implying that it is both difficult and effective.

--------

## Chunk 53

**Chunk:**

Dwarkesh Patel
A while for you is like half a year.
Leopold Aschenbrenner
I don’t know, between six months and three years. But it's possible. It’s also very related to the issue of the data wall. Here’s one intuition on learning by yourself. Pre-training is kind of like the teacher lecturing to you and the words are flying by. You’re just getting a little bit from it.

That's not what you do when you learn by yourself. When you learn by yourself, say you're reading a dense math textbook, you're not just skimming through it once. Some wordcels just skim through and reread and reread the math textbook and they memorize.

What you do is you read a page, think about it, have some internal monologue going on, and have a conversation with a study buddy. You try a practice problem and fail a bunch of times. At some point it clicks, and you're like, "this made sense." Then you read a few more pages.

We've kind of bootstrapped our way to just starting to be able to do that now with models. The question is, can you use all this sort of self-play, synthetic data, RL to make that thing work. Right now, there's in-context learning, which is super sample efficient. In the Gemini paper, it just learns a language in-context. Pre-training, on the other hand, is not at all sample efficient.

What humans do is a kind of in-context learning. You read a book, think about it, until eventually it clicks. Then you somehow distill that back into the weights. In some sense, that's what RL is trying to do. RL is super finicky, but when it works it's kind of magical.

It's the best possible data for the model. It’s when you try a practice problem, fail, and at some point figure it out in a way that makes sense to you. That's the best possible data for you because it's the way you would have solved the problem, rather than just reading how somebody else solved the problem, which doesn't initially click.
Dwarkesh Patel
By the way, if that take sounds familiar it's because it was part of the question I asked John Schulman. It goes to illustrate the thing I said in the intro. A bunch of the things I've learned about AI comes from these dinners we do before the interviews with me, you, Sholto, and a couple of others. We’re like, “what should I ask John Schulman, what I should ask Dario.” 

Suppose this is the way things go and we get these unhobblings—

**Extracted Belief:**

The best possible data for AI models is generated through experience and problem-solving, rather than simply being presented with solutions from others.

**Context:**

Aschenbrenner argues that the best data for AI models comes from the process of trying to solve a problem and failing, eventually figuring it out on their own, just as humans learn through experience.

**Justification:**

He compares this type of learning to the way humans learn by trying a practice problem and failing, eventually figuring it out themselves, rather than just reading how someone else solved it.

--------

## Chunk 54

**Chunk:**

Dwarkesh Patel
By the way, if that take sounds familiar it's because it was part of the question I asked John Schulman. It goes to illustrate the thing I said in the intro. A bunch of the things I've learned about AI comes from these dinners we do before the interviews with me, you, Sholto, and a couple of others. We’re like, “what should I ask John Schulman, what I should ask Dario.” 

Suppose this is the way things go and we get these unhobblings—
Leopold Aschenbrenner
And the scaling. You have this baseline of this enormous force of scaling. GPT-2 was amazing. It could string together plausible sentences, but it could barely do anything. It was kind of like a preschooler. GPT-4, on the other hand, could write code and do hard math, like a smart high schooler. This big jump in capability is explored in the essay series. I count the orders of magnitude of compute and scale-up of algorithmic progress.

Scaling alone by 2027-2028 is going to do another preschool to high school jump on top of GPT-4. At a per token level, the models will be incredibly smart. They'll gain more reliability, and with the addition of unhobblings, they'll look less like chatbots and more like agents or drop-in remote workers. That's when things really get going.

(00:20:31) – AI 2028: The return of history
Dwarkesh Patel
I want to ask more questions about this but let's zoom out. Suppose you're right about this. This is because of the 2027 cluster which is at 10 GW?

**Extracted Belief:**

GPT-2 had limited capabilities, comparable to a preschooler.

**Context:**

Leopold Aschenbrenner is discussing the advancements in AI capabilities from GPT-2 to GPT-4, using an analogy of a preschooler to describe GPT-2's limitations.

**Justification:**

Aschenbrenner's statement is based on his experience and observation of GPT-2's abilities.

--------

## Chunk 55

**Chunk:**

Dwarkesh Patel
By the way, if that take sounds familiar it's because it was part of the question I asked John Schulman. It goes to illustrate the thing I said in the intro. A bunch of the things I've learned about AI comes from these dinners we do before the interviews with me, you, Sholto, and a couple of others. We’re like, “what should I ask John Schulman, what I should ask Dario.” 

Suppose this is the way things go and we get these unhobblings—
Leopold Aschenbrenner
And the scaling. You have this baseline of this enormous force of scaling. GPT-2 was amazing. It could string together plausible sentences, but it could barely do anything. It was kind of like a preschooler. GPT-4, on the other hand, could write code and do hard math, like a smart high schooler. This big jump in capability is explored in the essay series. I count the orders of magnitude of compute and scale-up of algorithmic progress.

Scaling alone by 2027-2028 is going to do another preschool to high school jump on top of GPT-4. At a per token level, the models will be incredibly smart. They'll gain more reliability, and with the addition of unhobblings, they'll look less like chatbots and more like agents or drop-in remote workers. That's when things really get going.

(00:20:31) – AI 2028: The return of history
Dwarkesh Patel
I want to ask more questions about this but let's zoom out. Suppose you're right about this. This is because of the 2027 cluster which is at 10 GW?

**Extracted Belief:**

GPT-4 has advanced capabilities, comparable to a smart high schooler, including the ability to write code and perform complex mathematical tasks.

**Context:**

Leopold Aschenbrenner is discussing the advancements in AI capabilities from GPT-2 to GPT-4, using an analogy of a smart high schooler to describe GPT-4's abilities.

**Justification:**

Aschenbrenner's statement is based on his experience and observation of GPT-4's abilities.

--------

## Chunk 56

**Chunk:**

Dwarkesh Patel
By the way, if that take sounds familiar it's because it was part of the question I asked John Schulman. It goes to illustrate the thing I said in the intro. A bunch of the things I've learned about AI comes from these dinners we do before the interviews with me, you, Sholto, and a couple of others. We’re like, “what should I ask John Schulman, what I should ask Dario.” 

Suppose this is the way things go and we get these unhobblings—
Leopold Aschenbrenner
And the scaling. You have this baseline of this enormous force of scaling. GPT-2 was amazing. It could string together plausible sentences, but it could barely do anything. It was kind of like a preschooler. GPT-4, on the other hand, could write code and do hard math, like a smart high schooler. This big jump in capability is explored in the essay series. I count the orders of magnitude of compute and scale-up of algorithmic progress.

Scaling alone by 2027-2028 is going to do another preschool to high school jump on top of GPT-4. At a per token level, the models will be incredibly smart. They'll gain more reliability, and with the addition of unhobblings, they'll look less like chatbots and more like agents or drop-in remote workers. That's when things really get going.

(00:20:31) – AI 2028: The return of history
Dwarkesh Patel
I want to ask more questions about this but let's zoom out. Suppose you're right about this. This is because of the 2027 cluster which is at 10 GW?

**Extracted Belief:**

Scaling alone will lead to significant advancements in AI capabilities between 2027 and 2028, comparable to the leap from a preschooler to a smart high schooler.

**Context:**

Leopold Aschenbrenner is discussing the future of AI, predicting a significant improvement in capabilities due to scaling.

**Justification:**

Aschenbrenner's prediction is based on his expertise in the field of AI and his observation of previous trends.

--------

## Chunk 57

**Chunk:**

Dwarkesh Patel
By the way, if that take sounds familiar it's because it was part of the question I asked John Schulman. It goes to illustrate the thing I said in the intro. A bunch of the things I've learned about AI comes from these dinners we do before the interviews with me, you, Sholto, and a couple of others. We’re like, “what should I ask John Schulman, what I should ask Dario.” 

Suppose this is the way things go and we get these unhobblings—
Leopold Aschenbrenner
And the scaling. You have this baseline of this enormous force of scaling. GPT-2 was amazing. It could string together plausible sentences, but it could barely do anything. It was kind of like a preschooler. GPT-4, on the other hand, could write code and do hard math, like a smart high schooler. This big jump in capability is explored in the essay series. I count the orders of magnitude of compute and scale-up of algorithmic progress.

Scaling alone by 2027-2028 is going to do another preschool to high school jump on top of GPT-4. At a per token level, the models will be incredibly smart. They'll gain more reliability, and with the addition of unhobblings, they'll look less like chatbots and more like agents or drop-in remote workers. That's when things really get going.

(00:20:31) – AI 2028: The return of history
Dwarkesh Patel
I want to ask more questions about this but let's zoom out. Suppose you're right about this. This is because of the 2027 cluster which is at 10 GW?

**Extracted Belief:**

AI models will become more reliable and less chatbot-like due to scaling and unhobbling.

**Context:**

Leopold Aschenbrenner is describing the future development of AI, predicting an increase in reliability and a shift away from chatbot-like behavior.

**Justification:**

Aschenbrenner's prediction is based on his expertise in the field of AI and his observation of previous trends.

--------

## Chunk 58

**Chunk:**

Dwarkesh Patel
By the way, if that take sounds familiar it's because it was part of the question I asked John Schulman. It goes to illustrate the thing I said in the intro. A bunch of the things I've learned about AI comes from these dinners we do before the interviews with me, you, Sholto, and a couple of others. We’re like, “what should I ask John Schulman, what I should ask Dario.” 

Suppose this is the way things go and we get these unhobblings—
Leopold Aschenbrenner
And the scaling. You have this baseline of this enormous force of scaling. GPT-2 was amazing. It could string together plausible sentences, but it could barely do anything. It was kind of like a preschooler. GPT-4, on the other hand, could write code and do hard math, like a smart high schooler. This big jump in capability is explored in the essay series. I count the orders of magnitude of compute and scale-up of algorithmic progress.

Scaling alone by 2027-2028 is going to do another preschool to high school jump on top of GPT-4. At a per token level, the models will be incredibly smart. They'll gain more reliability, and with the addition of unhobblings, they'll look less like chatbots and more like agents or drop-in remote workers. That's when things really get going.

(00:20:31) – AI 2028: The return of history
Dwarkesh Patel
I want to ask more questions about this but let's zoom out. Suppose you're right about this. This is because of the 2027 cluster which is at 10 GW?

**Extracted Belief:**

AI models will evolve to become agents or remote workers due to scaling and unhobbling.

**Context:**

Leopold Aschenbrenner is describing the future development of AI, predicting a shift in functionality towards agents and remote workers.

**Justification:**

Aschenbrenner's prediction is based on his expertise in the field of AI and his observation of previous trends.

--------

## Chunk 59

**Chunk:**

Dwarkesh Patel
I want to ask more questions about this but let's zoom out. Suppose you're right about this. This is because of the 2027 cluster which is at 10 GW?
Leopold Aschenbrenner
2028 is 10 GW. Maybe it'll be pulled forward.
Dwarkesh Patel
Something like a 5.5 level by 2027, whatever that's called. What does the world look like at that point? You have these remote workers who can replace people. What is the reaction to that in terms of the economy, politics, and geopolitics?

**Extracted Belief:**

The year 2028 will be marked by a significant increase in AI capabilities, with the amount of computing power used reaching 10 GW.

**Context:**

Leopold Aschenbrenner is discussing the future of AI and the potential impact of increasing computing power.

**Justification:**

He states that '2028 is 10 GW,' indicating an expectation that AI development will reach this level of computing power.

--------

## Chunk 60

**Chunk:**

Dwarkesh Patel
I want to ask more questions about this but let's zoom out. Suppose you're right about this. This is because of the 2027 cluster which is at 10 GW?
Leopold Aschenbrenner
2028 is 10 GW. Maybe it'll be pulled forward.
Dwarkesh Patel
Something like a 5.5 level by 2027, whatever that's called. What does the world look like at that point? You have these remote workers who can replace people. What is the reaction to that in terms of the economy, politics, and geopolitics?

**Extracted Belief:**

The timeline for achieving 10 GW of computing power for AI development may be accelerated.

**Context:**

Leopold Aschenbrenner acknowledges the possibility of faster progress in AI development.

**Justification:**

He says 'Maybe it'll be pulled forward,' indicating that the 10 GW milestone could be reached before 2028.

--------

## Chunk 61

**Chunk:**

Dwarkesh Patel
Something like a 5.5 level by 2027, whatever that's called. What does the world look like at that point? You have these remote workers who can replace people. What is the reaction to that in terms of the economy, politics, and geopolitics?
Leopold Aschenbrenner
2023 was a really interesting year to experience as somebody who was really following the AI stuff.
Dwarkesh Patel
What were you doing in 2023?

**Extracted Belief:**

The year 2023 was a significant and interesting period for observing developments in artificial intelligence (AI).

**Context:**

Leopold Aschenbrenner is reflecting on the year 2023, highlighting its importance in the context of AI advancements.

**Justification:**

Aschenbrenner states that he was closely following AI developments in 2023, implying that he had access to information and observations about its progress.

--------

## Chunk 62

**Chunk:**

Dwarkesh Patel
What were you doing in 2023?
Leopold Aschenbrenner
OpenAI. When you were at OpenAI in 2023, it was a weird thing. You almost didn't want to talk about AI or AGI. It was kind of a dirty word. Then in 2023, people saw ChatGPT for the first time, they saw GPT-4, and it just exploded.

It triggered huge capital expenditures from all these firms and an explosion in revenue from Nvidia and so on. Things have been quiet since then, but the next thing has been in the oven. I expect every generation these g-forces to intensify. People will see the models. They won’t have counted the OOMs so they're going to be surprised. It'll be kind of crazy.

Revenue is going to accelerate. Suppose you do hit $10 billion by the end of this year. Suppose it just continues on the trajectory of revenue doubling every six months. It's not actually that far from $100 billion, maybe by 2026. At some point, what happened to Nvidia is going to happen to Big Tech. It's going to explode. A lot more people are going to feel it.

2023 was the moment for me where AGI went from being this theoretical, abstract thing. I see it, I feel it, and I see the path. I see where it's going. I can see the cluster it's trained on, the rough combination of algorithms, the people, how it's happening. Most of the world is not there yet. Most of the people who feel it are right here. A lot more of the world is going to start feeling it. That's going to start being intense.
Dwarkesh Patel
Right now, who feels it? You can go on Twitter and there are these GPT wrapper companies, like, "whoa, GPT-4 is going to change our business."

**Extracted Belief:**

The development of artificial general intelligence (AGI) is rapidly progressing.

**Context:**

Leopold Aschenbrenner describes his experience in 2023 at OpenAI, stating that AGI was considered an abstract concept but has since become more tangible and impactful.

**Justification:**

He highlights the release of ChatGPT and GPT-4 in 2023 as evidence of rapid progress in AI development, leading to increased capital expenditure and revenue for companies like Nvidia.

--------

## Chunk 63

**Chunk:**

Dwarkesh Patel
What were you doing in 2023?
Leopold Aschenbrenner
OpenAI. When you were at OpenAI in 2023, it was a weird thing. You almost didn't want to talk about AI or AGI. It was kind of a dirty word. Then in 2023, people saw ChatGPT for the first time, they saw GPT-4, and it just exploded.

It triggered huge capital expenditures from all these firms and an explosion in revenue from Nvidia and so on. Things have been quiet since then, but the next thing has been in the oven. I expect every generation these g-forces to intensify. People will see the models. They won’t have counted the OOMs so they're going to be surprised. It'll be kind of crazy.

Revenue is going to accelerate. Suppose you do hit $10 billion by the end of this year. Suppose it just continues on the trajectory of revenue doubling every six months. It's not actually that far from $100 billion, maybe by 2026. At some point, what happened to Nvidia is going to happen to Big Tech. It's going to explode. A lot more people are going to feel it.

2023 was the moment for me where AGI went from being this theoretical, abstract thing. I see it, I feel it, and I see the path. I see where it's going. I can see the cluster it's trained on, the rough combination of algorithms, the people, how it's happening. Most of the world is not there yet. Most of the people who feel it are right here. A lot more of the world is going to start feeling it. That's going to start being intense.
Dwarkesh Patel
Right now, who feels it? You can go on Twitter and there are these GPT wrapper companies, like, "whoa, GPT-4 is going to change our business."

**Extracted Belief:**

The future of AI development will involve increasingly powerful and unexpected models.

**Context:**

Leopold Aschenbrenner expresses his belief that future AI models will surpass current capabilities and surprise users.

**Justification:**

He mentions that these new models will be more powerful than expected,  suggesting a trend of increasing AI capabilities.

--------

## Chunk 64

**Chunk:**

Dwarkesh Patel
What were you doing in 2023?
Leopold Aschenbrenner
OpenAI. When you were at OpenAI in 2023, it was a weird thing. You almost didn't want to talk about AI or AGI. It was kind of a dirty word. Then in 2023, people saw ChatGPT for the first time, they saw GPT-4, and it just exploded.

It triggered huge capital expenditures from all these firms and an explosion in revenue from Nvidia and so on. Things have been quiet since then, but the next thing has been in the oven. I expect every generation these g-forces to intensify. People will see the models. They won’t have counted the OOMs so they're going to be surprised. It'll be kind of crazy.

Revenue is going to accelerate. Suppose you do hit $10 billion by the end of this year. Suppose it just continues on the trajectory of revenue doubling every six months. It's not actually that far from $100 billion, maybe by 2026. At some point, what happened to Nvidia is going to happen to Big Tech. It's going to explode. A lot more people are going to feel it.

2023 was the moment for me where AGI went from being this theoretical, abstract thing. I see it, I feel it, and I see the path. I see where it's going. I can see the cluster it's trained on, the rough combination of algorithms, the people, how it's happening. Most of the world is not there yet. Most of the people who feel it are right here. A lot more of the world is going to start feeling it. That's going to start being intense.
Dwarkesh Patel
Right now, who feels it? You can go on Twitter and there are these GPT wrapper companies, like, "whoa, GPT-4 is going to change our business."

**Extracted Belief:**

The revenue generated by AI technology will increase exponentially.

**Context:**

Leopold Aschenbrenner predicts that revenue from AI will continue to rise rapidly.

**Justification:**

He suggests that AI revenue will double every six months and eventually reach a level similar to Nvidia's growth, impacting large tech companies.

--------

## Chunk 65

**Chunk:**

Dwarkesh Patel
What were you doing in 2023?
Leopold Aschenbrenner
OpenAI. When you were at OpenAI in 2023, it was a weird thing. You almost didn't want to talk about AI or AGI. It was kind of a dirty word. Then in 2023, people saw ChatGPT for the first time, they saw GPT-4, and it just exploded.

It triggered huge capital expenditures from all these firms and an explosion in revenue from Nvidia and so on. Things have been quiet since then, but the next thing has been in the oven. I expect every generation these g-forces to intensify. People will see the models. They won’t have counted the OOMs so they're going to be surprised. It'll be kind of crazy.

Revenue is going to accelerate. Suppose you do hit $10 billion by the end of this year. Suppose it just continues on the trajectory of revenue doubling every six months. It's not actually that far from $100 billion, maybe by 2026. At some point, what happened to Nvidia is going to happen to Big Tech. It's going to explode. A lot more people are going to feel it.

2023 was the moment for me where AGI went from being this theoretical, abstract thing. I see it, I feel it, and I see the path. I see where it's going. I can see the cluster it's trained on, the rough combination of algorithms, the people, how it's happening. Most of the world is not there yet. Most of the people who feel it are right here. A lot more of the world is going to start feeling it. That's going to start being intense.
Dwarkesh Patel
Right now, who feels it? You can go on Twitter and there are these GPT wrapper companies, like, "whoa, GPT-4 is going to change our business."

**Extracted Belief:**

The development of AGI is moving from a theoretical concept to a tangible reality.

**Context:**

Leopold Aschenbrenner describes his personal experience of witnessing the shift from a theoretical understanding of AGI to a more concrete understanding.

**Justification:**

He states that in 2023, AGI moved from being a theoretical concept to a real, observable phenomenon for him.

--------

## Chunk 66

**Chunk:**

Dwarkesh Patel
Right now, who feels it? You can go on Twitter and there are these GPT wrapper companies, like, "whoa, GPT-4 is going to change our business."
Leopold Aschenbrenner
I'm so bearish on the wrapper companies because they're betting on stagnation. They're betting that you have these intermediate models and it takes so much schlep to integrate them. I'm really bearish because we're just going to sonic boom you. We're going to get the unhobblings. We're going to get the drop-in remote worker. Your stuff is not going to matter.
Dwarkesh Patel
So that's done. SF, this crowd, is paying attention now. Who is going to be paying attention in 2026 and 2027? Presumably, these are years in which hundreds of billions of capex is being spent on AI.

**Extracted Belief:**

Companies developing GPT wrappers are betting on the stagnation of AI technology, believing that intermediate models will remain prevalent and integration will be difficult.

**Context:**

Leopold Aschenbrenner is expressing his opinion on the future of AI and the potential for GPT wrapper companies, which he believes are making a risky bet.

**Justification:**

He states that these companies are "betting on stagnation" and that they are "betting that you have these intermediate models and it takes so much schlep to integrate them."

--------

## Chunk 67

**Chunk:**

Dwarkesh Patel
Right now, who feels it? You can go on Twitter and there are these GPT wrapper companies, like, "whoa, GPT-4 is going to change our business."
Leopold Aschenbrenner
I'm so bearish on the wrapper companies because they're betting on stagnation. They're betting that you have these intermediate models and it takes so much schlep to integrate them. I'm really bearish because we're just going to sonic boom you. We're going to get the unhobblings. We're going to get the drop-in remote worker. Your stuff is not going to matter.
Dwarkesh Patel
So that's done. SF, this crowd, is paying attention now. Who is going to be paying attention in 2026 and 2027? Presumably, these are years in which hundreds of billions of capex is being spent on AI.

**Extracted Belief:**

The development of AI technology will not stagnate, but rather will rapidly advance, leading to significant breakthroughs.

**Context:**

Leopold Aschenbrenner is contrasting his belief in rapid AI advancement with the stagnation he believes companies building GPT wrappers are betting on.

**Justification:**

He claims that AI development will "sonic boom" and "get the unhobblings", suggesting rapid progress.

--------

## Chunk 68

**Chunk:**

Dwarkesh Patel
Right now, who feels it? You can go on Twitter and there are these GPT wrapper companies, like, "whoa, GPT-4 is going to change our business."
Leopold Aschenbrenner
I'm so bearish on the wrapper companies because they're betting on stagnation. They're betting that you have these intermediate models and it takes so much schlep to integrate them. I'm really bearish because we're just going to sonic boom you. We're going to get the unhobblings. We're going to get the drop-in remote worker. Your stuff is not going to matter.
Dwarkesh Patel
So that's done. SF, this crowd, is paying attention now. Who is going to be paying attention in 2026 and 2027? Presumably, these are years in which hundreds of billions of capex is being spent on AI.

**Extracted Belief:**

The development of AI will lead to the creation of 'drop-in remote workers,' which will replace current intermediate models and render GPT wrappers obsolete.

**Context:**

Leopold Aschenbrenner is outlining his belief in the rapid evolution of AI, stating that it will eventually create replacements for current models and technologies.

**Justification:**

He says that AI will "get the drop-in remote worker" which will make existing technologies, like GPT wrappers, irrelevant.

--------

## Chunk 69

**Chunk:**

Dwarkesh Patel
So that's done. SF, this crowd, is paying attention now. Who is going to be paying attention in 2026 and 2027? Presumably, these are years in which hundreds of billions of capex is being spent on AI.
Leopold Aschenbrenner
The national security state is going to start paying a lot of attention. I hope we get to talk about that.
Dwarkesh Patel
Let’s talk about it now. What happens? What is the immediate political reaction? Looking internationally, I don't know if Xi Jinping sees the GPT-4 news and goes, "oh, my God, look at the MMLU score on that. What are we doing about this, comrade?"

So what happens when he sees a remote worker replacement and it has $100 billion in revenue? There’s a lot of businesses that have $100 billion in revenue, and people aren't staying up all night talking about it.

**Extracted Belief:**

The national security state will become increasingly interested in AI advancements, particularly those related to superintelligence.

**Context:**

Leopold Aschenbrenner responds to Dwarkesh Patel's question about who will be paying attention to AI advancements in the future, specifically in 2026 and 2027.

**Justification:**

Leopold Aschenbrenner states, "The national security state is going to start paying a lot of attention. I hope we get to talk about that."

--------

## Chunk 70

**Chunk:**

Dwarkesh Patel
Let’s talk about it now. What happens? What is the immediate political reaction? Looking internationally, I don't know if Xi Jinping sees the GPT-4 news and goes, "oh, my God, look at the MMLU score on that. What are we doing about this, comrade?"

So what happens when he sees a remote worker replacement and it has $100 billion in revenue? There’s a lot of businesses that have $100 billion in revenue, and people aren't staying up all night talking about it.
Leopold Aschenbrenner
The question is, when does the CCP and when does the American national security establishment realize that superintelligence is going to be absolutely decisive for national power? This is where the intelligence explosion stuff comes in, which we should talk about later.

You have AGI. You have this drop-in remote worker that can replace you or me, at least for remote jobs. Fairly quickly, you turn the crank one or two more times and you get a thing that's smarter than humans.

Even more than just turning the crank a few more times, one of the first jobs to be automated is going to be that of an AI researcher or engineer. If you can automate AI research, things can start going very fast.

Right now, there's already at this trend of 0.5 OOMs a year of algorithmic progress. At some point, you're going to have GPU fleets in the tens of millions for inference or more. You’re going to be able to run 100 million human equivalents of these automated AI researchers.

If you can do that, you can maybe do a decade's worth of ML research progress in a year. You get some sort of 10x speed up. You can make the jump to AI that is vastly smarter than humans within a year, a couple of years.

That broadens from there. You have this initial acceleration of AI research. You apply R&D to a bunch of other fields of technology. At this point, you have a billion super intelligent researchers, engineers, technicians, everything. They’re superbly competent at all things.

They're going to figure out robotics. We talked about that being a software problem. Well, you have a billion super smart — smarter than the smartest human researchers — AI researchers in your cluster. At some point during the intelligence explosion, they're going to be able to figure out robotics. Again, that’ll expand.

If you play this picture forward, it is fairly unlike any other technology. A couple years of lead could be utterly decisive in say, military competition. If you look at the first Gulf War, Western coalition forces had a 100:1 kill ratio. They had better sensors on their tanks. They had better precision missiles, GPS, and stealth. They had maybe 20-30 years of technological lead. They just completely crushed them.

Superintelligence applied to broad fields of R&D — and the industrial explosion that comes from it, robots making a lot of material — could compress a century’s worth of technological progress into less than a decade. That means that a couple years could mean a Gulf War 1-style advantage in military affairs. That’s including a decisive advantage that even preempts nukes.

How do you find nuclear stealth submarines? Right now, you have sensors and software to detect where they are. You can do that. You can find them. You have millions or billions of mosquito-sized drones, and they take out the nuclear submarines. They take out the mobile launchers. They take out the other nukes.

It’s potentially enormously destabilizing and enormously important for national power. At some point people are going to realize that. Not yet, but they will. When they do, it won’t just be the AI researchers in charge.

The CCP is going to have an all-out effort to infiltrate American AI labs. It’ll involve billions of dollars, thousands of people, and the full force of the Ministry of State Security. The CCP is going to try to outbuild us.

They added as much power in the last decade as an entire US electric grid. So the 100 GW cluster, at least the 100 GW part of it, is going to be a lot easier for them to get. By this point, it's going to be an extremely intense international competition.
Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.

**Extracted Belief:**

Superintelligence will become absolutely decisive for national power.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's query about the political reaction to the development of superintelligence, specifically in the context of international relations between the US and China.

**Justification:**

Aschenbrenner points to the historical example of the Gulf War, where technological advancements in sensors, missiles, and GPS provided the Western Coalition with a decisive advantage, emphasizing that superintelligence applied to military technology could result in a similar outcome, even potentially preempting nuclear weapons.

--------

## Chunk 71

**Chunk:**

Dwarkesh Patel
Let’s talk about it now. What happens? What is the immediate political reaction? Looking internationally, I don't know if Xi Jinping sees the GPT-4 news and goes, "oh, my God, look at the MMLU score on that. What are we doing about this, comrade?"

So what happens when he sees a remote worker replacement and it has $100 billion in revenue? There’s a lot of businesses that have $100 billion in revenue, and people aren't staying up all night talking about it.
Leopold Aschenbrenner
The question is, when does the CCP and when does the American national security establishment realize that superintelligence is going to be absolutely decisive for national power? This is where the intelligence explosion stuff comes in, which we should talk about later.

You have AGI. You have this drop-in remote worker that can replace you or me, at least for remote jobs. Fairly quickly, you turn the crank one or two more times and you get a thing that's smarter than humans.

Even more than just turning the crank a few more times, one of the first jobs to be automated is going to be that of an AI researcher or engineer. If you can automate AI research, things can start going very fast.

Right now, there's already at this trend of 0.5 OOMs a year of algorithmic progress. At some point, you're going to have GPU fleets in the tens of millions for inference or more. You’re going to be able to run 100 million human equivalents of these automated AI researchers.

If you can do that, you can maybe do a decade's worth of ML research progress in a year. You get some sort of 10x speed up. You can make the jump to AI that is vastly smarter than humans within a year, a couple of years.

That broadens from there. You have this initial acceleration of AI research. You apply R&D to a bunch of other fields of technology. At this point, you have a billion super intelligent researchers, engineers, technicians, everything. They’re superbly competent at all things.

They're going to figure out robotics. We talked about that being a software problem. Well, you have a billion super smart — smarter than the smartest human researchers — AI researchers in your cluster. At some point during the intelligence explosion, they're going to be able to figure out robotics. Again, that’ll expand.

If you play this picture forward, it is fairly unlike any other technology. A couple years of lead could be utterly decisive in say, military competition. If you look at the first Gulf War, Western coalition forces had a 100:1 kill ratio. They had better sensors on their tanks. They had better precision missiles, GPS, and stealth. They had maybe 20-30 years of technological lead. They just completely crushed them.

Superintelligence applied to broad fields of R&D — and the industrial explosion that comes from it, robots making a lot of material — could compress a century’s worth of technological progress into less than a decade. That means that a couple years could mean a Gulf War 1-style advantage in military affairs. That’s including a decisive advantage that even preempts nukes.

How do you find nuclear stealth submarines? Right now, you have sensors and software to detect where they are. You can do that. You can find them. You have millions or billions of mosquito-sized drones, and they take out the nuclear submarines. They take out the mobile launchers. They take out the other nukes.

It’s potentially enormously destabilizing and enormously important for national power. At some point people are going to realize that. Not yet, but they will. When they do, it won’t just be the AI researchers in charge.

The CCP is going to have an all-out effort to infiltrate American AI labs. It’ll involve billions of dollars, thousands of people, and the full force of the Ministry of State Security. The CCP is going to try to outbuild us.

They added as much power in the last decade as an entire US electric grid. So the 100 GW cluster, at least the 100 GW part of it, is going to be a lot easier for them to get. By this point, it's going to be an extremely intense international competition.
Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.

**Extracted Belief:**

AGI, or Artificial General Intelligence, can be developed by iteratively improving existing AI systems, eventually surpassing human intelligence.

**Context:**

Aschenbrenner describes the development of superintelligence as a gradual process of improving AI systems, starting with a 'drop-in remote worker' capable of replacing human workers, and then iteratively developing more advanced AI systems.

**Justification:**

Aschenbrenner references the current trend of 0.5 OOMs (orders of magnitude) per year in algorithmic progress, suggesting that with sufficient computational resources, such as GPU fleets, AI development could accelerate, ultimately leading to a superintelligence within a year or two.

--------

## Chunk 72

**Chunk:**

Dwarkesh Patel
Let’s talk about it now. What happens? What is the immediate political reaction? Looking internationally, I don't know if Xi Jinping sees the GPT-4 news and goes, "oh, my God, look at the MMLU score on that. What are we doing about this, comrade?"

So what happens when he sees a remote worker replacement and it has $100 billion in revenue? There’s a lot of businesses that have $100 billion in revenue, and people aren't staying up all night talking about it.
Leopold Aschenbrenner
The question is, when does the CCP and when does the American national security establishment realize that superintelligence is going to be absolutely decisive for national power? This is where the intelligence explosion stuff comes in, which we should talk about later.

You have AGI. You have this drop-in remote worker that can replace you or me, at least for remote jobs. Fairly quickly, you turn the crank one or two more times and you get a thing that's smarter than humans.

Even more than just turning the crank a few more times, one of the first jobs to be automated is going to be that of an AI researcher or engineer. If you can automate AI research, things can start going very fast.

Right now, there's already at this trend of 0.5 OOMs a year of algorithmic progress. At some point, you're going to have GPU fleets in the tens of millions for inference or more. You’re going to be able to run 100 million human equivalents of these automated AI researchers.

If you can do that, you can maybe do a decade's worth of ML research progress in a year. You get some sort of 10x speed up. You can make the jump to AI that is vastly smarter than humans within a year, a couple of years.

That broadens from there. You have this initial acceleration of AI research. You apply R&D to a bunch of other fields of technology. At this point, you have a billion super intelligent researchers, engineers, technicians, everything. They’re superbly competent at all things.

They're going to figure out robotics. We talked about that being a software problem. Well, you have a billion super smart — smarter than the smartest human researchers — AI researchers in your cluster. At some point during the intelligence explosion, they're going to be able to figure out robotics. Again, that’ll expand.

If you play this picture forward, it is fairly unlike any other technology. A couple years of lead could be utterly decisive in say, military competition. If you look at the first Gulf War, Western coalition forces had a 100:1 kill ratio. They had better sensors on their tanks. They had better precision missiles, GPS, and stealth. They had maybe 20-30 years of technological lead. They just completely crushed them.

Superintelligence applied to broad fields of R&D — and the industrial explosion that comes from it, robots making a lot of material — could compress a century’s worth of technological progress into less than a decade. That means that a couple years could mean a Gulf War 1-style advantage in military affairs. That’s including a decisive advantage that even preempts nukes.

How do you find nuclear stealth submarines? Right now, you have sensors and software to detect where they are. You can do that. You can find them. You have millions or billions of mosquito-sized drones, and they take out the nuclear submarines. They take out the mobile launchers. They take out the other nukes.

It’s potentially enormously destabilizing and enormously important for national power. At some point people are going to realize that. Not yet, but they will. When they do, it won’t just be the AI researchers in charge.

The CCP is going to have an all-out effort to infiltrate American AI labs. It’ll involve billions of dollars, thousands of people, and the full force of the Ministry of State Security. The CCP is going to try to outbuild us.

They added as much power in the last decade as an entire US electric grid. So the 100 GW cluster, at least the 100 GW part of it, is going to be a lot easier for them to get. By this point, it's going to be an extremely intense international competition.
Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.

**Extracted Belief:**

One of the first jobs to be automated by AI will be that of AI researchers and engineers.

**Context:**

Aschenbrenner outlines the process of AI development, specifically highlighting the potential for AI to automate its own development process.

**Justification:**

Aschenbrenner argues that this automation will lead to a significant acceleration in AI development, allowing for a decade's worth of progress in a single year.

--------

## Chunk 73

**Chunk:**

Dwarkesh Patel
Let’s talk about it now. What happens? What is the immediate political reaction? Looking internationally, I don't know if Xi Jinping sees the GPT-4 news and goes, "oh, my God, look at the MMLU score on that. What are we doing about this, comrade?"

So what happens when he sees a remote worker replacement and it has $100 billion in revenue? There’s a lot of businesses that have $100 billion in revenue, and people aren't staying up all night talking about it.
Leopold Aschenbrenner
The question is, when does the CCP and when does the American national security establishment realize that superintelligence is going to be absolutely decisive for national power? This is where the intelligence explosion stuff comes in, which we should talk about later.

You have AGI. You have this drop-in remote worker that can replace you or me, at least for remote jobs. Fairly quickly, you turn the crank one or two more times and you get a thing that's smarter than humans.

Even more than just turning the crank a few more times, one of the first jobs to be automated is going to be that of an AI researcher or engineer. If you can automate AI research, things can start going very fast.

Right now, there's already at this trend of 0.5 OOMs a year of algorithmic progress. At some point, you're going to have GPU fleets in the tens of millions for inference or more. You’re going to be able to run 100 million human equivalents of these automated AI researchers.

If you can do that, you can maybe do a decade's worth of ML research progress in a year. You get some sort of 10x speed up. You can make the jump to AI that is vastly smarter than humans within a year, a couple of years.

That broadens from there. You have this initial acceleration of AI research. You apply R&D to a bunch of other fields of technology. At this point, you have a billion super intelligent researchers, engineers, technicians, everything. They’re superbly competent at all things.

They're going to figure out robotics. We talked about that being a software problem. Well, you have a billion super smart — smarter than the smartest human researchers — AI researchers in your cluster. At some point during the intelligence explosion, they're going to be able to figure out robotics. Again, that’ll expand.

If you play this picture forward, it is fairly unlike any other technology. A couple years of lead could be utterly decisive in say, military competition. If you look at the first Gulf War, Western coalition forces had a 100:1 kill ratio. They had better sensors on their tanks. They had better precision missiles, GPS, and stealth. They had maybe 20-30 years of technological lead. They just completely crushed them.

Superintelligence applied to broad fields of R&D — and the industrial explosion that comes from it, robots making a lot of material — could compress a century’s worth of technological progress into less than a decade. That means that a couple years could mean a Gulf War 1-style advantage in military affairs. That’s including a decisive advantage that even preempts nukes.

How do you find nuclear stealth submarines? Right now, you have sensors and software to detect where they are. You can do that. You can find them. You have millions or billions of mosquito-sized drones, and they take out the nuclear submarines. They take out the mobile launchers. They take out the other nukes.

It’s potentially enormously destabilizing and enormously important for national power. At some point people are going to realize that. Not yet, but they will. When they do, it won’t just be the AI researchers in charge.

The CCP is going to have an all-out effort to infiltrate American AI labs. It’ll involve billions of dollars, thousands of people, and the full force of the Ministry of State Security. The CCP is going to try to outbuild us.

They added as much power in the last decade as an entire US electric grid. So the 100 GW cluster, at least the 100 GW part of it, is going to be a lot easier for them to get. By this point, it's going to be an extremely intense international competition.
Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.

**Extracted Belief:**

Superintelligent AI systems will be capable of solving complex problems in robotics, biology, and other technological fields.

**Context:**

Aschenbrenner speculates on the capabilities of superintelligent AI systems, particularly their potential to advance various fields, including robotics and biology, through accelerated research and development.

**Justification:**

He claims that a cluster of superintelligent AI researchers will be able to make significant breakthroughs in these areas due to their superior intelligence and ability to process information at a much faster rate than humans.

--------

## Chunk 74

**Chunk:**

Dwarkesh Patel
Let’s talk about it now. What happens? What is the immediate political reaction? Looking internationally, I don't know if Xi Jinping sees the GPT-4 news and goes, "oh, my God, look at the MMLU score on that. What are we doing about this, comrade?"

So what happens when he sees a remote worker replacement and it has $100 billion in revenue? There’s a lot of businesses that have $100 billion in revenue, and people aren't staying up all night talking about it.
Leopold Aschenbrenner
The question is, when does the CCP and when does the American national security establishment realize that superintelligence is going to be absolutely decisive for national power? This is where the intelligence explosion stuff comes in, which we should talk about later.

You have AGI. You have this drop-in remote worker that can replace you or me, at least for remote jobs. Fairly quickly, you turn the crank one or two more times and you get a thing that's smarter than humans.

Even more than just turning the crank a few more times, one of the first jobs to be automated is going to be that of an AI researcher or engineer. If you can automate AI research, things can start going very fast.

Right now, there's already at this trend of 0.5 OOMs a year of algorithmic progress. At some point, you're going to have GPU fleets in the tens of millions for inference or more. You’re going to be able to run 100 million human equivalents of these automated AI researchers.

If you can do that, you can maybe do a decade's worth of ML research progress in a year. You get some sort of 10x speed up. You can make the jump to AI that is vastly smarter than humans within a year, a couple of years.

That broadens from there. You have this initial acceleration of AI research. You apply R&D to a bunch of other fields of technology. At this point, you have a billion super intelligent researchers, engineers, technicians, everything. They’re superbly competent at all things.

They're going to figure out robotics. We talked about that being a software problem. Well, you have a billion super smart — smarter than the smartest human researchers — AI researchers in your cluster. At some point during the intelligence explosion, they're going to be able to figure out robotics. Again, that’ll expand.

If you play this picture forward, it is fairly unlike any other technology. A couple years of lead could be utterly decisive in say, military competition. If you look at the first Gulf War, Western coalition forces had a 100:1 kill ratio. They had better sensors on their tanks. They had better precision missiles, GPS, and stealth. They had maybe 20-30 years of technological lead. They just completely crushed them.

Superintelligence applied to broad fields of R&D — and the industrial explosion that comes from it, robots making a lot of material — could compress a century’s worth of technological progress into less than a decade. That means that a couple years could mean a Gulf War 1-style advantage in military affairs. That’s including a decisive advantage that even preempts nukes.

How do you find nuclear stealth submarines? Right now, you have sensors and software to detect where they are. You can do that. You can find them. You have millions or billions of mosquito-sized drones, and they take out the nuclear submarines. They take out the mobile launchers. They take out the other nukes.

It’s potentially enormously destabilizing and enormously important for national power. At some point people are going to realize that. Not yet, but they will. When they do, it won’t just be the AI researchers in charge.

The CCP is going to have an all-out effort to infiltrate American AI labs. It’ll involve billions of dollars, thousands of people, and the full force of the Ministry of State Security. The CCP is going to try to outbuild us.

They added as much power in the last decade as an entire US electric grid. So the 100 GW cluster, at least the 100 GW part of it, is going to be a lot easier for them to get. By this point, it's going to be an extremely intense international competition.
Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.

**Extracted Belief:**

The development of superintelligence will have a significant impact on national security and international power dynamics.

**Context:**

Aschenbrenner stresses the importance of superintelligence in the context of national security, suggesting that it will become a key factor in international competition.

**Justification:**

He uses the example of the Gulf War to illustrate how technological advancements can significantly influence military outcomes, implying that superintelligence could potentially give a nation a decisive advantage in military affairs.

--------

## Chunk 75

**Chunk:**

Dwarkesh Patel
Let’s talk about it now. What happens? What is the immediate political reaction? Looking internationally, I don't know if Xi Jinping sees the GPT-4 news and goes, "oh, my God, look at the MMLU score on that. What are we doing about this, comrade?"

So what happens when he sees a remote worker replacement and it has $100 billion in revenue? There’s a lot of businesses that have $100 billion in revenue, and people aren't staying up all night talking about it.
Leopold Aschenbrenner
The question is, when does the CCP and when does the American national security establishment realize that superintelligence is going to be absolutely decisive for national power? This is where the intelligence explosion stuff comes in, which we should talk about later.

You have AGI. You have this drop-in remote worker that can replace you or me, at least for remote jobs. Fairly quickly, you turn the crank one or two more times and you get a thing that's smarter than humans.

Even more than just turning the crank a few more times, one of the first jobs to be automated is going to be that of an AI researcher or engineer. If you can automate AI research, things can start going very fast.

Right now, there's already at this trend of 0.5 OOMs a year of algorithmic progress. At some point, you're going to have GPU fleets in the tens of millions for inference or more. You’re going to be able to run 100 million human equivalents of these automated AI researchers.

If you can do that, you can maybe do a decade's worth of ML research progress in a year. You get some sort of 10x speed up. You can make the jump to AI that is vastly smarter than humans within a year, a couple of years.

That broadens from there. You have this initial acceleration of AI research. You apply R&D to a bunch of other fields of technology. At this point, you have a billion super intelligent researchers, engineers, technicians, everything. They’re superbly competent at all things.

They're going to figure out robotics. We talked about that being a software problem. Well, you have a billion super smart — smarter than the smartest human researchers — AI researchers in your cluster. At some point during the intelligence explosion, they're going to be able to figure out robotics. Again, that’ll expand.

If you play this picture forward, it is fairly unlike any other technology. A couple years of lead could be utterly decisive in say, military competition. If you look at the first Gulf War, Western coalition forces had a 100:1 kill ratio. They had better sensors on their tanks. They had better precision missiles, GPS, and stealth. They had maybe 20-30 years of technological lead. They just completely crushed them.

Superintelligence applied to broad fields of R&D — and the industrial explosion that comes from it, robots making a lot of material — could compress a century’s worth of technological progress into less than a decade. That means that a couple years could mean a Gulf War 1-style advantage in military affairs. That’s including a decisive advantage that even preempts nukes.

How do you find nuclear stealth submarines? Right now, you have sensors and software to detect where they are. You can do that. You can find them. You have millions or billions of mosquito-sized drones, and they take out the nuclear submarines. They take out the mobile launchers. They take out the other nukes.

It’s potentially enormously destabilizing and enormously important for national power. At some point people are going to realize that. Not yet, but they will. When they do, it won’t just be the AI researchers in charge.

The CCP is going to have an all-out effort to infiltrate American AI labs. It’ll involve billions of dollars, thousands of people, and the full force of the Ministry of State Security. The CCP is going to try to outbuild us.

They added as much power in the last decade as an entire US electric grid. So the 100 GW cluster, at least the 100 GW part of it, is going to be a lot easier for them to get. By this point, it's going to be an extremely intense international competition.
Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.

**Extracted Belief:**

The Chinese government will invest significant resources in developing superintelligence and attempting to infiltrate American AI labs.

**Context:**

Aschenbrenner predicts the Chinese government's response to the development of superintelligence, suggesting they will engage in a competitive race to achieve a leading position.

**Justification:**

He points to the Chinese government's massive investments in infrastructure and technology in recent years, indicating that they have the resources and motivation to aggressively pursue superintelligence development.

--------

## Chunk 76

**Chunk:**

Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.
Leopold Aschenbrenner
It's a little bit more gradual. It's an explosion that starts narrowly. It can do cognitive jobs. The highest ROI use for cognitive jobs is to make the AI better and solve robotics. As you solve robotics, now you can do R&D in biology and other technology.

Initially, you start with the factory workers. They're wearing the glasses and AirPods, and the AI is instructing them because you can make any worker into a skilled technician. Then you have the robots come in. So this process expands.
Dwarkesh Patel
Meta's Ray-Bans are a complement to Llama.

**Extracted Belief:**

The development of Artificial General Intelligence (AGI) will be a gradual process, starting with a narrow focus on cognitive tasks.

**Context:**

Leopold Aschenbrenner is describing the potential progression of AGI development, suggesting that it won't be an immediate explosion but rather a gradual expansion from narrow applications.

**Justification:**

Aschenbrenner states that AGI development will be 'a little bit more gradual' and 'starts narrowly' before expanding to broader applications.

--------

## Chunk 77

**Chunk:**

Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.
Leopold Aschenbrenner
It's a little bit more gradual. It's an explosion that starts narrowly. It can do cognitive jobs. The highest ROI use for cognitive jobs is to make the AI better and solve robotics. As you solve robotics, now you can do R&D in biology and other technology.

Initially, you start with the factory workers. They're wearing the glasses and AirPods, and the AI is instructing them because you can make any worker into a skilled technician. Then you have the robots come in. So this process expands.
Dwarkesh Patel
Meta's Ray-Bans are a complement to Llama.

**Extracted Belief:**

The most efficient use of AGI in its early stages will be to improve itself and solve robotics.

**Context:**

Aschenbrenner is outlining the initial priorities for AGI development, emphasizing the importance of self-improvement and robotics as early applications.

**Justification:**

Aschenbrenner states that the 'highest ROI use for cognitive jobs is to make the AI better and solve robotics.'

--------

## Chunk 78

**Chunk:**

Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.
Leopold Aschenbrenner
It's a little bit more gradual. It's an explosion that starts narrowly. It can do cognitive jobs. The highest ROI use for cognitive jobs is to make the AI better and solve robotics. As you solve robotics, now you can do R&D in biology and other technology.

Initially, you start with the factory workers. They're wearing the glasses and AirPods, and the AI is instructing them because you can make any worker into a skilled technician. Then you have the robots come in. So this process expands.
Dwarkesh Patel
Meta's Ray-Bans are a complement to Llama.

**Extracted Belief:**

Once AGI solves robotics, it can be applied to research and development in other fields, such as biology and technology.

**Context:**

Aschenbrenner is describing the potential expansion of AGI applications beyond its initial focus, suggesting that solving robotics will enable further breakthroughs in other fields.

**Justification:**

Aschenbrenner states that 'as you solve robotics, now you can do R&D in biology and other technology.'

--------

## Chunk 79

**Chunk:**

Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.
Leopold Aschenbrenner
It's a little bit more gradual. It's an explosion that starts narrowly. It can do cognitive jobs. The highest ROI use for cognitive jobs is to make the AI better and solve robotics. As you solve robotics, now you can do R&D in biology and other technology.

Initially, you start with the factory workers. They're wearing the glasses and AirPods, and the AI is instructing them because you can make any worker into a skilled technician. Then you have the robots come in. So this process expands.
Dwarkesh Patel
Meta's Ray-Bans are a complement to Llama.

**Extracted Belief:**

AGI can be used to train factory workers and make them skilled technicians.

**Context:**

Aschenbrenner is describing a potential application of AGI in the manufacturing sector, suggesting that it can be used to enhance worker skills.

**Justification:**

Aschenbrenner states that 'you can make any worker into a skilled technician.'

--------

## Chunk 80

**Chunk:**

Dwarkesh Patel
One thing I'm uncertain about in this picture is if it’s like what you say, where it's more of an explosion. You’ve developed an AGI. You make it into an AI researcher. For a while, you're only using this ability to make hundreds of millions of other AI researchers. The thing that comes out of this really frenetic process is a superintelligence. Then that goes out in the world and is developing robotics and helping you take over other countries and whatever.
Leopold Aschenbrenner
It's a little bit more gradual. It's an explosion that starts narrowly. It can do cognitive jobs. The highest ROI use for cognitive jobs is to make the AI better and solve robotics. As you solve robotics, now you can do R&D in biology and other technology.

Initially, you start with the factory workers. They're wearing the glasses and AirPods, and the AI is instructing them because you can make any worker into a skilled technician. Then you have the robots come in. So this process expands.
Dwarkesh Patel
Meta's Ray-Bans are a complement to Llama.

**Extracted Belief:**

Robotics will eventually be integrated into the manufacturing process, replacing the need for human workers.

**Context:**

Aschenbrenner is discussing the potential impact of AGI on the workforce, suggesting that the development of robotics will lead to a reduction in human jobs.

**Justification:**

Aschenbrenner states that 'then you have the robots come in.'

--------

## Chunk 81

**Chunk:**

Dwarkesh Patel
Meta's Ray-Bans are a complement to Llama.
Leopold Aschenbrenner
With the fabs in the US, their constraint is skilled workers. Even if you don't have robots, you have the cognitive superintelligence and can kind of make them all into skilled workers immediately. That's a very brief period. Robots will come soon.
Dwarkesh Patel
Suppose this is actually how the tech progresses in the United States, maybe because these companies are already generating hundreds of billions of dollars of AI revenue

**Extracted Belief:**

The primary constraint for semiconductor fabrication plants (fabs) in the United States is a shortage of skilled workers.

**Context:**

Leopold Aschenbrenner is discussing the implications of advanced artificial intelligence (AI) on the US semiconductor industry.

**Justification:**

Leopold Aschenbrenner states, "With the fabs in the US, their constraint is skilled workers." This statement suggests that a lack of skilled workers is a key obstacle for US fabs.

--------

## Chunk 82

**Chunk:**

Dwarkesh Patel
Meta's Ray-Bans are a complement to Llama.
Leopold Aschenbrenner
With the fabs in the US, their constraint is skilled workers. Even if you don't have robots, you have the cognitive superintelligence and can kind of make them all into skilled workers immediately. That's a very brief period. Robots will come soon.
Dwarkesh Patel
Suppose this is actually how the tech progresses in the United States, maybe because these companies are already generating hundreds of billions of dollars of AI revenue

**Extracted Belief:**

Cognitive superintelligence can be used to quickly train unskilled workers to become skilled technicians.

**Context:**

Leopold Aschenbrenner is describing how AI can solve the workforce shortage issue in the US semiconductor industry.

**Justification:**

He says, "Even if you don't have robots, you have the cognitive superintelligence and can kind of make them all into skilled workers immediately." This implies that AI can quickly upskill workers.

--------

## Chunk 83

**Chunk:**

Dwarkesh Patel
Meta's Ray-Bans are a complement to Llama.
Leopold Aschenbrenner
With the fabs in the US, their constraint is skilled workers. Even if you don't have robots, you have the cognitive superintelligence and can kind of make them all into skilled workers immediately. That's a very brief period. Robots will come soon.
Dwarkesh Patel
Suppose this is actually how the tech progresses in the United States, maybe because these companies are already generating hundreds of billions of dollars of AI revenue

**Extracted Belief:**

Robots will eventually be deployed in US semiconductor fabs.

**Context:**

Leopold Aschenbrenner is discussing the future of automation in the semiconductor industry.

**Justification:**

He states, "That's a very brief period. Robots will come soon." This implies that he believes robots will be implemented soon, likely based on his understanding of the advancements in robotics and AI.

--------

## Chunk 84

**Chunk:**

Dwarkesh Patel
Suppose this is actually how the tech progresses in the United States, maybe because these companies are already generating hundreds of billions of dollars of AI revenue
Leopold Aschenbrenner
At this point, companies are borrowing hundreds of billions or more in the corporate debt markets.
Dwarkesh Patel
Why is a CCP bureaucrat, some 60-year-old guy, looking at this and going, "oh, Copilot has gotten better now" and now—

**Extracted Belief:**

Companies are borrowing hundreds of billions of dollars or more in the corporate debt markets.

**Context:**

Leopold Aschenbrenner is describing the current state of the technology industry and the financial landscape related to AI development.

**Justification:**

This statement reflects current economic trends and data on corporate debt markets, which Leopold Aschenbrenner is presumably aware of based on his expertise in the field.

--------

## Chunk 85

**Chunk:**

Dwarkesh Patel
Why is a CCP bureaucrat, some 60-year-old guy, looking at this and going, "oh, Copilot has gotten better now" and now—
Leopold Aschenbrenner
This is much more than Copilot has gotten better now.
Dwarkesh Patel
It’d require shifting the production of an entire country, dislocating energy that is otherwise being used for consumer goods or something, and feeding all that into the data centers. Part of this whole story is that you realize superintelligence is coming soon. You realize it and maybe I realize it. I'm not sure how much I realize it.

Will the national security apparatus in the United States and the CCP realize it?

**Extracted Belief:**

The advancement of AI technology, particularly in the form of Copilot, represents a significant development beyond simply improved functionality. It signifies a larger trend with broader implications.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement about a CCP bureaucrat's perception of Copilot's improvement. Aschenbrenner suggests that this is not just a simple upgrade but a sign of a larger trend.

**Justification:**

The statement is based on Aschenbrenner's observation of the rapid progress in AI technology, particularly in areas like Copilot.

--------

## Chunk 86

**Chunk:**

Dwarkesh Patel
Why is a CCP bureaucrat, some 60-year-old guy, looking at this and going, "oh, Copilot has gotten better now" and now—
Leopold Aschenbrenner
This is much more than Copilot has gotten better now.
Dwarkesh Patel
It’d require shifting the production of an entire country, dislocating energy that is otherwise being used for consumer goods or something, and feeding all that into the data centers. Part of this whole story is that you realize superintelligence is coming soon. You realize it and maybe I realize it. I'm not sure how much I realize it.

Will the national security apparatus in the United States and the CCP realize it?

**Extracted Belief:**

The development of superintelligence is imminent and its arrival is a significant event that needs to be recognized and addressed.

**Context:**

Aschenbrenner's statement is in response to the idea of a CCP bureaucrat failing to understand the broader implications of AI advancements. He emphasizes the importance of acknowledging the coming of superintelligence.

**Justification:**

The belief is supported by Aschenbrenner's earlier statement about AI advancements being more than just improved functionality, suggesting a belief in the imminent arrival of superintelligence.

--------

## Chunk 87

**Chunk:**

Dwarkesh Patel
It’d require shifting the production of an entire country, dislocating energy that is otherwise being used for consumer goods or something, and feeding all that into the data centers. Part of this whole story is that you realize superintelligence is coming soon. You realize it and maybe I realize it. I'm not sure how much I realize it.

Will the national security apparatus in the United States and the CCP realize it?
Leopold Aschenbrenner
This is a really key question. We have a few more years of mid-game. We have a few more 2023s. That just starts updating more and more people. The trend lines will become clear. 

You will see some amount of the COVID dynamic. COVID in February of 2020 honestly feels a lot like today. It feels like this utterly crazy thing is coming. You see the exponential and yet most of the world just doesn't realize it. The mayor of New York is like, "go out to the shows," and "this is just Asian racism." At some point, people saw it and then crazy, radical reactions came.
Dwarkesh Patel
By the way, what were you doing during COVID? Was it your freshman or sophomore year?

**Extracted Belief:**

The development of superintelligence is imminent and will have significant impact on the world.

**Context:**

In response to the question of whether the national security apparatus and the CCP realize the imminent threat of superintelligence, Leopold Aschenbrenner states that there are a few more years of "mid-game" before the trend lines become clear and more people understand the implications.

**Justification:**

Aschenbrenner's statement is based on his understanding of the current trends in AI development and his belief that superintelligence is on the horizon. He draws an analogy to the early stages of the COVID pandemic, where the exponential growth of the virus was not immediately recognized by the public. He believes that a similar awakening is coming regarding superintelligence.

--------

## Chunk 88

**Chunk:**

Dwarkesh Patel
By the way, what were you doing during COVID? Was it your freshman or sophomore year?
Leopold Aschenbrenner
Junior.
Dwarkesh Patel
Still, you were like a 17-year-old junior or something right? Did you short the market or something? Did you sell at the right time?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 89

**Chunk:**

Dwarkesh Patel
Still, you were like a 17-year-old junior or something right? Did you short the market or something? Did you sell at the right time?
Leopold Aschenbrenner
Yeah.
Dwarkesh Patel
So there will be a March 2020 moment.

You can make the analogy you make in the series that this will cause a reaction like, “we have to do the Manhattan Project again for America here.” I wonder what the politics of this will be like. The difference here is that it’s not just like, “we need the bomb to beat the Nazis.”

We'll be building this thing that makes all our energy prices go up a bunch and it's automating a lot of our jobs. The climate change stuff people are going to be like, "oh, my God, it's making climate change worse and it's helping Big Tech."

Politically, this doesn't seem like a dynamic where the national security apparatus or the president is like, "we have to step on the gas here and make sure America wins."

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 90

**Chunk:**

Dwarkesh Patel
So there will be a March 2020 moment.

You can make the analogy you make in the series that this will cause a reaction like, “we have to do the Manhattan Project again for America here.” I wonder what the politics of this will be like. The difference here is that it’s not just like, “we need the bomb to beat the Nazis.”

We'll be building this thing that makes all our energy prices go up a bunch and it's automating a lot of our jobs. The climate change stuff people are going to be like, "oh, my God, it's making climate change worse and it's helping Big Tech."

Politically, this doesn't seem like a dynamic where the national security apparatus or the president is like, "we have to step on the gas here and make sure America wins."
Leopold Aschenbrenner
Again, a lot of this really depends on how much people are feeling it and how much people are seeing it. Our generation is so used to peace, American hegemony and nothing matters. The historical norm is very much one of extremely intense and extraordinary things happening in the world with intense international competition.

There's a 20-year very unique period. In World War II, something like 50% of GDP went to war production. The US borrowed over 60% of GDP. With Germany and Japan I think it was over 100%. In World War I, the UK, France, and Germany all borrowed over 100% of GDP.

Much more was on the line. People talk about World War I being so destructive with 20 million Soviet soldiers dying and 20% of Poland. That happened all the time. During the Seven Years' War something like 20-30% of Prussia died. In the Thirty Years' War, up to 50% of a large swath of Germany died.

Will people see that the stakes here are really high and that history is actually back? The American national security state thinks very seriously about stuff like this. They think very seriously about competition with China. China very much thinks of itself on this historical mission of the rejuvenation of the Chinese nation. They think a lot about national power. They think a lot about the world order.

There's a real question on timing. Do they start taking this seriously when the intelligence explosion is already happening quite late. Do they start taking this seriously two years earlier? That matters a lot for how things play out.

At some point they will and they will realize that this will be utterly decisive for not just some proxy war but for major questions. Can liberal democracy continue to thrive? Can the CCP continue existing? That will activate forces that we haven't seen in a long time.
Dwarkesh Patel
The great power conflict definitely seems compelling. All kinds of different things seem much more likely when you think from a historical perspective. You zoom out beyond the liberal democracy that we’ve had the pleasure to live in America for say the last 80 years. That includes things like dictatorships, war, famine, etc.

I was reading The Gulag Archipelago and one of the chapters begins with Solzhenitsyn saying how if you had told a Russian citizen under the tsars that because of all these new technologies — we wouldn’t see some Great Russian revival with Russia becoming a great power and the citizens made wealthy — you would see tens of millions of Soviet citizens tortured by millions of beasts in the worst possible ways. If you’d told them that that would be the result of the 20th century, they wouldn’t have believed you. They’d have called you a slanderer.

**Extracted Belief:**

The current generation is accustomed to a prolonged period of peace, American dominance, and a lack of significant global events.

**Context:**

Leopold Aschenbrenner is contrasting the current state of relative peace and American hegemony with historical periods of intense conflict.

**Justification:**

Aschenbrenner mentions that the current generation is used to peace and American hegemony, implying this is an observable reality.

--------

## Chunk 91

**Chunk:**

Dwarkesh Patel
So there will be a March 2020 moment.

You can make the analogy you make in the series that this will cause a reaction like, “we have to do the Manhattan Project again for America here.” I wonder what the politics of this will be like. The difference here is that it’s not just like, “we need the bomb to beat the Nazis.”

We'll be building this thing that makes all our energy prices go up a bunch and it's automating a lot of our jobs. The climate change stuff people are going to be like, "oh, my God, it's making climate change worse and it's helping Big Tech."

Politically, this doesn't seem like a dynamic where the national security apparatus or the president is like, "we have to step on the gas here and make sure America wins."
Leopold Aschenbrenner
Again, a lot of this really depends on how much people are feeling it and how much people are seeing it. Our generation is so used to peace, American hegemony and nothing matters. The historical norm is very much one of extremely intense and extraordinary things happening in the world with intense international competition.

There's a 20-year very unique period. In World War II, something like 50% of GDP went to war production. The US borrowed over 60% of GDP. With Germany and Japan I think it was over 100%. In World War I, the UK, France, and Germany all borrowed over 100% of GDP.

Much more was on the line. People talk about World War I being so destructive with 20 million Soviet soldiers dying and 20% of Poland. That happened all the time. During the Seven Years' War something like 20-30% of Prussia died. In the Thirty Years' War, up to 50% of a large swath of Germany died.

Will people see that the stakes here are really high and that history is actually back? The American national security state thinks very seriously about stuff like this. They think very seriously about competition with China. China very much thinks of itself on this historical mission of the rejuvenation of the Chinese nation. They think a lot about national power. They think a lot about the world order.

There's a real question on timing. Do they start taking this seriously when the intelligence explosion is already happening quite late. Do they start taking this seriously two years earlier? That matters a lot for how things play out.

At some point they will and they will realize that this will be utterly decisive for not just some proxy war but for major questions. Can liberal democracy continue to thrive? Can the CCP continue existing? That will activate forces that we haven't seen in a long time.
Dwarkesh Patel
The great power conflict definitely seems compelling. All kinds of different things seem much more likely when you think from a historical perspective. You zoom out beyond the liberal democracy that we’ve had the pleasure to live in America for say the last 80 years. That includes things like dictatorships, war, famine, etc.

I was reading The Gulag Archipelago and one of the chapters begins with Solzhenitsyn saying how if you had told a Russian citizen under the tsars that because of all these new technologies — we wouldn’t see some Great Russian revival with Russia becoming a great power and the citizens made wealthy — you would see tens of millions of Soviet citizens tortured by millions of beasts in the worst possible ways. If you’d told them that that would be the result of the 20th century, they wouldn’t have believed you. They’d have called you a slanderer.

**Extracted Belief:**

Historically, the world has experienced periods of intense conflict and competition.

**Context:**

Aschenbrenner is comparing the current situation to historical norms, highlighting the potential for future conflict.

**Justification:**

Aschenbrenner provides specific examples from history such as World Wars I and II, the Seven Years' War, and the Thirty Years' War.

--------

## Chunk 92

**Chunk:**

Dwarkesh Patel
So there will be a March 2020 moment.

You can make the analogy you make in the series that this will cause a reaction like, “we have to do the Manhattan Project again for America here.” I wonder what the politics of this will be like. The difference here is that it’s not just like, “we need the bomb to beat the Nazis.”

We'll be building this thing that makes all our energy prices go up a bunch and it's automating a lot of our jobs. The climate change stuff people are going to be like, "oh, my God, it's making climate change worse and it's helping Big Tech."

Politically, this doesn't seem like a dynamic where the national security apparatus or the president is like, "we have to step on the gas here and make sure America wins."
Leopold Aschenbrenner
Again, a lot of this really depends on how much people are feeling it and how much people are seeing it. Our generation is so used to peace, American hegemony and nothing matters. The historical norm is very much one of extremely intense and extraordinary things happening in the world with intense international competition.

There's a 20-year very unique period. In World War II, something like 50% of GDP went to war production. The US borrowed over 60% of GDP. With Germany and Japan I think it was over 100%. In World War I, the UK, France, and Germany all borrowed over 100% of GDP.

Much more was on the line. People talk about World War I being so destructive with 20 million Soviet soldiers dying and 20% of Poland. That happened all the time. During the Seven Years' War something like 20-30% of Prussia died. In the Thirty Years' War, up to 50% of a large swath of Germany died.

Will people see that the stakes here are really high and that history is actually back? The American national security state thinks very seriously about stuff like this. They think very seriously about competition with China. China very much thinks of itself on this historical mission of the rejuvenation of the Chinese nation. They think a lot about national power. They think a lot about the world order.

There's a real question on timing. Do they start taking this seriously when the intelligence explosion is already happening quite late. Do they start taking this seriously two years earlier? That matters a lot for how things play out.

At some point they will and they will realize that this will be utterly decisive for not just some proxy war but for major questions. Can liberal democracy continue to thrive? Can the CCP continue existing? That will activate forces that we haven't seen in a long time.
Dwarkesh Patel
The great power conflict definitely seems compelling. All kinds of different things seem much more likely when you think from a historical perspective. You zoom out beyond the liberal democracy that we’ve had the pleasure to live in America for say the last 80 years. That includes things like dictatorships, war, famine, etc.

I was reading The Gulag Archipelago and one of the chapters begins with Solzhenitsyn saying how if you had told a Russian citizen under the tsars that because of all these new technologies — we wouldn’t see some Great Russian revival with Russia becoming a great power and the citizens made wealthy — you would see tens of millions of Soviet citizens tortured by millions of beasts in the worst possible ways. If you’d told them that that would be the result of the 20th century, they wouldn’t have believed you. They’d have called you a slanderer.

**Extracted Belief:**

The US national security state takes international competition with China seriously.

**Context:**

Aschenbrenner is expressing his understanding of the US's strategic outlook.

**Justification:**

Aschenbrenner states that the US national security state thinks 'very seriously' about competition with China.

--------

## Chunk 93

**Chunk:**

Dwarkesh Patel
So there will be a March 2020 moment.

You can make the analogy you make in the series that this will cause a reaction like, “we have to do the Manhattan Project again for America here.” I wonder what the politics of this will be like. The difference here is that it’s not just like, “we need the bomb to beat the Nazis.”

We'll be building this thing that makes all our energy prices go up a bunch and it's automating a lot of our jobs. The climate change stuff people are going to be like, "oh, my God, it's making climate change worse and it's helping Big Tech."

Politically, this doesn't seem like a dynamic where the national security apparatus or the president is like, "we have to step on the gas here and make sure America wins."
Leopold Aschenbrenner
Again, a lot of this really depends on how much people are feeling it and how much people are seeing it. Our generation is so used to peace, American hegemony and nothing matters. The historical norm is very much one of extremely intense and extraordinary things happening in the world with intense international competition.

There's a 20-year very unique period. In World War II, something like 50% of GDP went to war production. The US borrowed over 60% of GDP. With Germany and Japan I think it was over 100%. In World War I, the UK, France, and Germany all borrowed over 100% of GDP.

Much more was on the line. People talk about World War I being so destructive with 20 million Soviet soldiers dying and 20% of Poland. That happened all the time. During the Seven Years' War something like 20-30% of Prussia died. In the Thirty Years' War, up to 50% of a large swath of Germany died.

Will people see that the stakes here are really high and that history is actually back? The American national security state thinks very seriously about stuff like this. They think very seriously about competition with China. China very much thinks of itself on this historical mission of the rejuvenation of the Chinese nation. They think a lot about national power. They think a lot about the world order.

There's a real question on timing. Do they start taking this seriously when the intelligence explosion is already happening quite late. Do they start taking this seriously two years earlier? That matters a lot for how things play out.

At some point they will and they will realize that this will be utterly decisive for not just some proxy war but for major questions. Can liberal democracy continue to thrive? Can the CCP continue existing? That will activate forces that we haven't seen in a long time.
Dwarkesh Patel
The great power conflict definitely seems compelling. All kinds of different things seem much more likely when you think from a historical perspective. You zoom out beyond the liberal democracy that we’ve had the pleasure to live in America for say the last 80 years. That includes things like dictatorships, war, famine, etc.

I was reading The Gulag Archipelago and one of the chapters begins with Solzhenitsyn saying how if you had told a Russian citizen under the tsars that because of all these new technologies — we wouldn’t see some Great Russian revival with Russia becoming a great power and the citizens made wealthy — you would see tens of millions of Soviet citizens tortured by millions of beasts in the worst possible ways. If you’d told them that that would be the result of the 20th century, they wouldn’t have believed you. They’d have called you a slanderer.

**Extracted Belief:**

China is on a historical mission to revive its national power and influence.

**Context:**

Aschenbrenner is sharing his knowledge of China's strategic goals.

**Justification:**

Aschenbrenner mentions that China views itself as on a 'historical mission' of national rejuvenation, indicating their focus on national power and world order.

--------

## Chunk 94

**Chunk:**

Dwarkesh Patel
So there will be a March 2020 moment.

You can make the analogy you make in the series that this will cause a reaction like, “we have to do the Manhattan Project again for America here.” I wonder what the politics of this will be like. The difference here is that it’s not just like, “we need the bomb to beat the Nazis.”

We'll be building this thing that makes all our energy prices go up a bunch and it's automating a lot of our jobs. The climate change stuff people are going to be like, "oh, my God, it's making climate change worse and it's helping Big Tech."

Politically, this doesn't seem like a dynamic where the national security apparatus or the president is like, "we have to step on the gas here and make sure America wins."
Leopold Aschenbrenner
Again, a lot of this really depends on how much people are feeling it and how much people are seeing it. Our generation is so used to peace, American hegemony and nothing matters. The historical norm is very much one of extremely intense and extraordinary things happening in the world with intense international competition.

There's a 20-year very unique period. In World War II, something like 50% of GDP went to war production. The US borrowed over 60% of GDP. With Germany and Japan I think it was over 100%. In World War I, the UK, France, and Germany all borrowed over 100% of GDP.

Much more was on the line. People talk about World War I being so destructive with 20 million Soviet soldiers dying and 20% of Poland. That happened all the time. During the Seven Years' War something like 20-30% of Prussia died. In the Thirty Years' War, up to 50% of a large swath of Germany died.

Will people see that the stakes here are really high and that history is actually back? The American national security state thinks very seriously about stuff like this. They think very seriously about competition with China. China very much thinks of itself on this historical mission of the rejuvenation of the Chinese nation. They think a lot about national power. They think a lot about the world order.

There's a real question on timing. Do they start taking this seriously when the intelligence explosion is already happening quite late. Do they start taking this seriously two years earlier? That matters a lot for how things play out.

At some point they will and they will realize that this will be utterly decisive for not just some proxy war but for major questions. Can liberal democracy continue to thrive? Can the CCP continue existing? That will activate forces that we haven't seen in a long time.
Dwarkesh Patel
The great power conflict definitely seems compelling. All kinds of different things seem much more likely when you think from a historical perspective. You zoom out beyond the liberal democracy that we’ve had the pleasure to live in America for say the last 80 years. That includes things like dictatorships, war, famine, etc.

I was reading The Gulag Archipelago and one of the chapters begins with Solzhenitsyn saying how if you had told a Russian citizen under the tsars that because of all these new technologies — we wouldn’t see some Great Russian revival with Russia becoming a great power and the citizens made wealthy — you would see tens of millions of Soviet citizens tortured by millions of beasts in the worst possible ways. If you’d told them that that would be the result of the 20th century, they wouldn’t have believed you. They’d have called you a slanderer.

**Extracted Belief:**

The timing of the intelligence explosion will be a significant factor in shaping future events.

**Context:**

Aschenbrenner is discussing the potential consequences of the intelligence explosion.

**Justification:**

He argues that the speed at which the intelligence explosion unfolds will influence how actors react, potentially leading to different outcomes.

--------

## Chunk 95

**Chunk:**

Dwarkesh Patel
So there will be a March 2020 moment.

You can make the analogy you make in the series that this will cause a reaction like, “we have to do the Manhattan Project again for America here.” I wonder what the politics of this will be like. The difference here is that it’s not just like, “we need the bomb to beat the Nazis.”

We'll be building this thing that makes all our energy prices go up a bunch and it's automating a lot of our jobs. The climate change stuff people are going to be like, "oh, my God, it's making climate change worse and it's helping Big Tech."

Politically, this doesn't seem like a dynamic where the national security apparatus or the president is like, "we have to step on the gas here and make sure America wins."
Leopold Aschenbrenner
Again, a lot of this really depends on how much people are feeling it and how much people are seeing it. Our generation is so used to peace, American hegemony and nothing matters. The historical norm is very much one of extremely intense and extraordinary things happening in the world with intense international competition.

There's a 20-year very unique period. In World War II, something like 50% of GDP went to war production. The US borrowed over 60% of GDP. With Germany and Japan I think it was over 100%. In World War I, the UK, France, and Germany all borrowed over 100% of GDP.

Much more was on the line. People talk about World War I being so destructive with 20 million Soviet soldiers dying and 20% of Poland. That happened all the time. During the Seven Years' War something like 20-30% of Prussia died. In the Thirty Years' War, up to 50% of a large swath of Germany died.

Will people see that the stakes here are really high and that history is actually back? The American national security state thinks very seriously about stuff like this. They think very seriously about competition with China. China very much thinks of itself on this historical mission of the rejuvenation of the Chinese nation. They think a lot about national power. They think a lot about the world order.

There's a real question on timing. Do they start taking this seriously when the intelligence explosion is already happening quite late. Do they start taking this seriously two years earlier? That matters a lot for how things play out.

At some point they will and they will realize that this will be utterly decisive for not just some proxy war but for major questions. Can liberal democracy continue to thrive? Can the CCP continue existing? That will activate forces that we haven't seen in a long time.
Dwarkesh Patel
The great power conflict definitely seems compelling. All kinds of different things seem much more likely when you think from a historical perspective. You zoom out beyond the liberal democracy that we’ve had the pleasure to live in America for say the last 80 years. That includes things like dictatorships, war, famine, etc.

I was reading The Gulag Archipelago and one of the chapters begins with Solzhenitsyn saying how if you had told a Russian citizen under the tsars that because of all these new technologies — we wouldn’t see some Great Russian revival with Russia becoming a great power and the citizens made wealthy — you would see tens of millions of Soviet citizens tortured by millions of beasts in the worst possible ways. If you’d told them that that would be the result of the 20th century, they wouldn’t have believed you. They’d have called you a slanderer.

**Extracted Belief:**

The development of superintelligence will have major consequences for global power dynamics.

**Context:**

Aschenbrenner is discussing the potential impact of superintelligence on international relations.

**Justification:**

He argues that superintelligence will be 'utterly decisive' for key questions like the future of liberal democracy and the existence of the CCP.

--------

## Chunk 96

**Chunk:**

Dwarkesh Patel
The great power conflict definitely seems compelling. All kinds of different things seem much more likely when you think from a historical perspective. You zoom out beyond the liberal democracy that we’ve had the pleasure to live in America for say the last 80 years. That includes things like dictatorships, war, famine, etc.

I was reading The Gulag Archipelago and one of the chapters begins with Solzhenitsyn saying how if you had told a Russian citizen under the tsars that because of all these new technologies — we wouldn’t see some Great Russian revival with Russia becoming a great power and the citizens made wealthy — you would see tens of millions of Soviet citizens tortured by millions of beasts in the worst possible ways. If you’d told them that that would be the result of the 20th century, they wouldn’t have believed you. They’d have called you a slanderer.
Leopold Aschenbrenner
The possibilities for dictatorship with superintelligence are even crazier as well. Imagine you have a perfectly loyal military and security force. No more rebellions. No more popular uprisings. You have perfect lie detection. You have surveillance of everybody. You can perfectly figure out who's the dissenter and weed them out. No Gorbachev who had some doubts about the system would have ever risen to power. No military coup would have ever happened.

There's a real way in which part of why things have worked out is that ideas can evolve. There's some sense in which time heals a lot of wounds and solves a lot of debates. Throughout time, a lot of people had really strong convictions, but a lot of those have been overturned over time because there's been continued pluralism and evolution.

Imagine applying a CCP-like approach to truth where truth is what the party says. When you supercharge that with superintelligence, that could just be locked in and enshrined for a long time. The possibilities are pretty terrifying.

To your point about history and living in America for the past eight years, this is one of the things I took away from growing up in Germany. A lot of this stuff feels more visceral. My mother grew up in the former East, my father in the former West. They met shortly after the Wall fell. The end of the Cold War was this extremely pivotal moment for me because it's the reason I exist.

I grew up in Berlin with the former Wall. My great-grandmother, who is still alive, is very important in my life. She was born in 1934 and grew up during the Nazi era. In World War II, she saw the firebombing of Dresden from this country cottage where they were as kids. Then she spent most of her life in the East German communist dictatorship.

She'd tell me about how Soviet tanks came when there was the popular uprising in 1954. Her husband was telling her to get home really quickly and get off the streets. She had a son who tried to ride a motorcycle across the Iron Curtain and then was put in a Stasi prison for a while. Finally, when she's almost 60, it was the first time she lived in a free country, and a wealthy country.

When I was a kid, the thing she always really didn't want me to do was get involved in politics. Joining a political party had very bad connotations for her. She raised me when I was young. So it doesn't feel that long ago. It feels very close.
Dwarkesh Patel
There’s one thing I wonder about when we're talking today about the CCP. The people in China who will be doing their version of this project will be AI researchers who are somewhat Westernized. They’ll either have gotten educated in the West or have colleagues in the West.

Are they going to sign up for the CCP project that's going to hand over control to Xi Jinping? What's your sense of that? Fundamentally, they're just people, right? Can't you convince them about the dangers of superintelligence?

**Extracted Belief:**

The development of superintelligence could lead to the establishment of a dictatorship with a perfectly loyal military and security force.

**Context:**

Leopold Aschenbrenner discusses the potential dangers of superintelligence in the context of its ability to create a perfectly controlled and secure state.

**Justification:**

Aschenbrenner argues that superintelligence could enable perfect lie detection, surveillance, and identification of dissenters, eliminating the possibility of rebellion or dissent.

--------

## Chunk 97

**Chunk:**

Dwarkesh Patel
The great power conflict definitely seems compelling. All kinds of different things seem much more likely when you think from a historical perspective. You zoom out beyond the liberal democracy that we’ve had the pleasure to live in America for say the last 80 years. That includes things like dictatorships, war, famine, etc.

I was reading The Gulag Archipelago and one of the chapters begins with Solzhenitsyn saying how if you had told a Russian citizen under the tsars that because of all these new technologies — we wouldn’t see some Great Russian revival with Russia becoming a great power and the citizens made wealthy — you would see tens of millions of Soviet citizens tortured by millions of beasts in the worst possible ways. If you’d told them that that would be the result of the 20th century, they wouldn’t have believed you. They’d have called you a slanderer.
Leopold Aschenbrenner
The possibilities for dictatorship with superintelligence are even crazier as well. Imagine you have a perfectly loyal military and security force. No more rebellions. No more popular uprisings. You have perfect lie detection. You have surveillance of everybody. You can perfectly figure out who's the dissenter and weed them out. No Gorbachev who had some doubts about the system would have ever risen to power. No military coup would have ever happened.

There's a real way in which part of why things have worked out is that ideas can evolve. There's some sense in which time heals a lot of wounds and solves a lot of debates. Throughout time, a lot of people had really strong convictions, but a lot of those have been overturned over time because there's been continued pluralism and evolution.

Imagine applying a CCP-like approach to truth where truth is what the party says. When you supercharge that with superintelligence, that could just be locked in and enshrined for a long time. The possibilities are pretty terrifying.

To your point about history and living in America for the past eight years, this is one of the things I took away from growing up in Germany. A lot of this stuff feels more visceral. My mother grew up in the former East, my father in the former West. They met shortly after the Wall fell. The end of the Cold War was this extremely pivotal moment for me because it's the reason I exist.

I grew up in Berlin with the former Wall. My great-grandmother, who is still alive, is very important in my life. She was born in 1934 and grew up during the Nazi era. In World War II, she saw the firebombing of Dresden from this country cottage where they were as kids. Then she spent most of her life in the East German communist dictatorship.

She'd tell me about how Soviet tanks came when there was the popular uprising in 1954. Her husband was telling her to get home really quickly and get off the streets. She had a son who tried to ride a motorcycle across the Iron Curtain and then was put in a Stasi prison for a while. Finally, when she's almost 60, it was the first time she lived in a free country, and a wealthy country.

When I was a kid, the thing she always really didn't want me to do was get involved in politics. Joining a political party had very bad connotations for her. She raised me when I was young. So it doesn't feel that long ago. It feels very close.
Dwarkesh Patel
There’s one thing I wonder about when we're talking today about the CCP. The people in China who will be doing their version of this project will be AI researchers who are somewhat Westernized. They’ll either have gotten educated in the West or have colleagues in the West.

Are they going to sign up for the CCP project that's going to hand over control to Xi Jinping? What's your sense of that? Fundamentally, they're just people, right? Can't you convince them about the dangers of superintelligence?

**Extracted Belief:**

Superintelligence could lead to the consolidation of power in a CCP-like system where truth is defined by the ruling party.

**Context:**

Aschenbrenner emphasizes the potential for superintelligence to solidify an authoritarian regime by enforcing a rigid and unchanging definition of truth.

**Justification:**

Aschenbrenner uses the example of the CCP's control over truth to illustrate how superintelligence could be used to enforce and perpetuate a specific ideology.

--------

## Chunk 98

**Chunk:**

Dwarkesh Patel
The great power conflict definitely seems compelling. All kinds of different things seem much more likely when you think from a historical perspective. You zoom out beyond the liberal democracy that we’ve had the pleasure to live in America for say the last 80 years. That includes things like dictatorships, war, famine, etc.

I was reading The Gulag Archipelago and one of the chapters begins with Solzhenitsyn saying how if you had told a Russian citizen under the tsars that because of all these new technologies — we wouldn’t see some Great Russian revival with Russia becoming a great power and the citizens made wealthy — you would see tens of millions of Soviet citizens tortured by millions of beasts in the worst possible ways. If you’d told them that that would be the result of the 20th century, they wouldn’t have believed you. They’d have called you a slanderer.
Leopold Aschenbrenner
The possibilities for dictatorship with superintelligence are even crazier as well. Imagine you have a perfectly loyal military and security force. No more rebellions. No more popular uprisings. You have perfect lie detection. You have surveillance of everybody. You can perfectly figure out who's the dissenter and weed them out. No Gorbachev who had some doubts about the system would have ever risen to power. No military coup would have ever happened.

There's a real way in which part of why things have worked out is that ideas can evolve. There's some sense in which time heals a lot of wounds and solves a lot of debates. Throughout time, a lot of people had really strong convictions, but a lot of those have been overturned over time because there's been continued pluralism and evolution.

Imagine applying a CCP-like approach to truth where truth is what the party says. When you supercharge that with superintelligence, that could just be locked in and enshrined for a long time. The possibilities are pretty terrifying.

To your point about history and living in America for the past eight years, this is one of the things I took away from growing up in Germany. A lot of this stuff feels more visceral. My mother grew up in the former East, my father in the former West. They met shortly after the Wall fell. The end of the Cold War was this extremely pivotal moment for me because it's the reason I exist.

I grew up in Berlin with the former Wall. My great-grandmother, who is still alive, is very important in my life. She was born in 1934 and grew up during the Nazi era. In World War II, she saw the firebombing of Dresden from this country cottage where they were as kids. Then she spent most of her life in the East German communist dictatorship.

She'd tell me about how Soviet tanks came when there was the popular uprising in 1954. Her husband was telling her to get home really quickly and get off the streets. She had a son who tried to ride a motorcycle across the Iron Curtain and then was put in a Stasi prison for a while. Finally, when she's almost 60, it was the first time she lived in a free country, and a wealthy country.

When I was a kid, the thing she always really didn't want me to do was get involved in politics. Joining a political party had very bad connotations for her. She raised me when I was young. So it doesn't feel that long ago. It feels very close.
Dwarkesh Patel
There’s one thing I wonder about when we're talking today about the CCP. The people in China who will be doing their version of this project will be AI researchers who are somewhat Westernized. They’ll either have gotten educated in the West or have colleagues in the West.

Are they going to sign up for the CCP project that's going to hand over control to Xi Jinping? What's your sense of that? Fundamentally, they're just people, right? Can't you convince them about the dangers of superintelligence?

**Extracted Belief:**

The evolution of ideas and the resolution of debates are vital for societal progress.

**Context:**

Aschenbrenner contrasts the potential for superintelligence to freeze truth with the historical process of idea evolution and debate.

**Justification:**

Aschenbrenner argues that the ability of ideas to evolve and challenge established beliefs is crucial for progress, highlighting the importance of pluralism and intellectual freedom.

--------

## Chunk 99

**Chunk:**

Dwarkesh Patel
There’s one thing I wonder about when we're talking today about the CCP. The people in China who will be doing their version of this project will be AI researchers who are somewhat Westernized. They’ll either have gotten educated in the West or have colleagues in the West.

Are they going to sign up for the CCP project that's going to hand over control to Xi Jinping? What's your sense of that? Fundamentally, they're just people, right? Can't you convince them about the dangers of superintelligence?
Leopold Aschenbrenner
Will they be in charge though? In some sense, this is also the case in the US. This is like the rapidly depreciating influence of the lab employees. Right now, the AI lab employees have so much power. You saw this November event. It’s so much power.

Both are going to get automated and they're going to lose all their power. It'll just be a few people in charge with their armies of automated AIs. It’s also the politicians and the generals and the national security state. There are some of these classic scenes from the Oppenheimer movie. The scientists built it and then the bomb was shipped away and it was out of their hands.

It's good for lab employees to be aware of this. You have a lot of power now, but maybe not for that long. Use it wisely. I do think they would benefit from some more organs of representative democracy.
Dwarkesh Patel
What do you mean by that?

**Extracted Belief:**

AI lab employees currently have a lot of power in the field of AI.

**Context:**

Leopold Aschenbrenner is discussing the future of AI power structures in relation to the CCP's AI project and the potential for AI to take control.

**Justification:**

He points to the November event as evidence of the current power held by AI lab employees.

--------

## Chunk 100

**Chunk:**

Dwarkesh Patel
There’s one thing I wonder about when we're talking today about the CCP. The people in China who will be doing their version of this project will be AI researchers who are somewhat Westernized. They’ll either have gotten educated in the West or have colleagues in the West.

Are they going to sign up for the CCP project that's going to hand over control to Xi Jinping? What's your sense of that? Fundamentally, they're just people, right? Can't you convince them about the dangers of superintelligence?
Leopold Aschenbrenner
Will they be in charge though? In some sense, this is also the case in the US. This is like the rapidly depreciating influence of the lab employees. Right now, the AI lab employees have so much power. You saw this November event. It’s so much power.

Both are going to get automated and they're going to lose all their power. It'll just be a few people in charge with their armies of automated AIs. It’s also the politicians and the generals and the national security state. There are some of these classic scenes from the Oppenheimer movie. The scientists built it and then the bomb was shipped away and it was out of their hands.

It's good for lab employees to be aware of this. You have a lot of power now, but maybe not for that long. Use it wisely. I do think they would benefit from some more organs of representative democracy.
Dwarkesh Patel
What do you mean by that?

**Extracted Belief:**

AI lab employees will lose their power as AI automation advances.

**Context:**

Leopold Aschenbrenner is explaining the shift in power dynamics within the AI field as AI automation becomes more prevalent.

**Justification:**

He suggests that AI automation will lead to a scenario where a few individuals with access to automated AI systems will hold the most power.

--------

## Chunk 101

**Chunk:**

Dwarkesh Patel
There’s one thing I wonder about when we're talking today about the CCP. The people in China who will be doing their version of this project will be AI researchers who are somewhat Westernized. They’ll either have gotten educated in the West or have colleagues in the West.

Are they going to sign up for the CCP project that's going to hand over control to Xi Jinping? What's your sense of that? Fundamentally, they're just people, right? Can't you convince them about the dangers of superintelligence?
Leopold Aschenbrenner
Will they be in charge though? In some sense, this is also the case in the US. This is like the rapidly depreciating influence of the lab employees. Right now, the AI lab employees have so much power. You saw this November event. It’s so much power.

Both are going to get automated and they're going to lose all their power. It'll just be a few people in charge with their armies of automated AIs. It’s also the politicians and the generals and the national security state. There are some of these classic scenes from the Oppenheimer movie. The scientists built it and then the bomb was shipped away and it was out of their hands.

It's good for lab employees to be aware of this. You have a lot of power now, but maybe not for that long. Use it wisely. I do think they would benefit from some more organs of representative democracy.
Dwarkesh Patel
What do you mean by that?

**Extracted Belief:**

Representative democracy is beneficial for AI development.

**Context:**

Leopold Aschenbrenner is advocating for more representative structures in AI development to balance the power dynamics.

**Justification:**

He cites the OpenAI board events as an example of how direct democracy can lead to positive outcomes.

--------

## Chunk 102

**Chunk:**

Dwarkesh Patel
What do you mean by that?
Leopold Aschenbrenner
In the OpenAI board events, employee power is exercised in a very direct democracy way. How some of that went about really highlighted the benefits of representative democracy and having some deliberative organs.

(00:40:26) – Espionage & American AI superiority
Dwarkesh Patel
Interesting. Let's go back to the $100 billion revenue question. The companies are trying to build clusters that are this big. Where are they building it? Say it's the amount of energy that would be required for a small or medium-sized US state. Does Colorado then get no power because it's happening in the United States? Is it happening somewhere else?

**Extracted Belief:**

Representative democracy is beneficial and involves deliberative organs.

**Context:**

Leopold Aschenbrenner was responding to Dwarkesh Patel's question about the meaning of his previous statement, specifically referencing the OpenAI board events and the power dynamics at play.

**Justification:**

Aschenbrenner points out that the events at OpenAI demonstrated the benefits of representative democracy and deliberative organs, implying a positive view of these systems based on his observation of the events.

--------

## Chunk 103

**Chunk:**

Dwarkesh Patel
Interesting. Let's go back to the $100 billion revenue question. The companies are trying to build clusters that are this big. Where are they building it? Say it's the amount of energy that would be required for a small or medium-sized US state. Does Colorado then get no power because it's happening in the United States? Is it happening somewhere else?
Leopold Aschenbrenner
This is the thing that I always find funny, when you talk about Colorado getting no power. The easy way to get the power would be to displace less economically useful stuff. Buy up the aluminum smelting plant that has a gigawatt. We're going to replace it with the data center because that's important. That's not actually happening because a lot of these power contracts are really locked in long-term. Also, people don't like things like this.

In practice what it requires, at least right now, is building new power. That might change. That's when things get really interesting, when it's like, “no, we're just dedicating all of the power to the AGI.”

So right now it's building new power. 10 GW is quite doable. It's like a few percent of US natural gas production. When you have the 10 GW training cluster, you have a lot more inference. 100 gigawatts is where it starts getting pretty wild. That's over 20% of US electricity production. It's pretty doable, especially if you're willing to go for natural gas.

It is incredibly important that these clusters are in the United States.
Dwarkesh Patel
Why does it matter that it's in the US?

**Extracted Belief:**

Building new power plants is necessary to provide enough electricity for large-scale AI training clusters.

**Context:**

Leopold Aschenbrenner explains that while it's theoretically possible to displace less economically useful energy consumption, existing power contracts and public resistance make building new power infrastructure the most practical option for powering large AI clusters.

**Justification:**

Leopold Aschenbrenner mentions that current power contracts and public resistance hinder the displacement of less economically useful energy consumption. He then clarifies that building new power infrastructure is the current approach to provide enough electricity for AI clusters.

--------

## Chunk 104

**Chunk:**

Dwarkesh Patel
Interesting. Let's go back to the $100 billion revenue question. The companies are trying to build clusters that are this big. Where are they building it? Say it's the amount of energy that would be required for a small or medium-sized US state. Does Colorado then get no power because it's happening in the United States? Is it happening somewhere else?
Leopold Aschenbrenner
This is the thing that I always find funny, when you talk about Colorado getting no power. The easy way to get the power would be to displace less economically useful stuff. Buy up the aluminum smelting plant that has a gigawatt. We're going to replace it with the data center because that's important. That's not actually happening because a lot of these power contracts are really locked in long-term. Also, people don't like things like this.

In practice what it requires, at least right now, is building new power. That might change. That's when things get really interesting, when it's like, “no, we're just dedicating all of the power to the AGI.”

So right now it's building new power. 10 GW is quite doable. It's like a few percent of US natural gas production. When you have the 10 GW training cluster, you have a lot more inference. 100 gigawatts is where it starts getting pretty wild. That's over 20% of US electricity production. It's pretty doable, especially if you're willing to go for natural gas.

It is incredibly important that these clusters are in the United States.
Dwarkesh Patel
Why does it matter that it's in the US?

**Extracted Belief:**

Building AI clusters in the United States is crucial for national security.

**Context:**

Leopold Aschenbrenner emphasizes the importance of building AI clusters in the US, highlighting the national security risks of placing them in authoritarian countries.

**Justification:**

Leopold Aschenbrenner states that building AI clusters in authoritarian countries poses a national security risk due to the potential for unauthorized access and exploitation of the technology.

--------

## Chunk 105

**Chunk:**

Dwarkesh Patel
Why does it matter that it's in the US?
Leopold Aschenbrenner
There are some people who are trying to build clusters elsewhere. There's a lot of free-flowing Middle Eastern money that's trying to build clusters elsewhere. This comes back to the national security question we talked about. Would you do the Manhattan Project in the UAE?

You can put the clusters in the US and you can put them in allied democracies. Once you put them in authoritarian dictatorships, you create this irreversible security risk. Once the cluster is there, it's much easier for them to exfiltrate the weights. They can literally steal the AGI, the superintelligence. It’s like they got a direct copy of the atomic bomb. It makes it much easier for them. They have weird ties to China. They can ship that to China. That's a huge risk.

Another thing is they can just seize the compute. The issue here is people right now are thinking of this as ChatGPT, Big Tech product clusters. The clusters being planned now, three to five years out, may well be the AGI, superintelligence clusters. When things get hot, they might just seize the compute.

Suppose we put 25% of the compute capacity in these Middle Eastern dictatorships. Say they seize that. Now it's a ratio of compute of 3:1. We still have more, but even with only 25% of compute there it starts getting pretty hairy. 3:1 is not that great of a ratio. You can do a lot with that amount of compute.

Say they don't actually do this. Even if they don't actually seize the compute, even if they actually don't steal the weights, there's just a lot of implicit leverage you get. They get seats at the AGI table. I don't know why we're giving authoritarian dictatorships the seat at the AGI table.
Dwarkesh Patel
There's going to be a lot of compute in the Middle East if these deals go through.

First of all, who is it? Is it just every single Big Tech company trying to figure it out over there?

**Extracted Belief:**

Placing artificial general intelligence (AGI) superintelligence clusters in authoritarian dictatorships poses a significant security risk, as these entities could easily exfiltrate the underlying weights and steal the AGI, similar to acquiring a copy of the atomic bomb.

**Context:**

Leopold Aschenbrenner expresses concern regarding the potential security risks associated with building AGI clusters in authoritarian regimes, citing the possibility of stolen weights and exfiltration.

**Justification:**

He draws a parallel between stealing the AGI and acquiring the atomic bomb, suggesting that the potential consequences are significant and comparable to the proliferation of nuclear weapons.

--------

## Chunk 106

**Chunk:**

Dwarkesh Patel
Why does it matter that it's in the US?
Leopold Aschenbrenner
There are some people who are trying to build clusters elsewhere. There's a lot of free-flowing Middle Eastern money that's trying to build clusters elsewhere. This comes back to the national security question we talked about. Would you do the Manhattan Project in the UAE?

You can put the clusters in the US and you can put them in allied democracies. Once you put them in authoritarian dictatorships, you create this irreversible security risk. Once the cluster is there, it's much easier for them to exfiltrate the weights. They can literally steal the AGI, the superintelligence. It’s like they got a direct copy of the atomic bomb. It makes it much easier for them. They have weird ties to China. They can ship that to China. That's a huge risk.

Another thing is they can just seize the compute. The issue here is people right now are thinking of this as ChatGPT, Big Tech product clusters. The clusters being planned now, three to five years out, may well be the AGI, superintelligence clusters. When things get hot, they might just seize the compute.

Suppose we put 25% of the compute capacity in these Middle Eastern dictatorships. Say they seize that. Now it's a ratio of compute of 3:1. We still have more, but even with only 25% of compute there it starts getting pretty hairy. 3:1 is not that great of a ratio. You can do a lot with that amount of compute.

Say they don't actually do this. Even if they don't actually seize the compute, even if they actually don't steal the weights, there's just a lot of implicit leverage you get. They get seats at the AGI table. I don't know why we're giving authoritarian dictatorships the seat at the AGI table.
Dwarkesh Patel
There's going to be a lot of compute in the Middle East if these deals go through.

First of all, who is it? Is it just every single Big Tech company trying to figure it out over there?

**Extracted Belief:**

Authoritarian dictatorships could seize control of AGI compute clusters, potentially leading to a scenario where they have a significant advantage in computing power.

**Context:**

Leopold Aschenbrenner warns against the possibility of authoritarian regimes seizing control of AGI compute clusters, potentially leading to an imbalance in computing power and a strategic disadvantage for other nations.

**Justification:**

He provides a hypothetical scenario where a regime seizes 25% of the global compute capacity, resulting in a 3:1 ratio in favor of the seizing entity, highlighting the potential for a significant imbalance.

--------

## Chunk 107

**Chunk:**

Dwarkesh Patel
Why does it matter that it's in the US?
Leopold Aschenbrenner
There are some people who are trying to build clusters elsewhere. There's a lot of free-flowing Middle Eastern money that's trying to build clusters elsewhere. This comes back to the national security question we talked about. Would you do the Manhattan Project in the UAE?

You can put the clusters in the US and you can put them in allied democracies. Once you put them in authoritarian dictatorships, you create this irreversible security risk. Once the cluster is there, it's much easier for them to exfiltrate the weights. They can literally steal the AGI, the superintelligence. It’s like they got a direct copy of the atomic bomb. It makes it much easier for them. They have weird ties to China. They can ship that to China. That's a huge risk.

Another thing is they can just seize the compute. The issue here is people right now are thinking of this as ChatGPT, Big Tech product clusters. The clusters being planned now, three to five years out, may well be the AGI, superintelligence clusters. When things get hot, they might just seize the compute.

Suppose we put 25% of the compute capacity in these Middle Eastern dictatorships. Say they seize that. Now it's a ratio of compute of 3:1. We still have more, but even with only 25% of compute there it starts getting pretty hairy. 3:1 is not that great of a ratio. You can do a lot with that amount of compute.

Say they don't actually do this. Even if they don't actually seize the compute, even if they actually don't steal the weights, there's just a lot of implicit leverage you get. They get seats at the AGI table. I don't know why we're giving authoritarian dictatorships the seat at the AGI table.
Dwarkesh Patel
There's going to be a lot of compute in the Middle East if these deals go through.

First of all, who is it? Is it just every single Big Tech company trying to figure it out over there?

**Extracted Belief:**

Authoritarian dictatorships, even without seizing control of AGI compute clusters, gain significant leverage and influence by being involved in the development of AGI.

**Context:**

Leopold Aschenbrenner argues against involving authoritarian regimes in the development of AGI, even if they don't directly seize control of compute clusters, because their involvement grants them a level of influence and leverage that he deems inappropriate.

**Justification:**

He states that their involvement provides them with a 'seat at the AGI table', implying that they gain influence and potentially compromise the ethical development and deployment of AGI.

--------

## Chunk 108

**Chunk:**

Dwarkesh Patel
There's going to be a lot of compute in the Middle East if these deals go through.

First of all, who is it? Is it just every single Big Tech company trying to figure it out over there?
Leopold Aschenbrenner
It’s not everybody, some.
Dwarkesh Patel
There are reports, I think Microsoft. We'll get into it.

So say the UAE gets a bunch of compute because we're building the clusters there. Let's say they have 25% of the compute. Why does a compute ratio matter? If it's about them being able to kick off the intelligence explosion, isn't it just some threshold where you have 100 million AI researchers or you don't?

**Extracted Belief:**

Not all big tech companies are trying to build computing clusters in the Middle East.

**Context:**

Dwarkesh Patel asks if all big tech companies are trying to build computing clusters in the Middle East, and Leopold Aschenbrenner responds that it's not all of them, just some.

**Justification:**

Leopold Aschenbrenner's statement is based on his own knowledge of the situation.

--------

## Chunk 109

**Chunk:**

Dwarkesh Patel
There are reports, I think Microsoft. We'll get into it.

So say the UAE gets a bunch of compute because we're building the clusters there. Let's say they have 25% of the compute. Why does a compute ratio matter? If it's about them being able to kick off the intelligence explosion, isn't it just some threshold where you have 100 million AI researchers or you don't?
Leopold Aschenbrenner
You can do a lot with 33 million extremely smart scientists. That might be enough to build the crazy bio weapons. Then you're in a situation where they stole the weights and they seized the compute.

Now they can make these crazy new WMDs that will be possible with superintelligence. Now you've just proliferated the stuff that’ll be really powerful. Also, 3x on compute isn't actually that much.

The riskiest situation is if we're in some sort of really neck and neck, feverish international struggle. Say we're really close with the CCP and we're months apart. The situation we want to be in — and could be in if we play our cards right — is a little bit more like the US building the atomic bomb versus the German project years behind. If we have that, we just have so much more wiggle room to get safety right.

We're going to be building these crazy new WMDs that completely undermine nuclear deterrence. That's so much easier to deal with if you don't have somebody right on your tails and you have to go at maximum speed. You have no wiggle room. You're worried that at any time they can overtake you.

They can also just try to outbuild you. They might literally win. China might literally win if they can steal the weights, because they can outbuild you. They may have less caution, both good and bad caution in terms of whatever unreasonable regulations we have.

If you're in this really tight race, this sort of feverish struggle, that's when there's the greatest peril of self-destruction.
Dwarkesh Patel
Presumably the companies that are trying to build clusters in the Middle East realize this. Is it just that it’s impossible to do this in America? If you want American companies to do this at all, do you have to do it in the Middle East or not at all? Then you just have China build a Three Gorges Dam cluster.

**Extracted Belief:**

A significant number of highly intelligent scientists (around 33 million) could be sufficient to develop advanced biological weapons.

**Context:**

Leopold Aschenbrenner was discussing the risks associated with building superintelligence clusters in authoritarian regimes, and he used the example of biological weapons as a potential outcome of a large-scale deployment of AI in the wrong hands.

**Justification:**

The belief is based on the idea that a large enough pool of skilled scientists can achieve significant scientific breakthroughs, including the development of biological weapons, especially with the aid of superintelligence.

--------

## Chunk 110

**Chunk:**

Dwarkesh Patel
There are reports, I think Microsoft. We'll get into it.

So say the UAE gets a bunch of compute because we're building the clusters there. Let's say they have 25% of the compute. Why does a compute ratio matter? If it's about them being able to kick off the intelligence explosion, isn't it just some threshold where you have 100 million AI researchers or you don't?
Leopold Aschenbrenner
You can do a lot with 33 million extremely smart scientists. That might be enough to build the crazy bio weapons. Then you're in a situation where they stole the weights and they seized the compute.

Now they can make these crazy new WMDs that will be possible with superintelligence. Now you've just proliferated the stuff that’ll be really powerful. Also, 3x on compute isn't actually that much.

The riskiest situation is if we're in some sort of really neck and neck, feverish international struggle. Say we're really close with the CCP and we're months apart. The situation we want to be in — and could be in if we play our cards right — is a little bit more like the US building the atomic bomb versus the German project years behind. If we have that, we just have so much more wiggle room to get safety right.

We're going to be building these crazy new WMDs that completely undermine nuclear deterrence. That's so much easier to deal with if you don't have somebody right on your tails and you have to go at maximum speed. You have no wiggle room. You're worried that at any time they can overtake you.

They can also just try to outbuild you. They might literally win. China might literally win if they can steal the weights, because they can outbuild you. They may have less caution, both good and bad caution in terms of whatever unreasonable regulations we have.

If you're in this really tight race, this sort of feverish struggle, that's when there's the greatest peril of self-destruction.
Dwarkesh Patel
Presumably the companies that are trying to build clusters in the Middle East realize this. Is it just that it’s impossible to do this in America? If you want American companies to do this at all, do you have to do it in the Middle East or not at all? Then you just have China build a Three Gorges Dam cluster.

**Extracted Belief:**

Superintelligence could be used to create new and powerful weapons of mass destruction (WMDs).

**Context:**

Leopold Aschenbrenner was discussing the potential risks of superintelligence falling into the wrong hands, and he argued that this could lead to the creation of new and more powerful WMDs.

**Justification:**

This belief is based on the understanding that superintelligence could be used to advance scientific knowledge, including in the field of weapons development, potentially exceeding existing capabilities.

--------

## Chunk 111

**Chunk:**

Dwarkesh Patel
There are reports, I think Microsoft. We'll get into it.

So say the UAE gets a bunch of compute because we're building the clusters there. Let's say they have 25% of the compute. Why does a compute ratio matter? If it's about them being able to kick off the intelligence explosion, isn't it just some threshold where you have 100 million AI researchers or you don't?
Leopold Aschenbrenner
You can do a lot with 33 million extremely smart scientists. That might be enough to build the crazy bio weapons. Then you're in a situation where they stole the weights and they seized the compute.

Now they can make these crazy new WMDs that will be possible with superintelligence. Now you've just proliferated the stuff that’ll be really powerful. Also, 3x on compute isn't actually that much.

The riskiest situation is if we're in some sort of really neck and neck, feverish international struggle. Say we're really close with the CCP and we're months apart. The situation we want to be in — and could be in if we play our cards right — is a little bit more like the US building the atomic bomb versus the German project years behind. If we have that, we just have so much more wiggle room to get safety right.

We're going to be building these crazy new WMDs that completely undermine nuclear deterrence. That's so much easier to deal with if you don't have somebody right on your tails and you have to go at maximum speed. You have no wiggle room. You're worried that at any time they can overtake you.

They can also just try to outbuild you. They might literally win. China might literally win if they can steal the weights, because they can outbuild you. They may have less caution, both good and bad caution in terms of whatever unreasonable regulations we have.

If you're in this really tight race, this sort of feverish struggle, that's when there's the greatest peril of self-destruction.
Dwarkesh Patel
Presumably the companies that are trying to build clusters in the Middle East realize this. Is it just that it’s impossible to do this in America? If you want American companies to do this at all, do you have to do it in the Middle East or not at all? Then you just have China build a Three Gorges Dam cluster.

**Extracted Belief:**

The proliferation of advanced WMDs, such as those potentially created with superintelligence, poses a significant threat to global security.

**Context:**

Leopold Aschenbrenner was discussing the risks of superintelligence falling into the wrong hands and highlighted the potential for the creation of new WMDs, which would exacerbate global security risks.

**Justification:**

This belief is grounded in the understanding that the spread of powerful weapons can lead to increased tensions, potential conflicts, and a heightened risk of global catastrophe.

--------

## Chunk 112

**Chunk:**

Dwarkesh Patel
There are reports, I think Microsoft. We'll get into it.

So say the UAE gets a bunch of compute because we're building the clusters there. Let's say they have 25% of the compute. Why does a compute ratio matter? If it's about them being able to kick off the intelligence explosion, isn't it just some threshold where you have 100 million AI researchers or you don't?
Leopold Aschenbrenner
You can do a lot with 33 million extremely smart scientists. That might be enough to build the crazy bio weapons. Then you're in a situation where they stole the weights and they seized the compute.

Now they can make these crazy new WMDs that will be possible with superintelligence. Now you've just proliferated the stuff that’ll be really powerful. Also, 3x on compute isn't actually that much.

The riskiest situation is if we're in some sort of really neck and neck, feverish international struggle. Say we're really close with the CCP and we're months apart. The situation we want to be in — and could be in if we play our cards right — is a little bit more like the US building the atomic bomb versus the German project years behind. If we have that, we just have so much more wiggle room to get safety right.

We're going to be building these crazy new WMDs that completely undermine nuclear deterrence. That's so much easier to deal with if you don't have somebody right on your tails and you have to go at maximum speed. You have no wiggle room. You're worried that at any time they can overtake you.

They can also just try to outbuild you. They might literally win. China might literally win if they can steal the weights, because they can outbuild you. They may have less caution, both good and bad caution in terms of whatever unreasonable regulations we have.

If you're in this really tight race, this sort of feverish struggle, that's when there's the greatest peril of self-destruction.
Dwarkesh Patel
Presumably the companies that are trying to build clusters in the Middle East realize this. Is it just that it’s impossible to do this in America? If you want American companies to do this at all, do you have to do it in the Middle East or not at all? Then you just have China build a Three Gorges Dam cluster.

**Extracted Belief:**

A three-fold advantage in computing power is not insignificant and could be used to achieve significant advancements.

**Context:**

Leopold Aschenbrenner was discussing the implications of an authoritarian regime possessing a significant portion of global compute power for superintelligence development.

**Justification:**

This belief is based on the understanding that access to more computing resources can accelerate scientific progress and give a significant advantage in areas like AI development.

--------

## Chunk 113

**Chunk:**

Dwarkesh Patel
There are reports, I think Microsoft. We'll get into it.

So say the UAE gets a bunch of compute because we're building the clusters there. Let's say they have 25% of the compute. Why does a compute ratio matter? If it's about them being able to kick off the intelligence explosion, isn't it just some threshold where you have 100 million AI researchers or you don't?
Leopold Aschenbrenner
You can do a lot with 33 million extremely smart scientists. That might be enough to build the crazy bio weapons. Then you're in a situation where they stole the weights and they seized the compute.

Now they can make these crazy new WMDs that will be possible with superintelligence. Now you've just proliferated the stuff that’ll be really powerful. Also, 3x on compute isn't actually that much.

The riskiest situation is if we're in some sort of really neck and neck, feverish international struggle. Say we're really close with the CCP and we're months apart. The situation we want to be in — and could be in if we play our cards right — is a little bit more like the US building the atomic bomb versus the German project years behind. If we have that, we just have so much more wiggle room to get safety right.

We're going to be building these crazy new WMDs that completely undermine nuclear deterrence. That's so much easier to deal with if you don't have somebody right on your tails and you have to go at maximum speed. You have no wiggle room. You're worried that at any time they can overtake you.

They can also just try to outbuild you. They might literally win. China might literally win if they can steal the weights, because they can outbuild you. They may have less caution, both good and bad caution in terms of whatever unreasonable regulations we have.

If you're in this really tight race, this sort of feverish struggle, that's when there's the greatest peril of self-destruction.
Dwarkesh Patel
Presumably the companies that are trying to build clusters in the Middle East realize this. Is it just that it’s impossible to do this in America? If you want American companies to do this at all, do you have to do it in the Middle East or not at all? Then you just have China build a Three Gorges Dam cluster.

**Extracted Belief:**

The ideal scenario for managing the development of superintelligence involves maintaining a significant lead over potential adversaries.

**Context:**

Leopold Aschenbrenner was discussing the importance of the US maintaining a significant lead in the development of superintelligence in order to ensure safety and prevent its misuse.

**Justification:**

He argues that a lead in AI development would allow for more time and resources to implement safety measures and prevent the misuse of superintelligence by others, drawing a parallel to the US's advantage in developing the atomic bomb.

--------

## Chunk 114

**Chunk:**

Dwarkesh Patel
There are reports, I think Microsoft. We'll get into it.

So say the UAE gets a bunch of compute because we're building the clusters there. Let's say they have 25% of the compute. Why does a compute ratio matter? If it's about them being able to kick off the intelligence explosion, isn't it just some threshold where you have 100 million AI researchers or you don't?
Leopold Aschenbrenner
You can do a lot with 33 million extremely smart scientists. That might be enough to build the crazy bio weapons. Then you're in a situation where they stole the weights and they seized the compute.

Now they can make these crazy new WMDs that will be possible with superintelligence. Now you've just proliferated the stuff that’ll be really powerful. Also, 3x on compute isn't actually that much.

The riskiest situation is if we're in some sort of really neck and neck, feverish international struggle. Say we're really close with the CCP and we're months apart. The situation we want to be in — and could be in if we play our cards right — is a little bit more like the US building the atomic bomb versus the German project years behind. If we have that, we just have so much more wiggle room to get safety right.

We're going to be building these crazy new WMDs that completely undermine nuclear deterrence. That's so much easier to deal with if you don't have somebody right on your tails and you have to go at maximum speed. You have no wiggle room. You're worried that at any time they can overtake you.

They can also just try to outbuild you. They might literally win. China might literally win if they can steal the weights, because they can outbuild you. They may have less caution, both good and bad caution in terms of whatever unreasonable regulations we have.

If you're in this really tight race, this sort of feverish struggle, that's when there's the greatest peril of self-destruction.
Dwarkesh Patel
Presumably the companies that are trying to build clusters in the Middle East realize this. Is it just that it’s impossible to do this in America? If you want American companies to do this at all, do you have to do it in the Middle East or not at all? Then you just have China build a Three Gorges Dam cluster.

**Extracted Belief:**

Developing superintelligence without a significant lead over adversaries creates a high risk of self-destruction.

**Context:**

Leopold Aschenbrenner was discussing the dangers of a close race in the development of superintelligence, arguing that this scenario could lead to a dangerous arms race and ultimately self-destruction.

**Justification:**

This belief is based on the idea that a close race for technological superiority could lead to hasty decisions, lack of caution, and potentially disastrous consequences in the pursuit of outdoing rivals.

--------

## Chunk 115

**Chunk:**

Dwarkesh Patel
There are reports, I think Microsoft. We'll get into it.

So say the UAE gets a bunch of compute because we're building the clusters there. Let's say they have 25% of the compute. Why does a compute ratio matter? If it's about them being able to kick off the intelligence explosion, isn't it just some threshold where you have 100 million AI researchers or you don't?
Leopold Aschenbrenner
You can do a lot with 33 million extremely smart scientists. That might be enough to build the crazy bio weapons. Then you're in a situation where they stole the weights and they seized the compute.

Now they can make these crazy new WMDs that will be possible with superintelligence. Now you've just proliferated the stuff that’ll be really powerful. Also, 3x on compute isn't actually that much.

The riskiest situation is if we're in some sort of really neck and neck, feverish international struggle. Say we're really close with the CCP and we're months apart. The situation we want to be in — and could be in if we play our cards right — is a little bit more like the US building the atomic bomb versus the German project years behind. If we have that, we just have so much more wiggle room to get safety right.

We're going to be building these crazy new WMDs that completely undermine nuclear deterrence. That's so much easier to deal with if you don't have somebody right on your tails and you have to go at maximum speed. You have no wiggle room. You're worried that at any time they can overtake you.

They can also just try to outbuild you. They might literally win. China might literally win if they can steal the weights, because they can outbuild you. They may have less caution, both good and bad caution in terms of whatever unreasonable regulations we have.

If you're in this really tight race, this sort of feverish struggle, that's when there's the greatest peril of self-destruction.
Dwarkesh Patel
Presumably the companies that are trying to build clusters in the Middle East realize this. Is it just that it’s impossible to do this in America? If you want American companies to do this at all, do you have to do it in the Middle East or not at all? Then you just have China build a Three Gorges Dam cluster.

**Extracted Belief:**

China's potential to overtake the US in superintelligence development poses a significant threat, given their potential to invest more resources and potentially operate with less caution.

**Context:**

Leopold Aschenbrenner was discussing the risks of China potentially overtaking the US in superintelligence development, emphasizing their potential to invest more resources and potentially operate with less caution.

**Justification:**

This belief is based on the understanding that China's economic growth and its potential for greater investment in AI development, coupled with potentially different regulatory frameworks, could give them an advantage in the race for superintelligence.

--------

## Chunk 116

**Chunk:**

Dwarkesh Patel
Presumably the companies that are trying to build clusters in the Middle East realize this. Is it just that it’s impossible to do this in America? If you want American companies to do this at all, do you have to do it in the Middle East or not at all? Then you just have China build a Three Gorges Dam cluster.
Leopold Aschenbrenner
There’s a few reasons. People aren’t thinking about this as the AGI superintelligence cluster. They’re just like, “ah, cool clusters for my ChatGPT.”
Dwarkesh Patel
If you’re doing ones for inference, presumably you could spread them out across the country or something. The ones they’re building, they’re going to do one training run in a single thing they’re building.

**Extracted Belief:**

Large language model clusters are currently being built with the primary purpose of powering applications like ChatGPT, rather than being designed for advanced artificial general intelligence (AGI) development.

**Context:**

Leopold Aschenbrenner explains why large language model clusters are being built in the Middle East despite potential risks associated with AGI development.

**Justification:**

Aschenbrenner states that people are not thinking about these clusters as AGI superintelligence clusters, but rather as "cool clusters for my ChatGPT."

--------

## Chunk 117

**Chunk:**

Dwarkesh Patel
If you’re doing ones for inference, presumably you could spread them out across the country or something. The ones they’re building, they’re going to do one training run in a single thing they’re building.
Leopold Aschenbrenner
It’s just hard to distinguish between inference and training compute. People can claim it’s inference compute, but they might realize that actually this is going to be useful for training compute too.
Dwarkesh Patel
Because of synthetic data and things like that?

**Extracted Belief:**

It is difficult to distinguish between inference and training compute.

**Context:**

Leopold Aschenbrenner was responding to a question about the distinction between inference and training compute in the context of AI clusters.

**Justification:**

Leopold Aschenbrenner states that 'It’s just hard to distinguish between inference and training compute' without further elaboration. This implies that this belief is based on direct observation of how compute resources are used in the field.

--------

## Chunk 118

**Chunk:**

Dwarkesh Patel
If you’re doing ones for inference, presumably you could spread them out across the country or something. The ones they’re building, they’re going to do one training run in a single thing they’re building.
Leopold Aschenbrenner
It’s just hard to distinguish between inference and training compute. People can claim it’s inference compute, but they might realize that actually this is going to be useful for training compute too.
Dwarkesh Patel
Because of synthetic data and things like that?

**Extracted Belief:**

People may claim their compute is for inference, but it might actually be useful for training.

**Context:**

Leopold Aschenbrenner was explaining the potential for misrepresentation regarding the purpose of compute resources.

**Justification:**

Leopold Aschenbrenner states that 'People can claim it’s inference compute, but they might realize that actually this is going to be useful for training compute too.' This suggests that he has observed instances where this has happened.

--------

## Chunk 119

**Chunk:**

Dwarkesh Patel
Because of synthetic data and things like that?
Leopold Aschenbrenner
RL looks a lot like inference, for example. Or you just end up connecting them in time. It's a lot like raw materials. It's like placing your uranium refinement facilities there.

So there are a few reasons. One, they don't think about this as the AGI cluster. Another is just that there’s easy money coming from the Middle East.

Another one is that some people think that you can't do it in the US. We actually face a real system competition here. Some people think that only autocracies that can do this with top-down mobilization of industrial capacity and the power to get stuff done fast.

Again, this is the sort of thing we haven't faced in a while. But during the Cold War, there was this intense system competition. East vs. West Germany was this. It was West Germany as liberal democratic capitalism vs. state-planned communism.

Now it's obvious that the free world would win. But even as late as 1961, Paul Samuelson was predicting that the Soviet Union would outgrow the United States because they were able to mobilize industry better.

So there are some people who shitpost about loving America, but then in private they're betting against America. They're betting against the liberal order. Basically, it's just a bad bet. This stuff is really possible in the US.

To make it possible in the US, to some degree we have to get our act together. There are basically two paths to doing it in the US. One is you just have to be willing to do natural gas. There's ample natural gas. You put your cluster in West Texas. You put it in southwest Pennsylvania by the Marcellus Shale. The 10 GW cluster is super easy. The 100 GW cluster is also pretty doable. I think natural gas production in the United States has almost doubled in a decade. You do that one more time over the next seven years, you could power multiple trillion-dollar data centers.

The issue there is that a lot of people made these climate commitments, not just the government. It's actually the private companies themselves, Microsoft, Amazon, etc., that have made these climate commitments. So they won't do natural gas. I admire the climate commitments, but at some point the national interest and national security is more important.

The other path is doing green energy megaprojects. You do solar and batteries and SMRs and geothermal. If we want to do that, there needs to be a broad deregulatory push. You can't have permitting take a decade. You have to reform FERC. You have to have blanket NEPA exemptions for this stuff.

There are inane state-level regulations. You can build the solar panels and batteries next to your data center, but it'll still take years because you actually have to hook it up to the state electrical grid. You have to use governmental powers to create rights of way to have multiple clusters and connect them and have the cables.

Ideally we do both. Ideally we do natural gas and the broader deregulatory green agenda. We have to do at least one. Then this stuff is possible in the United States.
Dwarkesh Patel
Before the conversation I was reading a good book about World War II industrial mobilization in the United States called Freedom's Forge. I’m thinking back on that period, especially in the context of reading Patrick Collison’s Fast and the progress study stuff. There’s this narrative out there that we had state capacity back then and people just got shit done but that now it's a clusterfuck.

**Extracted Belief:**

The distinction between inference and training compute is becoming blurred, as what is initially considered inference compute can later become useful for training.

**Context:**

Leopold Aschenbrenner is discussing the reasons why companies are choosing to build AI clusters in the Middle East rather than in the United States.

**Justification:**

Aschenbrenner is stating that the distinction between inference and training compute is becoming blurred, suggesting this observation is based on real-world trends in the AI industry.

--------

## Chunk 120

**Chunk:**

Dwarkesh Patel
Because of synthetic data and things like that?
Leopold Aschenbrenner
RL looks a lot like inference, for example. Or you just end up connecting them in time. It's a lot like raw materials. It's like placing your uranium refinement facilities there.

So there are a few reasons. One, they don't think about this as the AGI cluster. Another is just that there’s easy money coming from the Middle East.

Another one is that some people think that you can't do it in the US. We actually face a real system competition here. Some people think that only autocracies that can do this with top-down mobilization of industrial capacity and the power to get stuff done fast.

Again, this is the sort of thing we haven't faced in a while. But during the Cold War, there was this intense system competition. East vs. West Germany was this. It was West Germany as liberal democratic capitalism vs. state-planned communism.

Now it's obvious that the free world would win. But even as late as 1961, Paul Samuelson was predicting that the Soviet Union would outgrow the United States because they were able to mobilize industry better.

So there are some people who shitpost about loving America, but then in private they're betting against America. They're betting against the liberal order. Basically, it's just a bad bet. This stuff is really possible in the US.

To make it possible in the US, to some degree we have to get our act together. There are basically two paths to doing it in the US. One is you just have to be willing to do natural gas. There's ample natural gas. You put your cluster in West Texas. You put it in southwest Pennsylvania by the Marcellus Shale. The 10 GW cluster is super easy. The 100 GW cluster is also pretty doable. I think natural gas production in the United States has almost doubled in a decade. You do that one more time over the next seven years, you could power multiple trillion-dollar data centers.

The issue there is that a lot of people made these climate commitments, not just the government. It's actually the private companies themselves, Microsoft, Amazon, etc., that have made these climate commitments. So they won't do natural gas. I admire the climate commitments, but at some point the national interest and national security is more important.

The other path is doing green energy megaprojects. You do solar and batteries and SMRs and geothermal. If we want to do that, there needs to be a broad deregulatory push. You can't have permitting take a decade. You have to reform FERC. You have to have blanket NEPA exemptions for this stuff.

There are inane state-level regulations. You can build the solar panels and batteries next to your data center, but it'll still take years because you actually have to hook it up to the state electrical grid. You have to use governmental powers to create rights of way to have multiple clusters and connect them and have the cables.

Ideally we do both. Ideally we do natural gas and the broader deregulatory green agenda. We have to do at least one. Then this stuff is possible in the United States.
Dwarkesh Patel
Before the conversation I was reading a good book about World War II industrial mobilization in the United States called Freedom's Forge. I’m thinking back on that period, especially in the context of reading Patrick Collison’s Fast and the progress study stuff. There’s this narrative out there that we had state capacity back then and people just got shit done but that now it's a clusterfuck.

**Extracted Belief:**

Some individuals believe that large-scale AI clusters can only be built in autocracies due to their ability for top-down mobilization of industrial capacity and fast execution.

**Context:**

Aschenbrenner is outlining the reasons why some believe building AI clusters in the US is challenging, citing the perceived inability of democracies to mobilize resources effectively.

**Justification:**

Aschenbrenner explicitly states that 'some people think that only autocracies that can do this with top-down mobilization of industrial capacity and the power to get stuff done fast.' This indicates the belief is based on the testimony of others.

--------

## Chunk 121

**Chunk:**

Dwarkesh Patel
Because of synthetic data and things like that?
Leopold Aschenbrenner
RL looks a lot like inference, for example. Or you just end up connecting them in time. It's a lot like raw materials. It's like placing your uranium refinement facilities there.

So there are a few reasons. One, they don't think about this as the AGI cluster. Another is just that there’s easy money coming from the Middle East.

Another one is that some people think that you can't do it in the US. We actually face a real system competition here. Some people think that only autocracies that can do this with top-down mobilization of industrial capacity and the power to get stuff done fast.

Again, this is the sort of thing we haven't faced in a while. But during the Cold War, there was this intense system competition. East vs. West Germany was this. It was West Germany as liberal democratic capitalism vs. state-planned communism.

Now it's obvious that the free world would win. But even as late as 1961, Paul Samuelson was predicting that the Soviet Union would outgrow the United States because they were able to mobilize industry better.

So there are some people who shitpost about loving America, but then in private they're betting against America. They're betting against the liberal order. Basically, it's just a bad bet. This stuff is really possible in the US.

To make it possible in the US, to some degree we have to get our act together. There are basically two paths to doing it in the US. One is you just have to be willing to do natural gas. There's ample natural gas. You put your cluster in West Texas. You put it in southwest Pennsylvania by the Marcellus Shale. The 10 GW cluster is super easy. The 100 GW cluster is also pretty doable. I think natural gas production in the United States has almost doubled in a decade. You do that one more time over the next seven years, you could power multiple trillion-dollar data centers.

The issue there is that a lot of people made these climate commitments, not just the government. It's actually the private companies themselves, Microsoft, Amazon, etc., that have made these climate commitments. So they won't do natural gas. I admire the climate commitments, but at some point the national interest and national security is more important.

The other path is doing green energy megaprojects. You do solar and batteries and SMRs and geothermal. If we want to do that, there needs to be a broad deregulatory push. You can't have permitting take a decade. You have to reform FERC. You have to have blanket NEPA exemptions for this stuff.

There are inane state-level regulations. You can build the solar panels and batteries next to your data center, but it'll still take years because you actually have to hook it up to the state electrical grid. You have to use governmental powers to create rights of way to have multiple clusters and connect them and have the cables.

Ideally we do both. Ideally we do natural gas and the broader deregulatory green agenda. We have to do at least one. Then this stuff is possible in the United States.
Dwarkesh Patel
Before the conversation I was reading a good book about World War II industrial mobilization in the United States called Freedom's Forge. I’m thinking back on that period, especially in the context of reading Patrick Collison’s Fast and the progress study stuff. There’s this narrative out there that we had state capacity back then and people just got shit done but that now it's a clusterfuck.

**Extracted Belief:**

The United States is capable of building large-scale AI clusters, despite some pessimism about its ability.

**Context:**

Aschenbrenner is arguing against the belief that the US cannot build AI clusters, emphasizing its capability for large-scale projects.

**Justification:**

Aschenbrenner states that 'this stuff is really possible in the US', implying that he believes the US can build these clusters, based on his understanding of US capabilities.

--------

## Chunk 122

**Chunk:**

Dwarkesh Patel
Because of synthetic data and things like that?
Leopold Aschenbrenner
RL looks a lot like inference, for example. Or you just end up connecting them in time. It's a lot like raw materials. It's like placing your uranium refinement facilities there.

So there are a few reasons. One, they don't think about this as the AGI cluster. Another is just that there’s easy money coming from the Middle East.

Another one is that some people think that you can't do it in the US. We actually face a real system competition here. Some people think that only autocracies that can do this with top-down mobilization of industrial capacity and the power to get stuff done fast.

Again, this is the sort of thing we haven't faced in a while. But during the Cold War, there was this intense system competition. East vs. West Germany was this. It was West Germany as liberal democratic capitalism vs. state-planned communism.

Now it's obvious that the free world would win. But even as late as 1961, Paul Samuelson was predicting that the Soviet Union would outgrow the United States because they were able to mobilize industry better.

So there are some people who shitpost about loving America, but then in private they're betting against America. They're betting against the liberal order. Basically, it's just a bad bet. This stuff is really possible in the US.

To make it possible in the US, to some degree we have to get our act together. There are basically two paths to doing it in the US. One is you just have to be willing to do natural gas. There's ample natural gas. You put your cluster in West Texas. You put it in southwest Pennsylvania by the Marcellus Shale. The 10 GW cluster is super easy. The 100 GW cluster is also pretty doable. I think natural gas production in the United States has almost doubled in a decade. You do that one more time over the next seven years, you could power multiple trillion-dollar data centers.

The issue there is that a lot of people made these climate commitments, not just the government. It's actually the private companies themselves, Microsoft, Amazon, etc., that have made these climate commitments. So they won't do natural gas. I admire the climate commitments, but at some point the national interest and national security is more important.

The other path is doing green energy megaprojects. You do solar and batteries and SMRs and geothermal. If we want to do that, there needs to be a broad deregulatory push. You can't have permitting take a decade. You have to reform FERC. You have to have blanket NEPA exemptions for this stuff.

There are inane state-level regulations. You can build the solar panels and batteries next to your data center, but it'll still take years because you actually have to hook it up to the state electrical grid. You have to use governmental powers to create rights of way to have multiple clusters and connect them and have the cables.

Ideally we do both. Ideally we do natural gas and the broader deregulatory green agenda. We have to do at least one. Then this stuff is possible in the United States.
Dwarkesh Patel
Before the conversation I was reading a good book about World War II industrial mobilization in the United States called Freedom's Forge. I’m thinking back on that period, especially in the context of reading Patrick Collison’s Fast and the progress study stuff. There’s this narrative out there that we had state capacity back then and people just got shit done but that now it's a clusterfuck.

**Extracted Belief:**

The United States has ample natural gas resources to support the construction of large-scale AI clusters.

**Context:**

Aschenbrenner presents natural gas as a potential energy source for powering AI clusters in the US, highlighting its availability.

**Justification:**

Aschenbrenner explicitly states that 'there's ample natural gas' in the US, suggesting this is a belief informed by real-world data on natural gas reserves.

--------

## Chunk 123

**Chunk:**

Dwarkesh Patel
Because of synthetic data and things like that?
Leopold Aschenbrenner
RL looks a lot like inference, for example. Or you just end up connecting them in time. It's a lot like raw materials. It's like placing your uranium refinement facilities there.

So there are a few reasons. One, they don't think about this as the AGI cluster. Another is just that there’s easy money coming from the Middle East.

Another one is that some people think that you can't do it in the US. We actually face a real system competition here. Some people think that only autocracies that can do this with top-down mobilization of industrial capacity and the power to get stuff done fast.

Again, this is the sort of thing we haven't faced in a while. But during the Cold War, there was this intense system competition. East vs. West Germany was this. It was West Germany as liberal democratic capitalism vs. state-planned communism.

Now it's obvious that the free world would win. But even as late as 1961, Paul Samuelson was predicting that the Soviet Union would outgrow the United States because they were able to mobilize industry better.

So there are some people who shitpost about loving America, but then in private they're betting against America. They're betting against the liberal order. Basically, it's just a bad bet. This stuff is really possible in the US.

To make it possible in the US, to some degree we have to get our act together. There are basically two paths to doing it in the US. One is you just have to be willing to do natural gas. There's ample natural gas. You put your cluster in West Texas. You put it in southwest Pennsylvania by the Marcellus Shale. The 10 GW cluster is super easy. The 100 GW cluster is also pretty doable. I think natural gas production in the United States has almost doubled in a decade. You do that one more time over the next seven years, you could power multiple trillion-dollar data centers.

The issue there is that a lot of people made these climate commitments, not just the government. It's actually the private companies themselves, Microsoft, Amazon, etc., that have made these climate commitments. So they won't do natural gas. I admire the climate commitments, but at some point the national interest and national security is more important.

The other path is doing green energy megaprojects. You do solar and batteries and SMRs and geothermal. If we want to do that, there needs to be a broad deregulatory push. You can't have permitting take a decade. You have to reform FERC. You have to have blanket NEPA exemptions for this stuff.

There are inane state-level regulations. You can build the solar panels and batteries next to your data center, but it'll still take years because you actually have to hook it up to the state electrical grid. You have to use governmental powers to create rights of way to have multiple clusters and connect them and have the cables.

Ideally we do both. Ideally we do natural gas and the broader deregulatory green agenda. We have to do at least one. Then this stuff is possible in the United States.
Dwarkesh Patel
Before the conversation I was reading a good book about World War II industrial mobilization in the United States called Freedom's Forge. I’m thinking back on that period, especially in the context of reading Patrick Collison’s Fast and the progress study stuff. There’s this narrative out there that we had state capacity back then and people just got shit done but that now it's a clusterfuck.

**Extracted Belief:**

Natural gas production in the United States has significantly increased in recent years, making it a viable energy source for AI clusters.

**Context:**

Aschenbrenner further emphasizes the feasibility of using natural gas for AI clusters by citing its growth in production.

**Justification:**

He states that 'natural gas production in the United States has almost doubled in a decade', demonstrating a belief supported by evidence of natural gas production growth.

--------

## Chunk 124

**Chunk:**

Dwarkesh Patel
Because of synthetic data and things like that?
Leopold Aschenbrenner
RL looks a lot like inference, for example. Or you just end up connecting them in time. It's a lot like raw materials. It's like placing your uranium refinement facilities there.

So there are a few reasons. One, they don't think about this as the AGI cluster. Another is just that there’s easy money coming from the Middle East.

Another one is that some people think that you can't do it in the US. We actually face a real system competition here. Some people think that only autocracies that can do this with top-down mobilization of industrial capacity and the power to get stuff done fast.

Again, this is the sort of thing we haven't faced in a while. But during the Cold War, there was this intense system competition. East vs. West Germany was this. It was West Germany as liberal democratic capitalism vs. state-planned communism.

Now it's obvious that the free world would win. But even as late as 1961, Paul Samuelson was predicting that the Soviet Union would outgrow the United States because they were able to mobilize industry better.

So there are some people who shitpost about loving America, but then in private they're betting against America. They're betting against the liberal order. Basically, it's just a bad bet. This stuff is really possible in the US.

To make it possible in the US, to some degree we have to get our act together. There are basically two paths to doing it in the US. One is you just have to be willing to do natural gas. There's ample natural gas. You put your cluster in West Texas. You put it in southwest Pennsylvania by the Marcellus Shale. The 10 GW cluster is super easy. The 100 GW cluster is also pretty doable. I think natural gas production in the United States has almost doubled in a decade. You do that one more time over the next seven years, you could power multiple trillion-dollar data centers.

The issue there is that a lot of people made these climate commitments, not just the government. It's actually the private companies themselves, Microsoft, Amazon, etc., that have made these climate commitments. So they won't do natural gas. I admire the climate commitments, but at some point the national interest and national security is more important.

The other path is doing green energy megaprojects. You do solar and batteries and SMRs and geothermal. If we want to do that, there needs to be a broad deregulatory push. You can't have permitting take a decade. You have to reform FERC. You have to have blanket NEPA exemptions for this stuff.

There are inane state-level regulations. You can build the solar panels and batteries next to your data center, but it'll still take years because you actually have to hook it up to the state electrical grid. You have to use governmental powers to create rights of way to have multiple clusters and connect them and have the cables.

Ideally we do both. Ideally we do natural gas and the broader deregulatory green agenda. We have to do at least one. Then this stuff is possible in the United States.
Dwarkesh Patel
Before the conversation I was reading a good book about World War II industrial mobilization in the United States called Freedom's Forge. I’m thinking back on that period, especially in the context of reading Patrick Collison’s Fast and the progress study stuff. There’s this narrative out there that we had state capacity back then and people just got shit done but that now it's a clusterfuck.

**Extracted Belief:**

The national interest and national security are more important than climate commitments when it comes to crucial infrastructure projects.

**Context:**

Aschenbrenner argues that the US needs to prioritize national interest over climate commitments in the context of building AI clusters.

**Justification:**

Aschenbrenner asserts that 'at some point the national interest and national security is more important', implying a belief that these values supersede climate concerns in certain situations.

--------

## Chunk 125

**Chunk:**

Dwarkesh Patel
Because of synthetic data and things like that?
Leopold Aschenbrenner
RL looks a lot like inference, for example. Or you just end up connecting them in time. It's a lot like raw materials. It's like placing your uranium refinement facilities there.

So there are a few reasons. One, they don't think about this as the AGI cluster. Another is just that there’s easy money coming from the Middle East.

Another one is that some people think that you can't do it in the US. We actually face a real system competition here. Some people think that only autocracies that can do this with top-down mobilization of industrial capacity and the power to get stuff done fast.

Again, this is the sort of thing we haven't faced in a while. But during the Cold War, there was this intense system competition. East vs. West Germany was this. It was West Germany as liberal democratic capitalism vs. state-planned communism.

Now it's obvious that the free world would win. But even as late as 1961, Paul Samuelson was predicting that the Soviet Union would outgrow the United States because they were able to mobilize industry better.

So there are some people who shitpost about loving America, but then in private they're betting against America. They're betting against the liberal order. Basically, it's just a bad bet. This stuff is really possible in the US.

To make it possible in the US, to some degree we have to get our act together. There are basically two paths to doing it in the US. One is you just have to be willing to do natural gas. There's ample natural gas. You put your cluster in West Texas. You put it in southwest Pennsylvania by the Marcellus Shale. The 10 GW cluster is super easy. The 100 GW cluster is also pretty doable. I think natural gas production in the United States has almost doubled in a decade. You do that one more time over the next seven years, you could power multiple trillion-dollar data centers.

The issue there is that a lot of people made these climate commitments, not just the government. It's actually the private companies themselves, Microsoft, Amazon, etc., that have made these climate commitments. So they won't do natural gas. I admire the climate commitments, but at some point the national interest and national security is more important.

The other path is doing green energy megaprojects. You do solar and batteries and SMRs and geothermal. If we want to do that, there needs to be a broad deregulatory push. You can't have permitting take a decade. You have to reform FERC. You have to have blanket NEPA exemptions for this stuff.

There are inane state-level regulations. You can build the solar panels and batteries next to your data center, but it'll still take years because you actually have to hook it up to the state electrical grid. You have to use governmental powers to create rights of way to have multiple clusters and connect them and have the cables.

Ideally we do both. Ideally we do natural gas and the broader deregulatory green agenda. We have to do at least one. Then this stuff is possible in the United States.
Dwarkesh Patel
Before the conversation I was reading a good book about World War II industrial mobilization in the United States called Freedom's Forge. I’m thinking back on that period, especially in the context of reading Patrick Collison’s Fast and the progress study stuff. There’s this narrative out there that we had state capacity back then and people just got shit done but that now it's a clusterfuck.

**Extracted Belief:**

Green energy megaprojects, including solar, batteries, SMRs, and geothermal, are a viable alternative for powering AI clusters in the United States.

**Context:**

Aschenbrenner proposes green energy megaprojects as a second path for powering AI clusters, demonstrating a belief in their feasibility.

**Justification:**

He explicitly states that 'You do solar and batteries and SMRs and geothermal', suggesting this is based on his understanding of the current state of green energy technology and its potential.

--------

## Chunk 126

**Chunk:**

Dwarkesh Patel
Because of synthetic data and things like that?
Leopold Aschenbrenner
RL looks a lot like inference, for example. Or you just end up connecting them in time. It's a lot like raw materials. It's like placing your uranium refinement facilities there.

So there are a few reasons. One, they don't think about this as the AGI cluster. Another is just that there’s easy money coming from the Middle East.

Another one is that some people think that you can't do it in the US. We actually face a real system competition here. Some people think that only autocracies that can do this with top-down mobilization of industrial capacity and the power to get stuff done fast.

Again, this is the sort of thing we haven't faced in a while. But during the Cold War, there was this intense system competition. East vs. West Germany was this. It was West Germany as liberal democratic capitalism vs. state-planned communism.

Now it's obvious that the free world would win. But even as late as 1961, Paul Samuelson was predicting that the Soviet Union would outgrow the United States because they were able to mobilize industry better.

So there are some people who shitpost about loving America, but then in private they're betting against America. They're betting against the liberal order. Basically, it's just a bad bet. This stuff is really possible in the US.

To make it possible in the US, to some degree we have to get our act together. There are basically two paths to doing it in the US. One is you just have to be willing to do natural gas. There's ample natural gas. You put your cluster in West Texas. You put it in southwest Pennsylvania by the Marcellus Shale. The 10 GW cluster is super easy. The 100 GW cluster is also pretty doable. I think natural gas production in the United States has almost doubled in a decade. You do that one more time over the next seven years, you could power multiple trillion-dollar data centers.

The issue there is that a lot of people made these climate commitments, not just the government. It's actually the private companies themselves, Microsoft, Amazon, etc., that have made these climate commitments. So they won't do natural gas. I admire the climate commitments, but at some point the national interest and national security is more important.

The other path is doing green energy megaprojects. You do solar and batteries and SMRs and geothermal. If we want to do that, there needs to be a broad deregulatory push. You can't have permitting take a decade. You have to reform FERC. You have to have blanket NEPA exemptions for this stuff.

There are inane state-level regulations. You can build the solar panels and batteries next to your data center, but it'll still take years because you actually have to hook it up to the state electrical grid. You have to use governmental powers to create rights of way to have multiple clusters and connect them and have the cables.

Ideally we do both. Ideally we do natural gas and the broader deregulatory green agenda. We have to do at least one. Then this stuff is possible in the United States.
Dwarkesh Patel
Before the conversation I was reading a good book about World War II industrial mobilization in the United States called Freedom's Forge. I’m thinking back on that period, especially in the context of reading Patrick Collison’s Fast and the progress study stuff. There’s this narrative out there that we had state capacity back then and people just got shit done but that now it's a clusterfuck.

**Extracted Belief:**

Significant deregulation is required to expedite the development of green energy megaprojects in the United States.

**Context:**

Aschenbrenner argues that green energy megaprojects in the US require extensive deregulation to overcome permitting and regulatory hurdles.

**Justification:**

He explicitly states that 'there needs to be a broad deregulatory push' and that 'You can't have permitting take a decade', indicating a belief derived from the perceived need for streamlined regulations.

--------

## Chunk 127

**Chunk:**

Dwarkesh Patel
Before the conversation I was reading a good book about World War II industrial mobilization in the United States called Freedom's Forge. I’m thinking back on that period, especially in the context of reading Patrick Collison’s Fast and the progress study stuff. There’s this narrative out there that we had state capacity back then and people just got shit done but that now it's a clusterfuck.
Leopold Aschenbrenner
It wasn’t at all the case!
Dwarkesh Patel
It was really interesting. You had people from the Detroit auto industry side, like William Knudsen, who were running mobilization for the United States. They were extremely competent. At the same time you had labor organization and agitation, which is very analogous to the climate change pledges and concerns we have today.

They would literally have these strikes, into 1941, costing millions of man-hours worth of time when we're trying to make tens of thousands of planes a month. They would just debilitate factories for trivial concessions from capital that were pennies on the dollar.

There were concerns that the auto companies were trying to use the pretext of a potential war to prevent paying labor the money it deserves. So with what climate change is today, you might think, "ah, America's fucked. We're not going to be able to build this shit if you look at NEPA or something,” I didn't realize how debilitating labor was in World War II.

**Extracted Belief:**

The United States' industrial mobilization during World War II was not a seamless process and was significantly hindered by labor strikes.

**Context:**

Leopold Aschenbrenner is refuting the common narrative that the US was highly effective in industrial mobilization during WWII, specifically in response to Dwarkesh Patel's comment that 'people just got shit done' back then.

**Justification:**

He cites examples of labor strikes in 1941 that cost millions of man-hours and disrupted production, even as the US was trying to ramp up aircraft production.

--------

## Chunk 128

**Chunk:**

Dwarkesh Patel
It was really interesting. You had people from the Detroit auto industry side, like William Knudsen, who were running mobilization for the United States. They were extremely competent. At the same time you had labor organization and agitation, which is very analogous to the climate change pledges and concerns we have today.

They would literally have these strikes, into 1941, costing millions of man-hours worth of time when we're trying to make tens of thousands of planes a month. They would just debilitate factories for trivial concessions from capital that were pennies on the dollar.

There were concerns that the auto companies were trying to use the pretext of a potential war to prevent paying labor the money it deserves. So with what climate change is today, you might think, "ah, America's fucked. We're not going to be able to build this shit if you look at NEPA or something,” I didn't realize how debilitating labor was in World War II.
Leopold Aschenbrenner
It wasn’ just that. Before 1939, the American military was in total shambles. You read about it and it reads a little bit like the German military today. Military expenditures were I think less than 2% of GDP. All the European countries had gone, even in peacetime, above 10% of GDP.

It was rapid mobilization starting from nothing. We were making no planes. There were no military contracts. Everything had been starved during the Great Depression. But there was this latent capacity. At some point the United States got its act together.

This applies the other way around too with China. Sometimes people count them out a little bit with the export controls and so on. They're able to make 7-nanometer chips now. There's a question of how many they could make. There's at least a possibility that they're going to mature that ability and make a lot of 7-nanometer chips.

There's a lot of latent industrial capacity in China. They are able to build a lot of power fast. Maybe that isn't activated for AI yet. At some point, the same way the United States and a lot of people in the US government are going to wake up, the CCP is going to wake up.
Dwarkesh Patel
Companies realize that scaling is a thing. Obviously their whole plans are contingent on scaling. So they understand that in 2028 we're going to be building 10 GW data centers.

At that point, the people who can keep up are Big Tech, potentially at the edge of their capabilities, sovereign wealth fund-funded things, and also major countries like America and China. What's their plan? With the AI labs, what's their plan given this landscape? Do they not want the leverage of being in the United States?

**Extracted Belief:**

Prior to 1939, the American military was in a severely weakened state, comparable to the current condition of the German military.

**Context:**

Leopold Aschenbrenner is discussing the rapid mobilization of the United States during World War II, contrasting it with the pre-war state of the military.

**Justification:**

He cites military expenditures being less than 2% of GDP, compared to over 10% for European countries, as evidence of the American military's weakness.

--------

## Chunk 129

**Chunk:**

Dwarkesh Patel
It was really interesting. You had people from the Detroit auto industry side, like William Knudsen, who were running mobilization for the United States. They were extremely competent. At the same time you had labor organization and agitation, which is very analogous to the climate change pledges and concerns we have today.

They would literally have these strikes, into 1941, costing millions of man-hours worth of time when we're trying to make tens of thousands of planes a month. They would just debilitate factories for trivial concessions from capital that were pennies on the dollar.

There were concerns that the auto companies were trying to use the pretext of a potential war to prevent paying labor the money it deserves. So with what climate change is today, you might think, "ah, America's fucked. We're not going to be able to build this shit if you look at NEPA or something,” I didn't realize how debilitating labor was in World War II.
Leopold Aschenbrenner
It wasn’ just that. Before 1939, the American military was in total shambles. You read about it and it reads a little bit like the German military today. Military expenditures were I think less than 2% of GDP. All the European countries had gone, even in peacetime, above 10% of GDP.

It was rapid mobilization starting from nothing. We were making no planes. There were no military contracts. Everything had been starved during the Great Depression. But there was this latent capacity. At some point the United States got its act together.

This applies the other way around too with China. Sometimes people count them out a little bit with the export controls and so on. They're able to make 7-nanometer chips now. There's a question of how many they could make. There's at least a possibility that they're going to mature that ability and make a lot of 7-nanometer chips.

There's a lot of latent industrial capacity in China. They are able to build a lot of power fast. Maybe that isn't activated for AI yet. At some point, the same way the United States and a lot of people in the US government are going to wake up, the CCP is going to wake up.
Dwarkesh Patel
Companies realize that scaling is a thing. Obviously their whole plans are contingent on scaling. So they understand that in 2028 we're going to be building 10 GW data centers.

At that point, the people who can keep up are Big Tech, potentially at the edge of their capabilities, sovereign wealth fund-funded things, and also major countries like America and China. What's their plan? With the AI labs, what's their plan given this landscape? Do they not want the leverage of being in the United States?

**Extracted Belief:**

The United States possessed latent industrial capacity that was activated during World War II, despite being in a state of decline during the Great Depression.

**Context:**

Aschenbrenner continues to explain the rapid mobilization, highlighting the ability of the US to overcome its economic challenges.

**Justification:**

He mentions that the US was 'making no planes' and had 'no military contracts' before the war, implying a dormant industrial capacity that was subsequently mobilized.

--------

## Chunk 130

**Chunk:**

Dwarkesh Patel
It was really interesting. You had people from the Detroit auto industry side, like William Knudsen, who were running mobilization for the United States. They were extremely competent. At the same time you had labor organization and agitation, which is very analogous to the climate change pledges and concerns we have today.

They would literally have these strikes, into 1941, costing millions of man-hours worth of time when we're trying to make tens of thousands of planes a month. They would just debilitate factories for trivial concessions from capital that were pennies on the dollar.

There were concerns that the auto companies were trying to use the pretext of a potential war to prevent paying labor the money it deserves. So with what climate change is today, you might think, "ah, America's fucked. We're not going to be able to build this shit if you look at NEPA or something,” I didn't realize how debilitating labor was in World War II.
Leopold Aschenbrenner
It wasn’ just that. Before 1939, the American military was in total shambles. You read about it and it reads a little bit like the German military today. Military expenditures were I think less than 2% of GDP. All the European countries had gone, even in peacetime, above 10% of GDP.

It was rapid mobilization starting from nothing. We were making no planes. There were no military contracts. Everything had been starved during the Great Depression. But there was this latent capacity. At some point the United States got its act together.

This applies the other way around too with China. Sometimes people count them out a little bit with the export controls and so on. They're able to make 7-nanometer chips now. There's a question of how many they could make. There's at least a possibility that they're going to mature that ability and make a lot of 7-nanometer chips.

There's a lot of latent industrial capacity in China. They are able to build a lot of power fast. Maybe that isn't activated for AI yet. At some point, the same way the United States and a lot of people in the US government are going to wake up, the CCP is going to wake up.
Dwarkesh Patel
Companies realize that scaling is a thing. Obviously their whole plans are contingent on scaling. So they understand that in 2028 we're going to be building 10 GW data centers.

At that point, the people who can keep up are Big Tech, potentially at the edge of their capabilities, sovereign wealth fund-funded things, and also major countries like America and China. What's their plan? With the AI labs, what's their plan given this landscape? Do they not want the leverage of being in the United States?

**Extracted Belief:**

China has a significant amount of latent industrial capacity that can be activated for purposes like building advanced semiconductor chips and data centers.

**Context:**

Aschenbrenner draws parallels between the US during World War II and China's current capabilities.

**Justification:**

He mentions that China is 'able to make 7-nanometer chips' and has 'latent industrial capacity' that can be quickly activated, suggesting their potential for large-scale development.

--------

## Chunk 131

**Chunk:**

Dwarkesh Patel
Companies realize that scaling is a thing. Obviously their whole plans are contingent on scaling. So they understand that in 2028 we're going to be building 10 GW data centers.

At that point, the people who can keep up are Big Tech, potentially at the edge of their capabilities, sovereign wealth fund-funded things, and also major countries like America and China. What's their plan? With the AI labs, what's their plan given this landscape? Do they not want the leverage of being in the United States?
Leopold Aschenbrenner
The Middle East does offer capital, but America has plenty of capital. We have trillion-dollar companies. What are these Middle Eastern states? They're kind of like trillion-dollar oil companies. We have trillion-dollar companies and very deep financial markets. Microsoft could issue hundreds of billions of dollars of bonds and they can pay for these clusters.

Another argument being made, which is worth taking seriously, is that if we don't work with the UAE or with these Middle Eastern countries, they're just going to go to China. They're going to build data centers and pour money into AI regardless. If we don't work with them, they'll just support China.

There's some merit to the argument in the sense that we should be doing benefit-sharing with them. On the road to AGI, there should be two tiers of coalitions. There should be a narrow coalition of democracies that's developing AGI. Then there should be a broader coalition of other countries, including dictatorships, and we should offer them some of the benefits of AI.

If the UAE wants to use AI products, run Meta recommendation engines, or run the last-generation models, that's fine. By default, they just wouldn't have had this seat at the AGI table. So they have some money, but a lot of people have money.

The only reason they're getting this seat at the AGI table and giving these dictators this leverage over this extremely important national security technology, is because we're getting them excited and offering it to them.
Dwarkesh Patel
Who specifically is doing this? Who are the companies who are going there to fundraise?

**Extracted Belief:**

The United States has abundant capital, with trillion-dollar companies and deep financial markets, capable of funding large-scale AI projects.

**Context:**

Responding to a question about the potential for Middle Eastern capital to influence AI development, Leopold Aschenbrenner emphasizes the financial capacity of the US.

**Justification:**

He cites the existence of trillion-dollar companies like Microsoft, implying that they possess the financial resources to fund AI initiatives through methods like bond issuance.

--------

## Chunk 132

**Chunk:**

Dwarkesh Patel
Companies realize that scaling is a thing. Obviously their whole plans are contingent on scaling. So they understand that in 2028 we're going to be building 10 GW data centers.

At that point, the people who can keep up are Big Tech, potentially at the edge of their capabilities, sovereign wealth fund-funded things, and also major countries like America and China. What's their plan? With the AI labs, what's their plan given this landscape? Do they not want the leverage of being in the United States?
Leopold Aschenbrenner
The Middle East does offer capital, but America has plenty of capital. We have trillion-dollar companies. What are these Middle Eastern states? They're kind of like trillion-dollar oil companies. We have trillion-dollar companies and very deep financial markets. Microsoft could issue hundreds of billions of dollars of bonds and they can pay for these clusters.

Another argument being made, which is worth taking seriously, is that if we don't work with the UAE or with these Middle Eastern countries, they're just going to go to China. They're going to build data centers and pour money into AI regardless. If we don't work with them, they'll just support China.

There's some merit to the argument in the sense that we should be doing benefit-sharing with them. On the road to AGI, there should be two tiers of coalitions. There should be a narrow coalition of democracies that's developing AGI. Then there should be a broader coalition of other countries, including dictatorships, and we should offer them some of the benefits of AI.

If the UAE wants to use AI products, run Meta recommendation engines, or run the last-generation models, that's fine. By default, they just wouldn't have had this seat at the AGI table. So they have some money, but a lot of people have money.

The only reason they're getting this seat at the AGI table and giving these dictators this leverage over this extremely important national security technology, is because we're getting them excited and offering it to them.
Dwarkesh Patel
Who specifically is doing this? Who are the companies who are going there to fundraise?

**Extracted Belief:**

If the United States does not collaborate with the UAE and other Middle Eastern countries in AI development, they will likely turn to China for support and investment.

**Context:**

Leopold Aschenbrenner presents a potential scenario where Middle Eastern countries might choose to invest in Chinese AI initiatives if they are excluded from US-led development.

**Justification:**

He suggests that Middle Eastern countries are likely to pursue AI development regardless of their partnership with the US, and they may align with China if they lack alternative options.

--------

## Chunk 133

**Chunk:**

Dwarkesh Patel
Companies realize that scaling is a thing. Obviously their whole plans are contingent on scaling. So they understand that in 2028 we're going to be building 10 GW data centers.

At that point, the people who can keep up are Big Tech, potentially at the edge of their capabilities, sovereign wealth fund-funded things, and also major countries like America and China. What's their plan? With the AI labs, what's their plan given this landscape? Do they not want the leverage of being in the United States?
Leopold Aschenbrenner
The Middle East does offer capital, but America has plenty of capital. We have trillion-dollar companies. What are these Middle Eastern states? They're kind of like trillion-dollar oil companies. We have trillion-dollar companies and very deep financial markets. Microsoft could issue hundreds of billions of dollars of bonds and they can pay for these clusters.

Another argument being made, which is worth taking seriously, is that if we don't work with the UAE or with these Middle Eastern countries, they're just going to go to China. They're going to build data centers and pour money into AI regardless. If we don't work with them, they'll just support China.

There's some merit to the argument in the sense that we should be doing benefit-sharing with them. On the road to AGI, there should be two tiers of coalitions. There should be a narrow coalition of democracies that's developing AGI. Then there should be a broader coalition of other countries, including dictatorships, and we should offer them some of the benefits of AI.

If the UAE wants to use AI products, run Meta recommendation engines, or run the last-generation models, that's fine. By default, they just wouldn't have had this seat at the AGI table. So they have some money, but a lot of people have money.

The only reason they're getting this seat at the AGI table and giving these dictators this leverage over this extremely important national security technology, is because we're getting them excited and offering it to them.
Dwarkesh Patel
Who specifically is doing this? Who are the companies who are going there to fundraise?

**Extracted Belief:**

Collaboration in AI development should involve benefit-sharing between a core group of democracies leading the research and a broader coalition of nations, including authoritarian regimes.

**Context:**

Leopold Aschenbrenner proposes a two-tiered approach to international AI collaboration, advocating for both a narrow group of democratic nations leading development and a broader coalition with access to benefits.

**Justification:**

He argues that offering benefits to a wider range of countries, including those with less democratic governance, is necessary to ensure wider adoption and prevent them from aligning with rivals like China.

--------

## Chunk 134

**Chunk:**

Dwarkesh Patel
Companies realize that scaling is a thing. Obviously their whole plans are contingent on scaling. So they understand that in 2028 we're going to be building 10 GW data centers.

At that point, the people who can keep up are Big Tech, potentially at the edge of their capabilities, sovereign wealth fund-funded things, and also major countries like America and China. What's their plan? With the AI labs, what's their plan given this landscape? Do they not want the leverage of being in the United States?
Leopold Aschenbrenner
The Middle East does offer capital, but America has plenty of capital. We have trillion-dollar companies. What are these Middle Eastern states? They're kind of like trillion-dollar oil companies. We have trillion-dollar companies and very deep financial markets. Microsoft could issue hundreds of billions of dollars of bonds and they can pay for these clusters.

Another argument being made, which is worth taking seriously, is that if we don't work with the UAE or with these Middle Eastern countries, they're just going to go to China. They're going to build data centers and pour money into AI regardless. If we don't work with them, they'll just support China.

There's some merit to the argument in the sense that we should be doing benefit-sharing with them. On the road to AGI, there should be two tiers of coalitions. There should be a narrow coalition of democracies that's developing AGI. Then there should be a broader coalition of other countries, including dictatorships, and we should offer them some of the benefits of AI.

If the UAE wants to use AI products, run Meta recommendation engines, or run the last-generation models, that's fine. By default, they just wouldn't have had this seat at the AGI table. So they have some money, but a lot of people have money.

The only reason they're getting this seat at the AGI table and giving these dictators this leverage over this extremely important national security technology, is because we're getting them excited and offering it to them.
Dwarkesh Patel
Who specifically is doing this? Who are the companies who are going there to fundraise?

**Extracted Belief:**

Middle Eastern nations are gaining undue influence over AI development by being offered opportunities to participate, despite lacking significant contributions to the field.

**Context:**

Leopold Aschenbrenner expresses concern about the influence of Middle Eastern countries on AI development, suggesting that they are being given leverage that is not commensurate with their contributions.

**Justification:**

He argues that they have financial resources but lack expertise in AI, implying that their involvement is primarily driven by investment and not merit.

--------

## Chunk 135

**Chunk:**

Dwarkesh Patel
Who specifically is doing this? Who are the companies who are going there to fundraise?
Leopold Aschenbrenner
It’s been reported that Sam Altman is trying to raise $7 trillion or whatever for a chip project. It's unclear how many of the clusters will be there, but definitely stuff is happening.

There’s another reason I'm a little suspicious of this argument that if the US doesn't work with them, they'll go to China. I've heard from multiple people — not from my time at OpenAI, and I haven't seen the memo — that at some point several years ago, OpenAI leadership had laid out a plan to fund and sell AGI by starting a bidding war between the governments of the United States, China, and Russia.

It's surprising to me that they're willing to sell AGI to the Chinese and Russian governments. There's also something that feels eerily familiar about starting this bidding war and then playing them off each other, saying, "well, if you don't do this, China will do it."
Dwarkesh Patel
Interesting. That's pretty fucked up. 

Suppose you're right. We ended up in this place because, as one of our friends put it, the Middle East has billions or trillions of dollars up for persuasion like no other place in the world.

**Extracted Belief:**

OpenAI leadership had a plan to fund and sell AGI by starting a bidding war between the US, China, and Russia.

**Context:**

Leopold Aschenbrenner expressed suspicion about the argument that if the US doesn't work with the Middle East, they will go to China. He countered this argument by sharing information about a past plan by OpenAI leadership.

**Justification:**

Leopold Aschenbrenner stated he had heard this information from multiple people, not from his time at OpenAI, and that he had not seen a written memo.

--------

## Chunk 136

**Chunk:**

Dwarkesh Patel
Who specifically is doing this? Who are the companies who are going there to fundraise?
Leopold Aschenbrenner
It’s been reported that Sam Altman is trying to raise $7 trillion or whatever for a chip project. It's unclear how many of the clusters will be there, but definitely stuff is happening.

There’s another reason I'm a little suspicious of this argument that if the US doesn't work with them, they'll go to China. I've heard from multiple people — not from my time at OpenAI, and I haven't seen the memo — that at some point several years ago, OpenAI leadership had laid out a plan to fund and sell AGI by starting a bidding war between the governments of the United States, China, and Russia.

It's surprising to me that they're willing to sell AGI to the Chinese and Russian governments. There's also something that feels eerily familiar about starting this bidding war and then playing them off each other, saying, "well, if you don't do this, China will do it."
Dwarkesh Patel
Interesting. That's pretty fucked up. 

Suppose you're right. We ended up in this place because, as one of our friends put it, the Middle East has billions or trillions of dollars up for persuasion like no other place in the world.

**Extracted Belief:**

OpenAI leadership was willing to sell AGI to the Chinese and Russian governments.

**Context:**

Leopold Aschenbrenner expressed surprise at the potential sale of AGI to the Chinese and Russian governments.

**Justification:**

Leopold Aschenbrenner stated he found it surprising that OpenAI leadership would be willing to sell AGI to these governments.

--------

## Chunk 137

**Chunk:**

Dwarkesh Patel
Interesting. That's pretty fucked up. 

Suppose you're right. We ended up in this place because, as one of our friends put it, the Middle East has billions or trillions of dollars up for persuasion like no other place in the world.
Leopold Aschenbrenner
With little accountability. There’s no Microsoft board. It's only the dictator.
Dwarkesh Patel
Let's say you're right, that you shouldn't have gotten them excited about AGI in the first place. Now we're in a place where they are excited about AGI and they're like, "fuck, we want to have GPT-5 while you're going to be off building superintelligence. This Atoms for Peace thing doesn't work for us." If you're in this place, don't they already have the leverage?

**Extracted Belief:**

The United Arab Emirates (UAE) has little accountability in its decision-making regarding artificial general intelligence (AGI).

**Context:**

Leopold Aschenbrenner is expressing his concern about the UAE's involvement in AGI development, specifically highlighting the lack of oversight and accountability in their decision-making process.

**Justification:**

He points out that there is no equivalent of a board like Microsoft's to provide oversight and accountability, implying that the dictator's decisions are largely unchecked.

--------

## Chunk 138

**Chunk:**

Dwarkesh Patel
Let's say you're right, that you shouldn't have gotten them excited about AGI in the first place. Now we're in a place where they are excited about AGI and they're like, "fuck, we want to have GPT-5 while you're going to be off building superintelligence. This Atoms for Peace thing doesn't work for us." If you're in this place, don't they already have the leverage?
Leopold Aschenbrenner
The UAE on its own is not competitive. They're already export-controlled. You're not supposed to ship Nvidia chips over there. It's not like they have any of the leading AI labs. They have money, but it's hard to just translate money into progress.
Dwarkesh Patel
But I want to go back to other things you've been saying in laying out your vision. There's this almost industrial process of putting in the compute and algorithms, adding that up, and getting AGI on the other end. If it's something more like that, then the case for somebody being able to catch up rapidly seems more compelling than if it's some bespoke...

**Extracted Belief:**

The United Arab Emirates (UAE) is not competitive in the field of artificial intelligence (AI) due to export control restrictions and a lack of leading AI labs.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's argument that the UAE may have leverage in the development of AGI due to its wealth and interest.

**Justification:**

Aschenbrenner states that the UAE is export-controlled, meaning that they cannot receive certain technology, like Nvidia chips, which are essential for AI development. Additionally, he mentions that the UAE lacks leading AI labs.

--------

## Chunk 139

**Chunk:**

Dwarkesh Patel
Let's say you're right, that you shouldn't have gotten them excited about AGI in the first place. Now we're in a place where they are excited about AGI and they're like, "fuck, we want to have GPT-5 while you're going to be off building superintelligence. This Atoms for Peace thing doesn't work for us." If you're in this place, don't they already have the leverage?
Leopold Aschenbrenner
The UAE on its own is not competitive. They're already export-controlled. You're not supposed to ship Nvidia chips over there. It's not like they have any of the leading AI labs. They have money, but it's hard to just translate money into progress.
Dwarkesh Patel
But I want to go back to other things you've been saying in laying out your vision. There's this almost industrial process of putting in the compute and algorithms, adding that up, and getting AGI on the other end. If it's something more like that, then the case for somebody being able to catch up rapidly seems more compelling than if it's some bespoke...

**Extracted Belief:**

It is difficult to translate money into progress in the field of artificial intelligence (AI).

**Context:**

Leopold Aschenbrenner is continuing his explanation of why the UAE is not a significant threat in the AI race.

**Justification:**

Aschenbrenner states that while the UAE has money, it is difficult to turn that money into advancements in AI.

--------

## Chunk 140

**Chunk:**

Dwarkesh Patel
But I want to go back to other things you've been saying in laying out your vision. There's this almost industrial process of putting in the compute and algorithms, adding that up, and getting AGI on the other end. If it's something more like that, then the case for somebody being able to catch up rapidly seems more compelling than if it's some bespoke...
Leopold Aschenbrenner
Well, if they can steal the algorithms and if they can steal the weights, that’s really important.
Dwarkesh Patel
How easy would it be for an actor to steal the things that are not the trivial released things, like Scarlett Johansson's voice, but the RL things we're talking about, the unhobblings?

**Extracted Belief:**

Stealing algorithms and weights is extremely important in the development of artificial general intelligence (AGI).

**Context:**

Leopold Aschenbrenner is discussing the potential for China to catch up in the race for AGI, and he believes that stealing algorithms and weights would be a significant advantage.

**Justification:**

The statement is based on Leopold Aschenbrenner's expertise in the field of AI and his understanding of the importance of these components in AGI development.

--------

## Chunk 141

**Chunk:**

Dwarkesh Patel
How easy would it be for an actor to steal the things that are not the trivial released things, like Scarlett Johansson's voice, but the RL things we're talking about, the unhobblings?
Leopold Aschenbrenner
It’s all extremely easy. They don’t make the claim that it’s hard. DeepMind put out their Frontier Safety Framework and they lay out security levels, zero to four. Four is resistant to state activity. They say, we're at level zero. Just recently, there was an indictment of a guy who stole a bunch of really important AI code and went to China with it. All he had to do to steal the code was copy it, put it into Apple Notes, and export it as a PDF. That got past their monitoring.

Google has the best security of any of the AI labs probably, because they have the Google infrastructure. I would think of the security of a startup. What does security of a startup look like? It's not that good. It's easy to steal.
Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?

**Extracted Belief:**

Current AI security measures are ineffective, even at major companies like Google.

**Context:**

Leopold Aschenbrenner is responding to a question about the ease of stealing AI algorithms and code, specifically focusing on the security of DeepMind and Google.

**Justification:**

He cites DeepMind's Frontier Safety Framework, which acknowledges that their security level is at zero out of four, indicating a lack of resistance to state-level attacks. He also refers to the recent indictment of a person who stole AI code and escaped detection by simply copying it and exporting it as a PDF.

--------

## Chunk 142

**Chunk:**

Dwarkesh Patel
How easy would it be for an actor to steal the things that are not the trivial released things, like Scarlett Johansson's voice, but the RL things we're talking about, the unhobblings?
Leopold Aschenbrenner
It’s all extremely easy. They don’t make the claim that it’s hard. DeepMind put out their Frontier Safety Framework and they lay out security levels, zero to four. Four is resistant to state activity. They say, we're at level zero. Just recently, there was an indictment of a guy who stole a bunch of really important AI code and went to China with it. All he had to do to steal the code was copy it, put it into Apple Notes, and export it as a PDF. That got past their monitoring.

Google has the best security of any of the AI labs probably, because they have the Google infrastructure. I would think of the security of a startup. What does security of a startup look like? It's not that good. It's easy to steal.
Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?

**Extracted Belief:**

Security measures at startups are generally weaker than those at major companies like Google, making it easier to steal algorithms and code from them.

**Context:**

Leopold Aschenbrenner continues to discuss the ease of stealing AI code and algorithms, comparing the security measures of startups to those of established companies like Google.

**Justification:**

He explicitly states that the security of startups is 'not that good' and that it's 'easy to steal' from them, implying a significant difference in security levels compared to larger companies.

--------

## Chunk 143

**Chunk:**

Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?
Leopold Aschenbrenner
There are a few different things, right? One threat model is just them stealing the weights themselves. The weights one is particularly insane because they can just steal the literal end product — just make a replica of the atomic bomb — and then they're ready to go. That one is extremely important around the time we have AGI and superintelligence because China can build a big cluster by default. We'd have a big lead because we have the better scientists, but if we make the superintelligence and they just steal it, they're off to the races.

Weights are a little bit less important right now because who cares if they steal the GPT-4 weights. We still have to get started on weight security now because if we think there’s AGI by 2027, this stuff is going to take a while. It's not just going to be like, "oh, we do some access control." If you actually want to be resistant to Chinese espionage, it needs to be much more intense.

The thing that people aren't paying enough attention to is the secrets. The compute stuff is sexy, but people underrate the secrets. The half an order of magnitude a year is just by default, sort of algorithmic progress. That's huge. If we have a few years of lead, by default, that's a 10-30x, 100x bigger cluster, if we protect them.

There's this additional layer of the data wall. We have to get through the data wall. That means we actually have to figure out some sort of basic new paradigm. So it’s the “AlphaGo step two.” “AlphaGo step one” learns from human imitation. “AlphaGo step two” is the kind of self-play RL thing that everyone's working on right now. Maybe we're going to crack it. If China can't steal that, then they're stuck. If they can steal it, they're off to the races.
Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?

**Extracted Belief:**

Stealing AI weights is a significant threat, particularly in the development of AGI and superintelligence, as it allows for a quick replica of the advanced technology.

**Context:**

Leopold Aschenbrenner is describing the potential security risks associated with AI development, highlighting the vulnerability of weights, which represent the trained parameters of a model.

**Justification:**

Aschenbrenner compares stealing AI weights to stealing the 'atomic bomb,' emphasizing the potential for quick replication and deployment of the stolen technology.

--------

## Chunk 144

**Chunk:**

Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?
Leopold Aschenbrenner
There are a few different things, right? One threat model is just them stealing the weights themselves. The weights one is particularly insane because they can just steal the literal end product — just make a replica of the atomic bomb — and then they're ready to go. That one is extremely important around the time we have AGI and superintelligence because China can build a big cluster by default. We'd have a big lead because we have the better scientists, but if we make the superintelligence and they just steal it, they're off to the races.

Weights are a little bit less important right now because who cares if they steal the GPT-4 weights. We still have to get started on weight security now because if we think there’s AGI by 2027, this stuff is going to take a while. It's not just going to be like, "oh, we do some access control." If you actually want to be resistant to Chinese espionage, it needs to be much more intense.

The thing that people aren't paying enough attention to is the secrets. The compute stuff is sexy, but people underrate the secrets. The half an order of magnitude a year is just by default, sort of algorithmic progress. That's huge. If we have a few years of lead, by default, that's a 10-30x, 100x bigger cluster, if we protect them.

There's this additional layer of the data wall. We have to get through the data wall. That means we actually have to figure out some sort of basic new paradigm. So it’s the “AlphaGo step two.” “AlphaGo step one” learns from human imitation. “AlphaGo step two” is the kind of self-play RL thing that everyone's working on right now. Maybe we're going to crack it. If China can't steal that, then they're stuck. If they can steal it, they're off to the races.
Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?

**Extracted Belief:**

China has the capacity to build large AI clusters, and therefore poses a potential threat to steal and replicate advanced AI technologies like superintelligence.

**Context:**

Aschenbrenner is discussing the potential for China to quickly catch up in AI development by leveraging its resources to build large computing infrastructure.

**Justification:**

Aschenbrenner highlights China's capability to construct large AI clusters, indicating their potential to catch up or even surpass other countries in the development and deployment of AI.

--------

## Chunk 145

**Chunk:**

Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?
Leopold Aschenbrenner
There are a few different things, right? One threat model is just them stealing the weights themselves. The weights one is particularly insane because they can just steal the literal end product — just make a replica of the atomic bomb — and then they're ready to go. That one is extremely important around the time we have AGI and superintelligence because China can build a big cluster by default. We'd have a big lead because we have the better scientists, but if we make the superintelligence and they just steal it, they're off to the races.

Weights are a little bit less important right now because who cares if they steal the GPT-4 weights. We still have to get started on weight security now because if we think there’s AGI by 2027, this stuff is going to take a while. It's not just going to be like, "oh, we do some access control." If you actually want to be resistant to Chinese espionage, it needs to be much more intense.

The thing that people aren't paying enough attention to is the secrets. The compute stuff is sexy, but people underrate the secrets. The half an order of magnitude a year is just by default, sort of algorithmic progress. That's huge. If we have a few years of lead, by default, that's a 10-30x, 100x bigger cluster, if we protect them.

There's this additional layer of the data wall. We have to get through the data wall. That means we actually have to figure out some sort of basic new paradigm. So it’s the “AlphaGo step two.” “AlphaGo step one” learns from human imitation. “AlphaGo step two” is the kind of self-play RL thing that everyone's working on right now. Maybe we're going to crack it. If China can't steal that, then they're stuck. If they can steal it, they're off to the races.
Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?

**Extracted Belief:**

The United States currently holds a significant lead in AI development due to its superior scientists and published research.

**Context:**

Aschenbrenner acknowledges the US advantage in AI, attributing it to the excellence of its research community and the open dissemination of research findings.

**Justification:**

Aschenbrenner emphasizes the US's lead in AI development, highlighting the role of its scientific community and the previous practice of open research publications.

--------

## Chunk 146

**Chunk:**

Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?
Leopold Aschenbrenner
There are a few different things, right? One threat model is just them stealing the weights themselves. The weights one is particularly insane because they can just steal the literal end product — just make a replica of the atomic bomb — and then they're ready to go. That one is extremely important around the time we have AGI and superintelligence because China can build a big cluster by default. We'd have a big lead because we have the better scientists, but if we make the superintelligence and they just steal it, they're off to the races.

Weights are a little bit less important right now because who cares if they steal the GPT-4 weights. We still have to get started on weight security now because if we think there’s AGI by 2027, this stuff is going to take a while. It's not just going to be like, "oh, we do some access control." If you actually want to be resistant to Chinese espionage, it needs to be much more intense.

The thing that people aren't paying enough attention to is the secrets. The compute stuff is sexy, but people underrate the secrets. The half an order of magnitude a year is just by default, sort of algorithmic progress. That's huge. If we have a few years of lead, by default, that's a 10-30x, 100x bigger cluster, if we protect them.

There's this additional layer of the data wall. We have to get through the data wall. That means we actually have to figure out some sort of basic new paradigm. So it’s the “AlphaGo step two.” “AlphaGo step one” learns from human imitation. “AlphaGo step two” is the kind of self-play RL thing that everyone's working on right now. Maybe we're going to crack it. If China can't steal that, then they're stuck. If they can steal it, they're off to the races.
Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?

**Extracted Belief:**

The United States could maintain its lead in AI by keeping key developments secret, thereby hindering other countries' ability to catch up.

**Context:**

Aschenbrenner suggests that keeping critical AI developments secret could provide the US with a significant advantage in the AI race.

**Justification:**

Aschenbrenner argues that secrecy surrounding AI advancements could prevent other countries from gaining access to these developments and thus hinder their progress.

--------

## Chunk 147

**Chunk:**

Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?
Leopold Aschenbrenner
There are a few different things, right? One threat model is just them stealing the weights themselves. The weights one is particularly insane because they can just steal the literal end product — just make a replica of the atomic bomb — and then they're ready to go. That one is extremely important around the time we have AGI and superintelligence because China can build a big cluster by default. We'd have a big lead because we have the better scientists, but if we make the superintelligence and they just steal it, they're off to the races.

Weights are a little bit less important right now because who cares if they steal the GPT-4 weights. We still have to get started on weight security now because if we think there’s AGI by 2027, this stuff is going to take a while. It's not just going to be like, "oh, we do some access control." If you actually want to be resistant to Chinese espionage, it needs to be much more intense.

The thing that people aren't paying enough attention to is the secrets. The compute stuff is sexy, but people underrate the secrets. The half an order of magnitude a year is just by default, sort of algorithmic progress. That's huge. If we have a few years of lead, by default, that's a 10-30x, 100x bigger cluster, if we protect them.

There's this additional layer of the data wall. We have to get through the data wall. That means we actually have to figure out some sort of basic new paradigm. So it’s the “AlphaGo step two.” “AlphaGo step one” learns from human imitation. “AlphaGo step two” is the kind of self-play RL thing that everyone's working on right now. Maybe we're going to crack it. If China can't steal that, then they're stuck. If they can steal it, they're off to the races.
Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?

**Extracted Belief:**

Algorithmic progress in AI is advancing rapidly, at a rate of about half an order of magnitude per year.

**Context:**

Aschenbrenner emphasizes the rapid pace of AI development, highlighting the significant advancements in algorithmic efficiency and performance.

**Justification:**

Aschenbrenner states that AI algorithms are improving at an average rate of half an order of magnitude per year, indicating a significant and continuous growth trajectory.

--------

## Chunk 148

**Chunk:**

Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?
Leopold Aschenbrenner
There are a few different things, right? One threat model is just them stealing the weights themselves. The weights one is particularly insane because they can just steal the literal end product — just make a replica of the atomic bomb — and then they're ready to go. That one is extremely important around the time we have AGI and superintelligence because China can build a big cluster by default. We'd have a big lead because we have the better scientists, but if we make the superintelligence and they just steal it, they're off to the races.

Weights are a little bit less important right now because who cares if they steal the GPT-4 weights. We still have to get started on weight security now because if we think there’s AGI by 2027, this stuff is going to take a while. It's not just going to be like, "oh, we do some access control." If you actually want to be resistant to Chinese espionage, it needs to be much more intense.

The thing that people aren't paying enough attention to is the secrets. The compute stuff is sexy, but people underrate the secrets. The half an order of magnitude a year is just by default, sort of algorithmic progress. That's huge. If we have a few years of lead, by default, that's a 10-30x, 100x bigger cluster, if we protect them.

There's this additional layer of the data wall. We have to get through the data wall. That means we actually have to figure out some sort of basic new paradigm. So it’s the “AlphaGo step two.” “AlphaGo step one” learns from human imitation. “AlphaGo step two” is the kind of self-play RL thing that everyone's working on right now. Maybe we're going to crack it. If China can't steal that, then they're stuck. If they can steal it, they're off to the races.
Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?

**Extracted Belief:**

A significant challenge in AI development is overcoming the 'data wall,' which refers to the need for novel methods to generate and acquire data for AI training.

**Context:**

Aschenbrenner describes the 'data wall' as a major hurdle in AI development, emphasizing the requirement for innovative solutions to overcome limitations in data availability.

**Justification:**

Aschenbrenner introduces the 'data wall' as a significant challenge, highlighting the need for new paradigms and approaches to acquire and generate sufficient data for training AI models.

--------

## Chunk 149

**Chunk:**

Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?
Leopold Aschenbrenner
There are a few different things, right? One threat model is just them stealing the weights themselves. The weights one is particularly insane because they can just steal the literal end product — just make a replica of the atomic bomb — and then they're ready to go. That one is extremely important around the time we have AGI and superintelligence because China can build a big cluster by default. We'd have a big lead because we have the better scientists, but if we make the superintelligence and they just steal it, they're off to the races.

Weights are a little bit less important right now because who cares if they steal the GPT-4 weights. We still have to get started on weight security now because if we think there’s AGI by 2027, this stuff is going to take a while. It's not just going to be like, "oh, we do some access control." If you actually want to be resistant to Chinese espionage, it needs to be much more intense.

The thing that people aren't paying enough attention to is the secrets. The compute stuff is sexy, but people underrate the secrets. The half an order of magnitude a year is just by default, sort of algorithmic progress. That's huge. If we have a few years of lead, by default, that's a 10-30x, 100x bigger cluster, if we protect them.

There's this additional layer of the data wall. We have to get through the data wall. That means we actually have to figure out some sort of basic new paradigm. So it’s the “AlphaGo step two.” “AlphaGo step one” learns from human imitation. “AlphaGo step two” is the kind of self-play RL thing that everyone's working on right now. Maybe we're going to crack it. If China can't steal that, then they're stuck. If they can steal it, they're off to the races.
Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?

**Extracted Belief:**

Self-play reinforcement learning (RL) is a key area of research currently being explored to overcome the 'data wall' in AI development.

**Context:**

Aschenbrenner discusses the potential of self-play RL as a solution to the 'data wall' problem, highlighting its significance in the current AI research landscape.

**Justification:**

Aschenbrenner mentions self-play RL as a current area of focus in AI research, suggesting its potential to address the data limitations and enable further progress.

--------

## Chunk 150

**Chunk:**

Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?
Leopold Aschenbrenner
There are a few different things, right? One threat model is just them stealing the weights themselves. The weights one is particularly insane because they can just steal the literal end product — just make a replica of the atomic bomb — and then they're ready to go. That one is extremely important around the time we have AGI and superintelligence because China can build a big cluster by default. We'd have a big lead because we have the better scientists, but if we make the superintelligence and they just steal it, they're off to the races.

Weights are a little bit less important right now because who cares if they steal the GPT-4 weights. We still have to get started on weight security now because if we think there’s AGI by 2027, this stuff is going to take a while. It's not just going to be like, "oh, we do some access control." If you actually want to be resistant to Chinese espionage, it needs to be much more intense.

The thing that people aren't paying enough attention to is the secrets. The compute stuff is sexy, but people underrate the secrets. The half an order of magnitude a year is just by default, sort of algorithmic progress. That's huge. If we have a few years of lead, by default, that's a 10-30x, 100x bigger cluster, if we protect them.

There's this additional layer of the data wall. We have to get through the data wall. That means we actually have to figure out some sort of basic new paradigm. So it’s the “AlphaGo step two.” “AlphaGo step one” learns from human imitation. “AlphaGo step two” is the kind of self-play RL thing that everyone's working on right now. Maybe we're going to crack it. If China can't steal that, then they're stuck. If they can steal it, they're off to the races.
Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?

**Extracted Belief:**

The success of AI development relies on both the fundamental approach and the specific details of its implementation.

**Context:**

Aschenbrenner explains the importance of both fundamental concepts and specific implementation details in achieving successful AI development.

**Justification:**

Aschenbrenner distinguishes between the fundamental approach and the specific details, highlighting their combined importance in achieving a successful AI system.

--------

## Chunk 151

**Chunk:**

Dwarkesh Patel
Even if that's the case, a lot of your post is making the argument for why we are going to get the intelligence explosion. If we have somebody with the intuition of an Alec Radford to come up with all these ideas, that intuition is extremely valuable and you can scale that up.

If it's just intuition, then that's not going to be just in the code, right? Also because of export controls, these countries are going to have slightly different hardware. You're going to have to make different trade-offs and probably rewrite things to be compatible with that.

Is it just a matter of getting the right pen drive and plugging it into the gigawatt data center next to the Three Gorges Dam and then you're off to the races?
Leopold Aschenbrenner
There are a few different things, right? One threat model is just them stealing the weights themselves. The weights one is particularly insane because they can just steal the literal end product — just make a replica of the atomic bomb — and then they're ready to go. That one is extremely important around the time we have AGI and superintelligence because China can build a big cluster by default. We'd have a big lead because we have the better scientists, but if we make the superintelligence and they just steal it, they're off to the races.

Weights are a little bit less important right now because who cares if they steal the GPT-4 weights. We still have to get started on weight security now because if we think there’s AGI by 2027, this stuff is going to take a while. It's not just going to be like, "oh, we do some access control." If you actually want to be resistant to Chinese espionage, it needs to be much more intense.

The thing that people aren't paying enough attention to is the secrets. The compute stuff is sexy, but people underrate the secrets. The half an order of magnitude a year is just by default, sort of algorithmic progress. That's huge. If we have a few years of lead, by default, that's a 10-30x, 100x bigger cluster, if we protect them.

There's this additional layer of the data wall. We have to get through the data wall. That means we actually have to figure out some sort of basic new paradigm. So it’s the “AlphaGo step two.” “AlphaGo step one” learns from human imitation. “AlphaGo step two” is the kind of self-play RL thing that everyone's working on right now. Maybe we're going to crack it. If China can't steal that, then they're stuck. If they can steal it, they're off to the races.
Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?

**Extracted Belief:**

Large-scale engineering efforts are essential for making AI training runs work effectively, and these efforts are more likely to be replicated by other countries than the fundamental breakthroughs in AI.

**Context:**

Aschenbrenner acknowledges the importance of large-scale engineering in AI development but suggests that this aspect is more readily replicated than the fundamental breakthroughs in AI algorithms.

**Justification:**

Aschenbrenner distinguishes between the challenges of replicating engineering aspects of AI versus the more challenging task of replicating breakthroughs in AI algorithms and theoretical foundations.

--------

## Chunk 152

**Chunk:**

Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?
Leopold Aschenbrenner
There are a few layers to this. At the top is the fundamental approach. On pre-training it might be unsupervised learning, next token prediction, training on the entire Internet. You actually get a lot of juice out of that already. That one's very quick to communicate.

Then there's a lot of details that matter, and you were talking about this earlier. It's probably going to be somewhat obvious in retrospect, or there's going to be some not too complicated thing that'll work, but there's going to be a lot of details to get that.
Dwarkesh Patel
If that's true, then again, why do we think that getting state-level security in these startups will prevent China from catching up? It’s just like, "oh, we know some sort of self-play RL will be required to get past the data wall."

It's going to be solved by 2027, right? It's not that hard.

**Extracted Belief:**

The fundamental approach to pre-training large language models often involves unsupervised learning, next token prediction, and training on the entire internet.

**Context:**

Leopold Aschenbrenner explains the layers of secrets in AI development, starting with the fundamental approach to pre-training.

**Justification:**

He states that this approach is "very quick to communicate" and "you actually get a lot of juice out of that already."

--------

## Chunk 153

**Chunk:**

Dwarkesh Patel
Whatever that thing is, can I literally write it down on the back of a napkin? If it's that easy, then why is it so hard for them to figure it out? If it's more about the intuitions, then don't you just have to hire Alec Radford? What are you copying down?
Leopold Aschenbrenner
There are a few layers to this. At the top is the fundamental approach. On pre-training it might be unsupervised learning, next token prediction, training on the entire Internet. You actually get a lot of juice out of that already. That one's very quick to communicate.

Then there's a lot of details that matter, and you were talking about this earlier. It's probably going to be somewhat obvious in retrospect, or there's going to be some not too complicated thing that'll work, but there's going to be a lot of details to get that.
Dwarkesh Patel
If that's true, then again, why do we think that getting state-level security in these startups will prevent China from catching up? It’s just like, "oh, we know some sort of self-play RL will be required to get past the data wall."

It's going to be solved by 2027, right? It's not that hard.

**Extracted Belief:**

The details of implementing AI algorithms are crucial and will likely be considered obvious in retrospect.

**Context:**

Leopold Aschenbrenner suggests that the details of AI implementation are significant and might seem straightforward after they are achieved.

**Justification:**

He states, "it's probably going to be somewhat obvious in retrospect, or there's going to be some not too complicated thing that'll work, but there's going to be a lot of details to get that."

--------

## Chunk 154

**Chunk:**

Dwarkesh Patel
If that's true, then again, why do we think that getting state-level security in these startups will prevent China from catching up? It’s just like, "oh, we know some sort of self-play RL will be required to get past the data wall."

It's going to be solved by 2027, right? It's not that hard.
Leopold Aschenbrenner
The US, and the leading labs in the United States, have this huge lead. By default, China actually has some good LLMs because they're just using open source code, like Llama. People really underrate both the divergence on algorithmic progress and the lead the US would have by default because all this stuff was published until recently.

Look at Chinchilla Scaling laws, MoE papers, transformers. All that stuff was published. That's why open source is good and why China can make some good models. Now, they're not publishing it anymore. If we actually kept it secret, it would be a huge edge.

To your point about tacit knowledge and Alec Radford, there's another layer at the bottom that is something about large-scale engineering work to make these big training runs work. That is a little bit more like tacit knowledge, but China will be able to figure that out. It's engineering schlep, and they're going to figure out how to do it.
Dwarkesh Patel
Why can't they figure that out, but not how to get the RL thing working?

**Extracted Belief:**

The United States and its leading labs have a significant advantage in AI development.

**Context:**

Leopold Aschenbrenner asserts that the US has a significant lead in AI development, highlighting the importance of secrecy in maintaining that advantage.

**Justification:**

He points to the fact that the US has been publishing research on key AI concepts like Chinchilla Scaling laws, MoE papers, and transformers, while China has been able to utilize open-source code, like Llama, to build good LLMs.

--------

## Chunk 155

**Chunk:**

Dwarkesh Patel
If that's true, then again, why do we think that getting state-level security in these startups will prevent China from catching up? It’s just like, "oh, we know some sort of self-play RL will be required to get past the data wall."

It's going to be solved by 2027, right? It's not that hard.
Leopold Aschenbrenner
The US, and the leading labs in the United States, have this huge lead. By default, China actually has some good LLMs because they're just using open source code, like Llama. People really underrate both the divergence on algorithmic progress and the lead the US would have by default because all this stuff was published until recently.

Look at Chinchilla Scaling laws, MoE papers, transformers. All that stuff was published. That's why open source is good and why China can make some good models. Now, they're not publishing it anymore. If we actually kept it secret, it would be a huge edge.

To your point about tacit knowledge and Alec Radford, there's another layer at the bottom that is something about large-scale engineering work to make these big training runs work. That is a little bit more like tacit knowledge, but China will be able to figure that out. It's engineering schlep, and they're going to figure out how to do it.
Dwarkesh Patel
Why can't they figure that out, but not how to get the RL thing working?

**Extracted Belief:**

Open-source AI development allows China to create good language models (LLMs).

**Context:**

Leopold Aschenbrenner argues that the open-source nature of AI development has enabled China to create good language models.

**Justification:**

He cites the example of China using open-source code like Llama to build good LLMs, suggesting that open-source development provides China with access to crucial AI technology.

--------

## Chunk 156

**Chunk:**

Dwarkesh Patel
If that's true, then again, why do we think that getting state-level security in these startups will prevent China from catching up? It’s just like, "oh, we know some sort of self-play RL will be required to get past the data wall."

It's going to be solved by 2027, right? It's not that hard.
Leopold Aschenbrenner
The US, and the leading labs in the United States, have this huge lead. By default, China actually has some good LLMs because they're just using open source code, like Llama. People really underrate both the divergence on algorithmic progress and the lead the US would have by default because all this stuff was published until recently.

Look at Chinchilla Scaling laws, MoE papers, transformers. All that stuff was published. That's why open source is good and why China can make some good models. Now, they're not publishing it anymore. If we actually kept it secret, it would be a huge edge.

To your point about tacit knowledge and Alec Radford, there's another layer at the bottom that is something about large-scale engineering work to make these big training runs work. That is a little bit more like tacit knowledge, but China will be able to figure that out. It's engineering schlep, and they're going to figure out how to do it.
Dwarkesh Patel
Why can't they figure that out, but not how to get the RL thing working?

**Extracted Belief:**

Secrecy in AI research would give the US a significant advantage.

**Context:**

Leopold Aschenbrenner argues that keeping AI research secret would be beneficial for the US.

**Justification:**

He explains that the US has been sharing valuable AI research publicly, allowing China to catch up by using open-source code and models. He asserts that maintaining secrecy would prevent China from easily accessing this critical information.

--------

## Chunk 157

**Chunk:**

Dwarkesh Patel
If that's true, then again, why do we think that getting state-level security in these startups will prevent China from catching up? It’s just like, "oh, we know some sort of self-play RL will be required to get past the data wall."

It's going to be solved by 2027, right? It's not that hard.
Leopold Aschenbrenner
The US, and the leading labs in the United States, have this huge lead. By default, China actually has some good LLMs because they're just using open source code, like Llama. People really underrate both the divergence on algorithmic progress and the lead the US would have by default because all this stuff was published until recently.

Look at Chinchilla Scaling laws, MoE papers, transformers. All that stuff was published. That's why open source is good and why China can make some good models. Now, they're not publishing it anymore. If we actually kept it secret, it would be a huge edge.

To your point about tacit knowledge and Alec Radford, there's another layer at the bottom that is something about large-scale engineering work to make these big training runs work. That is a little bit more like tacit knowledge, but China will be able to figure that out. It's engineering schlep, and they're going to figure out how to do it.
Dwarkesh Patel
Why can't they figure that out, but not how to get the RL thing working?

**Extracted Belief:**

China will eventually figure out the large-scale engineering aspects of AI training.

**Context:**

Leopold Aschenbrenner discusses the potential for China to overcome the US lead in AI by developing the engineering capabilities for large-scale AI training.

**Justification:**

He acknowledges that the US has an advantage in AI engineering, but suggests that China, with its resources and dedication, will eventually acquire the necessary knowledge and skills to train large-scale AI models.

--------

## Chunk 158

**Chunk:**

Dwarkesh Patel
Why can't they figure that out, but not how to get the RL thing working?
Leopold Aschenbrenner
I don't know. Germany during World War II went down the wrong path with heavy water. There's an amazing anecdote in The Making of the Atomic Bomb about this.

Secrecy was one of the most contentious issues early on. Leo Szilard really thought a nuclear chain reaction and an atomic bomb were possible. He went around saying, "this is going to be of enormous strategic and military importance." A lot of people didn't believe it or thought, "maybe this is possible, but I'm going to act as though it's not, and science should be open."

In the early days, there had been some incorrect measurements made on graphite as a moderator. Germany thought graphite wasn't going to work, so they had to do heavy water. But then Enrico Fermi made new measurements indicating that graphite would work. This was really important.

Szilard assaulted Fermi with another secrecy appeal and Fermi was pissed off, throwing a temper tantrum. He thought it was absurd, saying, "come on, this is crazy." But Szilard persisted, and they roped in another guy, George Pegram. In the end, Fermi didn't publish it.

That was just in time. Fermi not publishing meant that the Nazis didn't figure out graphite would work. They went down the path of heavy water, which was the wrong path. This is a key reason why the German project didn't work out. They were way behind.

We face a similar situation now. Are we just going to instantly leak how to get past the data wall and what the next paradigm is? Or are we not?
Dwarkesh Patel
The reason this would matter is if being one year ahead would be a huge advantage. In the world where you deploy AI over time they're just going to catch up anyway.

I interviewed Richard Rhodes, the guy who wrote The Making of the Atomic Bomb. One of the anecdotes he had was when the Soviets realized America had the bomb. Obviously, we dropped it in Japan.

Lavrentiy Beria — the guy who ran the NKVD, a famously ruthless and evil guy — goes to the Soviet scientist who was running their version of the Manhattan Project. He says, "comrade, you will get us the American bomb." The guy says, "well, listen, their implosion device actually is not optimal. We should make it a different way." Beria says, "no, you will get us the American bomb, or your family will be camp dust."

The thing that's relevant about that anecdote is that the Soviets would have had a better bomb if they hadn't copied the American design, at least initially. That suggests something about history, not just for the Manhattan Project. There's often this pattern of parallel invention because the tech tree implies that a certain thing is next — in this case, a self-play RL — and people work on that and are going to figure it out around the same time. There's not going to be that much gap in who gets it first.

Famously, a bunch of people invented the light bulb around the same time. Is it the case that it might be true but the one year or six months makes the difference?

**Extracted Belief:**

Secrecy in scientific research can have a significant impact on the outcome of technological advancements, potentially hindering or accelerating progress.

**Context:**

Leopold Aschenbrenner uses the example of the Manhattan Project and the development of the atomic bomb to highlight the influence of secrecy on scientific breakthroughs.

**Justification:**

He cites the anecdote of incorrect measurements on graphite as a moderator, which led Germany down the wrong path with heavy water, ultimately delaying their atomic bomb project.

--------

## Chunk 159

**Chunk:**

Dwarkesh Patel
Why can't they figure that out, but not how to get the RL thing working?
Leopold Aschenbrenner
I don't know. Germany during World War II went down the wrong path with heavy water. There's an amazing anecdote in The Making of the Atomic Bomb about this.

Secrecy was one of the most contentious issues early on. Leo Szilard really thought a nuclear chain reaction and an atomic bomb were possible. He went around saying, "this is going to be of enormous strategic and military importance." A lot of people didn't believe it or thought, "maybe this is possible, but I'm going to act as though it's not, and science should be open."

In the early days, there had been some incorrect measurements made on graphite as a moderator. Germany thought graphite wasn't going to work, so they had to do heavy water. But then Enrico Fermi made new measurements indicating that graphite would work. This was really important.

Szilard assaulted Fermi with another secrecy appeal and Fermi was pissed off, throwing a temper tantrum. He thought it was absurd, saying, "come on, this is crazy." But Szilard persisted, and they roped in another guy, George Pegram. In the end, Fermi didn't publish it.

That was just in time. Fermi not publishing meant that the Nazis didn't figure out graphite would work. They went down the path of heavy water, which was the wrong path. This is a key reason why the German project didn't work out. They were way behind.

We face a similar situation now. Are we just going to instantly leak how to get past the data wall and what the next paradigm is? Or are we not?
Dwarkesh Patel
The reason this would matter is if being one year ahead would be a huge advantage. In the world where you deploy AI over time they're just going to catch up anyway.

I interviewed Richard Rhodes, the guy who wrote The Making of the Atomic Bomb. One of the anecdotes he had was when the Soviets realized America had the bomb. Obviously, we dropped it in Japan.

Lavrentiy Beria — the guy who ran the NKVD, a famously ruthless and evil guy — goes to the Soviet scientist who was running their version of the Manhattan Project. He says, "comrade, you will get us the American bomb." The guy says, "well, listen, their implosion device actually is not optimal. We should make it a different way." Beria says, "no, you will get us the American bomb, or your family will be camp dust."

The thing that's relevant about that anecdote is that the Soviets would have had a better bomb if they hadn't copied the American design, at least initially. That suggests something about history, not just for the Manhattan Project. There's often this pattern of parallel invention because the tech tree implies that a certain thing is next — in this case, a self-play RL — and people work on that and are going to figure it out around the same time. There's not going to be that much gap in who gets it first.

Famously, a bunch of people invented the light bulb around the same time. Is it the case that it might be true but the one year or six months makes the difference?

**Extracted Belief:**

The current state of artificial intelligence (AI) research mirrors the secrecy dynamics of the Manhattan Project, where the United States held a significant lead in the development of the atomic bomb due to secrecy and scientific advancement.

**Context:**

Leopold Aschenbrenner draws a parallel between the secrecy surrounding AI research and the secrecy surrounding the Manhattan Project, suggesting that the US currently holds a similar advantage in AI research.

**Justification:**

He highlights the US lead in AI research through the publication of significant papers and advancements in the field, referencing Chinchilla Scaling laws, MoE papers, and transformers, all of which were published before being kept secret.

--------

## Chunk 160

**Chunk:**

Dwarkesh Patel
Why can't they figure that out, but not how to get the RL thing working?
Leopold Aschenbrenner
I don't know. Germany during World War II went down the wrong path with heavy water. There's an amazing anecdote in The Making of the Atomic Bomb about this.

Secrecy was one of the most contentious issues early on. Leo Szilard really thought a nuclear chain reaction and an atomic bomb were possible. He went around saying, "this is going to be of enormous strategic and military importance." A lot of people didn't believe it or thought, "maybe this is possible, but I'm going to act as though it's not, and science should be open."

In the early days, there had been some incorrect measurements made on graphite as a moderator. Germany thought graphite wasn't going to work, so they had to do heavy water. But then Enrico Fermi made new measurements indicating that graphite would work. This was really important.

Szilard assaulted Fermi with another secrecy appeal and Fermi was pissed off, throwing a temper tantrum. He thought it was absurd, saying, "come on, this is crazy." But Szilard persisted, and they roped in another guy, George Pegram. In the end, Fermi didn't publish it.

That was just in time. Fermi not publishing meant that the Nazis didn't figure out graphite would work. They went down the path of heavy water, which was the wrong path. This is a key reason why the German project didn't work out. They were way behind.

We face a similar situation now. Are we just going to instantly leak how to get past the data wall and what the next paradigm is? Or are we not?
Dwarkesh Patel
The reason this would matter is if being one year ahead would be a huge advantage. In the world where you deploy AI over time they're just going to catch up anyway.

I interviewed Richard Rhodes, the guy who wrote The Making of the Atomic Bomb. One of the anecdotes he had was when the Soviets realized America had the bomb. Obviously, we dropped it in Japan.

Lavrentiy Beria — the guy who ran the NKVD, a famously ruthless and evil guy — goes to the Soviet scientist who was running their version of the Manhattan Project. He says, "comrade, you will get us the American bomb." The guy says, "well, listen, their implosion device actually is not optimal. We should make it a different way." Beria says, "no, you will get us the American bomb, or your family will be camp dust."

The thing that's relevant about that anecdote is that the Soviets would have had a better bomb if they hadn't copied the American design, at least initially. That suggests something about history, not just for the Manhattan Project. There's often this pattern of parallel invention because the tech tree implies that a certain thing is next — in this case, a self-play RL — and people work on that and are going to figure it out around the same time. There's not going to be that much gap in who gets it first.

Famously, a bunch of people invented the light bulb around the same time. Is it the case that it might be true but the one year or six months makes the difference?

**Extracted Belief:**

A short time lead in AI development could have a significant impact on the trajectory of future technological advancements, potentially leading to a decisive advantage for the leading nation.

**Context:**

Leopold Aschenbrenner draws a parallel between the early stages of the atomic bomb project and the current state of AI development, arguing that a small time advantage could have significant implications for the future.

**Justification:**

He uses the analogy of the Manhattan Project, where a small lead in developing the atomic bomb gave the United States a decisive advantage in World War II, to illustrate the potential impact of a similar advantage in AI development.

--------

## Chunk 161

**Chunk:**

Dwarkesh Patel
The reason this would matter is if being one year ahead would be a huge advantage. In the world where you deploy AI over time they're just going to catch up anyway.

I interviewed Richard Rhodes, the guy who wrote The Making of the Atomic Bomb. One of the anecdotes he had was when the Soviets realized America had the bomb. Obviously, we dropped it in Japan.

Lavrentiy Beria — the guy who ran the NKVD, a famously ruthless and evil guy — goes to the Soviet scientist who was running their version of the Manhattan Project. He says, "comrade, you will get us the American bomb." The guy says, "well, listen, their implosion device actually is not optimal. We should make it a different way." Beria says, "no, you will get us the American bomb, or your family will be camp dust."

The thing that's relevant about that anecdote is that the Soviets would have had a better bomb if they hadn't copied the American design, at least initially. That suggests something about history, not just for the Manhattan Project. There's often this pattern of parallel invention because the tech tree implies that a certain thing is next — in this case, a self-play RL — and people work on that and are going to figure it out around the same time. There's not going to be that much gap in who gets it first.

Famously, a bunch of people invented the light bulb around the same time. Is it the case that it might be true but the one year or six months makes the difference?
Leopold Aschenbrenner
Two years makes all the difference.
Dwarkesh Patel
I don't know if it'll be two years though.

**Extracted Belief:**

A lead of two years in artificial intelligence development is significant.

**Context:**

Leopold Aschenbrenner responded to Dwarkesh Patel's question about the significance of a one-year or six-month lead in AI development by asserting that two years would make a substantial difference.

**Justification:**

The belief was stated directly in response to a question about the importance of a time lead in AI development, with 'two years' being the specific time frame.

--------

## Chunk 162

**Chunk:**

Dwarkesh Patel
I don't know if it'll be two years though.
Leopold Aschenbrenner
If we lock down the labs, we have much better scientists. We're way ahead. It would be two years. Even six months, a year, would make a huge difference. This gets back to the intelligence explosion dynamics. A year might be the difference between a system that's sort of human-level and a system that is vastly superhuman. It might be like five OOMs.

Look at the current pace. Three years ago, on the math benchmark — these are really difficult high school competition math problems — we were at a few percent, we couldn't solve anything. Now it's solved. That was at the normal pace of AI progress. You didn't have a billion superintelligent researchers.

A year is a huge difference, particularly after superintelligence. Once this is applied to many elements of R&D, you get an industrial explosion with robots and other advanced technologies. A couple of years might yield decades worth of progress. Again, it’s like the technological lead the U.S. had in the first Gulf War, when the 20-30 years of technological lead proved totally decisive. It really matters.

Here’s another reason it really matters. Suppose they steal the weights, suppose they steal the algorithms, and they're close on our tails. Suppose we still pull out ahead. We're a little bit faster and we're three months ahead.

The world in which we're really neck and neck, we only have a three-month lead, is incredibly dangerous. We're in this feverish struggle where if they get ahead, they get to dominate, maybe they get a decisive advantage. They're building clusters like crazy. They're willing to throw all caution to the wind. We have to keep up.

There are crazy new WMDs popping up. Then we're going to be in the situation where it's crazy new military technology, crazy new WMDs, deterrence, mutually assured destruction keeps changing every few weeks. It's a completely unstable, volatile situation that is incredibly dangerous.

So you have to look at it from the point of view that these technologies are dangerous, from the alignment point of view. It might be really important during the intelligence explosion to have a six-month wiggle room to be like, “look, we're going to dedicate more compute to alignment during this period because we have to get it right. We're feeling uneasy about how it's going.”

One of the most important inputs to whether we will destroy ourselves or whether we will get through this incredibly crazy period is whether we have that buffer.

(01:08:20) – Geopolitical implications of AI
Dwarkesh Patel
Before we go further, it's very much worth noting that almost nobody I talk to thinks about the geopolitical implications of AI. I have some object-level disagreements that we'll get into, things I want to iron out. I may not disagree in the end.

The basic premise is that if you keep scaling, if people realize that this is where intelligence is headed, it's not just going to be the same old world. It won't just be about what model we're deploying tomorrow or what the latest thing is. People on Twitter are like, "oh, GPT-4 is going to shake your expectations" or whatever.

COVID is really interesting because when March 2020 hit, it became clear to the world — presidents, CEOs, media, the average person — that there are other things happening in the world right now but the main thing we as a world are dealing with right now is COVID.

**Extracted Belief:**

A one-year lead in artificial intelligence research could be the difference between a system that is slightly beyond human-level intelligence and one that is vastly superhuman.

**Context:**

Leopold Aschenbrenner discusses the significance of a lead in AI research, particularly in relation to the concept of an intelligence explosion, and claims that a one-year advantage could lead to a dramatic difference in capabilities, potentially with a five-order-of-magnitude difference.

**Justification:**

Leopold Aschenbrenner uses the example of progress in AI solving math benchmarks, noting that a few years ago, AI was unable to solve complex math problems, but now it can. He argues that this rapid progress demonstrates the potential for a short time difference to result in significant advancements, particularly after reaching a level of superintelligence.

--------

## Chunk 163

**Chunk:**

Dwarkesh Patel
I don't know if it'll be two years though.
Leopold Aschenbrenner
If we lock down the labs, we have much better scientists. We're way ahead. It would be two years. Even six months, a year, would make a huge difference. This gets back to the intelligence explosion dynamics. A year might be the difference between a system that's sort of human-level and a system that is vastly superhuman. It might be like five OOMs.

Look at the current pace. Three years ago, on the math benchmark — these are really difficult high school competition math problems — we were at a few percent, we couldn't solve anything. Now it's solved. That was at the normal pace of AI progress. You didn't have a billion superintelligent researchers.

A year is a huge difference, particularly after superintelligence. Once this is applied to many elements of R&D, you get an industrial explosion with robots and other advanced technologies. A couple of years might yield decades worth of progress. Again, it’s like the technological lead the U.S. had in the first Gulf War, when the 20-30 years of technological lead proved totally decisive. It really matters.

Here’s another reason it really matters. Suppose they steal the weights, suppose they steal the algorithms, and they're close on our tails. Suppose we still pull out ahead. We're a little bit faster and we're three months ahead.

The world in which we're really neck and neck, we only have a three-month lead, is incredibly dangerous. We're in this feverish struggle where if they get ahead, they get to dominate, maybe they get a decisive advantage. They're building clusters like crazy. They're willing to throw all caution to the wind. We have to keep up.

There are crazy new WMDs popping up. Then we're going to be in the situation where it's crazy new military technology, crazy new WMDs, deterrence, mutually assured destruction keeps changing every few weeks. It's a completely unstable, volatile situation that is incredibly dangerous.

So you have to look at it from the point of view that these technologies are dangerous, from the alignment point of view. It might be really important during the intelligence explosion to have a six-month wiggle room to be like, “look, we're going to dedicate more compute to alignment during this period because we have to get it right. We're feeling uneasy about how it's going.”

One of the most important inputs to whether we will destroy ourselves or whether we will get through this incredibly crazy period is whether we have that buffer.

(01:08:20) – Geopolitical implications of AI
Dwarkesh Patel
Before we go further, it's very much worth noting that almost nobody I talk to thinks about the geopolitical implications of AI. I have some object-level disagreements that we'll get into, things I want to iron out. I may not disagree in the end.

The basic premise is that if you keep scaling, if people realize that this is where intelligence is headed, it's not just going to be the same old world. It won't just be about what model we're deploying tomorrow or what the latest thing is. People on Twitter are like, "oh, GPT-4 is going to shake your expectations" or whatever.

COVID is really interesting because when March 2020 hit, it became clear to the world — presidents, CEOs, media, the average person — that there are other things happening in the world right now but the main thing we as a world are dealing with right now is COVID.

**Extracted Belief:**

A one-year lead in AI research could result in an industrial explosion, with robots and other advanced technologies rapidly emerging.

**Context:**

Leopold Aschenbrenner argues that a lead in AI research could result in significant technological advancements, drawing a comparison to the technological lead the U.S. had in the first Gulf War, which he asserts was a decisive factor in the outcome.

**Justification:**

He uses the example of the U.S. technological advantage in the first Gulf War, where a 20-30-year lead proved to be decisive, to suggest that a significant technological advantage in AI could have similar effects.

--------

## Chunk 164

**Chunk:**

Dwarkesh Patel
I don't know if it'll be two years though.
Leopold Aschenbrenner
If we lock down the labs, we have much better scientists. We're way ahead. It would be two years. Even six months, a year, would make a huge difference. This gets back to the intelligence explosion dynamics. A year might be the difference between a system that's sort of human-level and a system that is vastly superhuman. It might be like five OOMs.

Look at the current pace. Three years ago, on the math benchmark — these are really difficult high school competition math problems — we were at a few percent, we couldn't solve anything. Now it's solved. That was at the normal pace of AI progress. You didn't have a billion superintelligent researchers.

A year is a huge difference, particularly after superintelligence. Once this is applied to many elements of R&D, you get an industrial explosion with robots and other advanced technologies. A couple of years might yield decades worth of progress. Again, it’s like the technological lead the U.S. had in the first Gulf War, when the 20-30 years of technological lead proved totally decisive. It really matters.

Here’s another reason it really matters. Suppose they steal the weights, suppose they steal the algorithms, and they're close on our tails. Suppose we still pull out ahead. We're a little bit faster and we're three months ahead.

The world in which we're really neck and neck, we only have a three-month lead, is incredibly dangerous. We're in this feverish struggle where if they get ahead, they get to dominate, maybe they get a decisive advantage. They're building clusters like crazy. They're willing to throw all caution to the wind. We have to keep up.

There are crazy new WMDs popping up. Then we're going to be in the situation where it's crazy new military technology, crazy new WMDs, deterrence, mutually assured destruction keeps changing every few weeks. It's a completely unstable, volatile situation that is incredibly dangerous.

So you have to look at it from the point of view that these technologies are dangerous, from the alignment point of view. It might be really important during the intelligence explosion to have a six-month wiggle room to be like, “look, we're going to dedicate more compute to alignment during this period because we have to get it right. We're feeling uneasy about how it's going.”

One of the most important inputs to whether we will destroy ourselves or whether we will get through this incredibly crazy period is whether we have that buffer.

(01:08:20) – Geopolitical implications of AI
Dwarkesh Patel
Before we go further, it's very much worth noting that almost nobody I talk to thinks about the geopolitical implications of AI. I have some object-level disagreements that we'll get into, things I want to iron out. I may not disagree in the end.

The basic premise is that if you keep scaling, if people realize that this is where intelligence is headed, it's not just going to be the same old world. It won't just be about what model we're deploying tomorrow or what the latest thing is. People on Twitter are like, "oh, GPT-4 is going to shake your expectations" or whatever.

COVID is really interesting because when March 2020 hit, it became clear to the world — presidents, CEOs, media, the average person — that there are other things happening in the world right now but the main thing we as a world are dealing with right now is COVID.

**Extracted Belief:**

A one-year lead in AI research could lead to decades of progress in technology.

**Context:**

Leopold Aschenbrenner argues that a lead in AI research could lead to significant technological advancements, drawing a comparison to the technological lead the U.S. had in the first Gulf War, which he asserts was a decisive factor in the outcome.

**Justification:**

He uses the example of the U.S. technological advantage in the first Gulf War, where a 20-30-year lead proved to be decisive, to suggest that a significant technological advantage in AI could have similar effects.

--------

## Chunk 165

**Chunk:**

Dwarkesh Patel
I don't know if it'll be two years though.
Leopold Aschenbrenner
If we lock down the labs, we have much better scientists. We're way ahead. It would be two years. Even six months, a year, would make a huge difference. This gets back to the intelligence explosion dynamics. A year might be the difference between a system that's sort of human-level and a system that is vastly superhuman. It might be like five OOMs.

Look at the current pace. Three years ago, on the math benchmark — these are really difficult high school competition math problems — we were at a few percent, we couldn't solve anything. Now it's solved. That was at the normal pace of AI progress. You didn't have a billion superintelligent researchers.

A year is a huge difference, particularly after superintelligence. Once this is applied to many elements of R&D, you get an industrial explosion with robots and other advanced technologies. A couple of years might yield decades worth of progress. Again, it’s like the technological lead the U.S. had in the first Gulf War, when the 20-30 years of technological lead proved totally decisive. It really matters.

Here’s another reason it really matters. Suppose they steal the weights, suppose they steal the algorithms, and they're close on our tails. Suppose we still pull out ahead. We're a little bit faster and we're three months ahead.

The world in which we're really neck and neck, we only have a three-month lead, is incredibly dangerous. We're in this feverish struggle where if they get ahead, they get to dominate, maybe they get a decisive advantage. They're building clusters like crazy. They're willing to throw all caution to the wind. We have to keep up.

There are crazy new WMDs popping up. Then we're going to be in the situation where it's crazy new military technology, crazy new WMDs, deterrence, mutually assured destruction keeps changing every few weeks. It's a completely unstable, volatile situation that is incredibly dangerous.

So you have to look at it from the point of view that these technologies are dangerous, from the alignment point of view. It might be really important during the intelligence explosion to have a six-month wiggle room to be like, “look, we're going to dedicate more compute to alignment during this period because we have to get it right. We're feeling uneasy about how it's going.”

One of the most important inputs to whether we will destroy ourselves or whether we will get through this incredibly crazy period is whether we have that buffer.

(01:08:20) – Geopolitical implications of AI
Dwarkesh Patel
Before we go further, it's very much worth noting that almost nobody I talk to thinks about the geopolitical implications of AI. I have some object-level disagreements that we'll get into, things I want to iron out. I may not disagree in the end.

The basic premise is that if you keep scaling, if people realize that this is where intelligence is headed, it's not just going to be the same old world. It won't just be about what model we're deploying tomorrow or what the latest thing is. People on Twitter are like, "oh, GPT-4 is going to shake your expectations" or whatever.

COVID is really interesting because when March 2020 hit, it became clear to the world — presidents, CEOs, media, the average person — that there are other things happening in the world right now but the main thing we as a world are dealing with right now is COVID.

**Extracted Belief:**

The world is in a feverish struggle for AI dominance, with nations willing to take significant risks to gain an advantage.

**Context:**

Leopold Aschenbrenner describes the current AI landscape as a competitive race, highlighting the willingness of nations to take risks to gain an advantage.

**Justification:**

He asserts that nations are willing to throw caution to the wind, building massive computing clusters and potentially engaging in espionage, suggesting a highly competitive and potentially dangerous environment.

--------

## Chunk 166

**Chunk:**

Dwarkesh Patel
I don't know if it'll be two years though.
Leopold Aschenbrenner
If we lock down the labs, we have much better scientists. We're way ahead. It would be two years. Even six months, a year, would make a huge difference. This gets back to the intelligence explosion dynamics. A year might be the difference between a system that's sort of human-level and a system that is vastly superhuman. It might be like five OOMs.

Look at the current pace. Three years ago, on the math benchmark — these are really difficult high school competition math problems — we were at a few percent, we couldn't solve anything. Now it's solved. That was at the normal pace of AI progress. You didn't have a billion superintelligent researchers.

A year is a huge difference, particularly after superintelligence. Once this is applied to many elements of R&D, you get an industrial explosion with robots and other advanced technologies. A couple of years might yield decades worth of progress. Again, it’s like the technological lead the U.S. had in the first Gulf War, when the 20-30 years of technological lead proved totally decisive. It really matters.

Here’s another reason it really matters. Suppose they steal the weights, suppose they steal the algorithms, and they're close on our tails. Suppose we still pull out ahead. We're a little bit faster and we're three months ahead.

The world in which we're really neck and neck, we only have a three-month lead, is incredibly dangerous. We're in this feverish struggle where if they get ahead, they get to dominate, maybe they get a decisive advantage. They're building clusters like crazy. They're willing to throw all caution to the wind. We have to keep up.

There are crazy new WMDs popping up. Then we're going to be in the situation where it's crazy new military technology, crazy new WMDs, deterrence, mutually assured destruction keeps changing every few weeks. It's a completely unstable, volatile situation that is incredibly dangerous.

So you have to look at it from the point of view that these technologies are dangerous, from the alignment point of view. It might be really important during the intelligence explosion to have a six-month wiggle room to be like, “look, we're going to dedicate more compute to alignment during this period because we have to get it right. We're feeling uneasy about how it's going.”

One of the most important inputs to whether we will destroy ourselves or whether we will get through this incredibly crazy period is whether we have that buffer.

(01:08:20) – Geopolitical implications of AI
Dwarkesh Patel
Before we go further, it's very much worth noting that almost nobody I talk to thinks about the geopolitical implications of AI. I have some object-level disagreements that we'll get into, things I want to iron out. I may not disagree in the end.

The basic premise is that if you keep scaling, if people realize that this is where intelligence is headed, it's not just going to be the same old world. It won't just be about what model we're deploying tomorrow or what the latest thing is. People on Twitter are like, "oh, GPT-4 is going to shake your expectations" or whatever.

COVID is really interesting because when March 2020 hit, it became clear to the world — presidents, CEOs, media, the average person — that there are other things happening in the world right now but the main thing we as a world are dealing with right now is COVID.

**Extracted Belief:**

A small lead in AI research could be incredibly dangerous, as it could lead to a situation of unstable deterrence and potential for conflict.

**Context:**

Leopold Aschenbrenner expresses concern about the potential for instability in the event of a close race in AI development, highlighting the potential for a dangerous situation.

**Justification:**

He describes a scenario where a nation with a small lead could feel compelled to act aggressively, potentially leading to a situation of unstable deterrence and increased risk of conflict.

--------

## Chunk 167

**Chunk:**

Dwarkesh Patel
I don't know if it'll be two years though.
Leopold Aschenbrenner
If we lock down the labs, we have much better scientists. We're way ahead. It would be two years. Even six months, a year, would make a huge difference. This gets back to the intelligence explosion dynamics. A year might be the difference between a system that's sort of human-level and a system that is vastly superhuman. It might be like five OOMs.

Look at the current pace. Three years ago, on the math benchmark — these are really difficult high school competition math problems — we were at a few percent, we couldn't solve anything. Now it's solved. That was at the normal pace of AI progress. You didn't have a billion superintelligent researchers.

A year is a huge difference, particularly after superintelligence. Once this is applied to many elements of R&D, you get an industrial explosion with robots and other advanced technologies. A couple of years might yield decades worth of progress. Again, it’s like the technological lead the U.S. had in the first Gulf War, when the 20-30 years of technological lead proved totally decisive. It really matters.

Here’s another reason it really matters. Suppose they steal the weights, suppose they steal the algorithms, and they're close on our tails. Suppose we still pull out ahead. We're a little bit faster and we're three months ahead.

The world in which we're really neck and neck, we only have a three-month lead, is incredibly dangerous. We're in this feverish struggle where if they get ahead, they get to dominate, maybe they get a decisive advantage. They're building clusters like crazy. They're willing to throw all caution to the wind. We have to keep up.

There are crazy new WMDs popping up. Then we're going to be in the situation where it's crazy new military technology, crazy new WMDs, deterrence, mutually assured destruction keeps changing every few weeks. It's a completely unstable, volatile situation that is incredibly dangerous.

So you have to look at it from the point of view that these technologies are dangerous, from the alignment point of view. It might be really important during the intelligence explosion to have a six-month wiggle room to be like, “look, we're going to dedicate more compute to alignment during this period because we have to get it right. We're feeling uneasy about how it's going.”

One of the most important inputs to whether we will destroy ourselves or whether we will get through this incredibly crazy period is whether we have that buffer.

(01:08:20) – Geopolitical implications of AI
Dwarkesh Patel
Before we go further, it's very much worth noting that almost nobody I talk to thinks about the geopolitical implications of AI. I have some object-level disagreements that we'll get into, things I want to iron out. I may not disagree in the end.

The basic premise is that if you keep scaling, if people realize that this is where intelligence is headed, it's not just going to be the same old world. It won't just be about what model we're deploying tomorrow or what the latest thing is. People on Twitter are like, "oh, GPT-4 is going to shake your expectations" or whatever.

COVID is really interesting because when March 2020 hit, it became clear to the world — presidents, CEOs, media, the average person — that there are other things happening in the world right now but the main thing we as a world are dealing with right now is COVID.

**Extracted Belief:**

The development of new weapons of mass destruction is a significant risk in the context of AI advancement.

**Context:**

Leopold Aschenbrenner expresses concern about the potential for the development of new weapons of mass destruction in the context of AI advancements.

**Justification:**

He suggests that advanced AI capabilities could lead to the development of new and potentially more dangerous weapons, highlighting the need for careful consideration of the potential consequences.

--------

## Chunk 168

**Chunk:**

Dwarkesh Patel
I don't know if it'll be two years though.
Leopold Aschenbrenner
If we lock down the labs, we have much better scientists. We're way ahead. It would be two years. Even six months, a year, would make a huge difference. This gets back to the intelligence explosion dynamics. A year might be the difference between a system that's sort of human-level and a system that is vastly superhuman. It might be like five OOMs.

Look at the current pace. Three years ago, on the math benchmark — these are really difficult high school competition math problems — we were at a few percent, we couldn't solve anything. Now it's solved. That was at the normal pace of AI progress. You didn't have a billion superintelligent researchers.

A year is a huge difference, particularly after superintelligence. Once this is applied to many elements of R&D, you get an industrial explosion with robots and other advanced technologies. A couple of years might yield decades worth of progress. Again, it’s like the technological lead the U.S. had in the first Gulf War, when the 20-30 years of technological lead proved totally decisive. It really matters.

Here’s another reason it really matters. Suppose they steal the weights, suppose they steal the algorithms, and they're close on our tails. Suppose we still pull out ahead. We're a little bit faster and we're three months ahead.

The world in which we're really neck and neck, we only have a three-month lead, is incredibly dangerous. We're in this feverish struggle where if they get ahead, they get to dominate, maybe they get a decisive advantage. They're building clusters like crazy. They're willing to throw all caution to the wind. We have to keep up.

There are crazy new WMDs popping up. Then we're going to be in the situation where it's crazy new military technology, crazy new WMDs, deterrence, mutually assured destruction keeps changing every few weeks. It's a completely unstable, volatile situation that is incredibly dangerous.

So you have to look at it from the point of view that these technologies are dangerous, from the alignment point of view. It might be really important during the intelligence explosion to have a six-month wiggle room to be like, “look, we're going to dedicate more compute to alignment during this period because we have to get it right. We're feeling uneasy about how it's going.”

One of the most important inputs to whether we will destroy ourselves or whether we will get through this incredibly crazy period is whether we have that buffer.

(01:08:20) – Geopolitical implications of AI
Dwarkesh Patel
Before we go further, it's very much worth noting that almost nobody I talk to thinks about the geopolitical implications of AI. I have some object-level disagreements that we'll get into, things I want to iron out. I may not disagree in the end.

The basic premise is that if you keep scaling, if people realize that this is where intelligence is headed, it's not just going to be the same old world. It won't just be about what model we're deploying tomorrow or what the latest thing is. People on Twitter are like, "oh, GPT-4 is going to shake your expectations" or whatever.

COVID is really interesting because when March 2020 hit, it became clear to the world — presidents, CEOs, media, the average person — that there are other things happening in the world right now but the main thing we as a world are dealing with right now is COVID.

**Extracted Belief:**

It is important to dedicate resources to ensuring the safe and ethical development of AI, particularly during the intelligence explosion.

**Context:**

Leopold Aschenbrenner emphasizes the importance of prioritizing AI safety and alignment, particularly during the intelligence explosion.

**Justification:**

He argues that it is crucial to have a buffer period to focus on AI alignment, suggesting that failing to do so could lead to negative consequences, potentially even the destruction of humanity.

--------

## Chunk 169

**Chunk:**

Dwarkesh Patel
Before we go further, it's very much worth noting that almost nobody I talk to thinks about the geopolitical implications of AI. I have some object-level disagreements that we'll get into, things I want to iron out. I may not disagree in the end.

The basic premise is that if you keep scaling, if people realize that this is where intelligence is headed, it's not just going to be the same old world. It won't just be about what model we're deploying tomorrow or what the latest thing is. People on Twitter are like, "oh, GPT-4 is going to shake your expectations" or whatever.

COVID is really interesting because when March 2020 hit, it became clear to the world — presidents, CEOs, media, the average person — that there are other things happening in the world right now but the main thing we as a world are dealing with right now is COVID.
Leopold Aschenbrenner
Soon it will be AGI. This is the quiet period. Maybe you want to go on vacation. Maybe now is the last time you can have some kids. My girlfriend sometimes complains when I’m off doing work that I don’t spend enough time with her. She threatens to replace me with GPT-6 or whatever. I'm like, “GPT-6 will also be too busy doing AI research.”
Dwarkesh Patel
Why aren't other people talking about national security?

**Extracted Belief:**

Artificial general intelligence (AGI) is a near-future reality.

**Context:**

Leopold Aschenbrenner believes that AGI is imminent and uses the phrase "Soon it will be AGI." He is suggesting that the current period is a time of relative calm before the arrival of AGI.

**Justification:**

His belief is based on the rapid advancements in AI development, as evidenced by his statement that "this is the quiet period." He also alludes to a sense of urgency in his suggestion that people should enjoy time with their families before AGI becomes a dominant force in their lives.

--------

## Chunk 170

**Chunk:**

Dwarkesh Patel
Before we go further, it's very much worth noting that almost nobody I talk to thinks about the geopolitical implications of AI. I have some object-level disagreements that we'll get into, things I want to iron out. I may not disagree in the end.

The basic premise is that if you keep scaling, if people realize that this is where intelligence is headed, it's not just going to be the same old world. It won't just be about what model we're deploying tomorrow or what the latest thing is. People on Twitter are like, "oh, GPT-4 is going to shake your expectations" or whatever.

COVID is really interesting because when March 2020 hit, it became clear to the world — presidents, CEOs, media, the average person — that there are other things happening in the world right now but the main thing we as a world are dealing with right now is COVID.
Leopold Aschenbrenner
Soon it will be AGI. This is the quiet period. Maybe you want to go on vacation. Maybe now is the last time you can have some kids. My girlfriend sometimes complains when I’m off doing work that I don’t spend enough time with her. She threatens to replace me with GPT-6 or whatever. I'm like, “GPT-6 will also be too busy doing AI research.”
Dwarkesh Patel
Why aren't other people talking about national security?

**Extracted Belief:**

AGI will be deeply involved in AI research.

**Context:**

He believes that AGI will be so advanced and consumed with AI research that even personal relationships will be affected.

**Justification:**

Leopold Aschenbrenner expresses this belief in response to Dwarkesh Patel's question about why people are not talking about national security. Aschenbrenner makes the statement that "GPT-6 will also be too busy doing AI research." He is implying that future AGI will be so focused on its own research that it will have limited time for other activities.

--------

## Chunk 171

**Chunk:**

Dwarkesh Patel
Why aren't other people talking about national security?
Leopold Aschenbrenner
I made this mistake with COVID. In February of 2020, I thought it was going to sweep the world and all the hospitals would collapse. It would be crazy, and then it'd be over. A lot of people thought this kind of thing at the beginning of COVID. They shut down their office for a month or whatever.

The thing I just really didn't price in was societal reaction. Within weeks, Congress spent over 10% of GDP on COVID measures. The entire country was shut down. It was crazy. I didn't sufficiently price it in with COVID.

Why do people underrate it? Being in the trenches actually gives you a less clear picture of the trend lines. You don’t have to zoom out that much, only a few years.

When you're in the trenches, you're trying to get the next model to work. There's always something that's hard. You might underrate algorithmic progress because you're like, "ah, things are hard right now," or "data wall" or whatever. When you zoom out just a few years and count up how much algorithmic progress was made, it's enormous.

People also just don’t think about this stuff. Smart people really underrate espionage. Part of the security issue is that people don't realize how intense state-level espionage can be. This Israeli company had software that could just zero-click hack any iPhone. They just put in your number and it was a straight download of everything. The United States infiltrated an air-gapped atomic weapons program. Wild.
Dwarkesh Patel
Are you talking about Stuxnet?

**Extracted Belief:**

People tend to underestimate the significance of algorithmic progress in artificial intelligence because they focus too much on the immediate challenges and difficulties encountered in the field.

**Context:**

Leopold Aschenbrenner is explaining why people underestimate the threat of AI espionage, arguing that they are too focused on the day-to-day struggles and fail to see the broader picture of rapid progress in the field.

**Justification:**

He provides the example of being 'in the trenches' and trying to get the next model to work, where one might be preoccupied with current difficulties and miss the larger picture of advancement over time.

--------

## Chunk 172

**Chunk:**

Dwarkesh Patel
Why aren't other people talking about national security?
Leopold Aschenbrenner
I made this mistake with COVID. In February of 2020, I thought it was going to sweep the world and all the hospitals would collapse. It would be crazy, and then it'd be over. A lot of people thought this kind of thing at the beginning of COVID. They shut down their office for a month or whatever.

The thing I just really didn't price in was societal reaction. Within weeks, Congress spent over 10% of GDP on COVID measures. The entire country was shut down. It was crazy. I didn't sufficiently price it in with COVID.

Why do people underrate it? Being in the trenches actually gives you a less clear picture of the trend lines. You don’t have to zoom out that much, only a few years.

When you're in the trenches, you're trying to get the next model to work. There's always something that's hard. You might underrate algorithmic progress because you're like, "ah, things are hard right now," or "data wall" or whatever. When you zoom out just a few years and count up how much algorithmic progress was made, it's enormous.

People also just don’t think about this stuff. Smart people really underrate espionage. Part of the security issue is that people don't realize how intense state-level espionage can be. This Israeli company had software that could just zero-click hack any iPhone. They just put in your number and it was a straight download of everything. The United States infiltrated an air-gapped atomic weapons program. Wild.
Dwarkesh Patel
Are you talking about Stuxnet?

**Extracted Belief:**

Smart people often underestimate the intensity and effectiveness of state-level espionage.

**Context:**

Leopold Aschenbrenner is highlighting the underestimation of state-level espionage as a security concern in the context of AI development.

**Justification:**

He supports this belief by citing examples of sophisticated espionage techniques, including the Israeli company's software that could hack any iPhone and the US infiltration of an air-gapped atomic weapons program.

--------

## Chunk 173

**Chunk:**

Dwarkesh Patel
Are you talking about Stuxnet?
Leopold Aschenbrenner
Yeah. Intelligence agencies have stockpiles of zero-days. When things get really hot, maybe we'll send special forces to go to the data center or something. China does this. They threaten people's families. They’re like, “if you don't cooperate, if you don't give us the intel…”

There's a good book along the lines of The Gulag Archipelago called Inside the Aquarium, which is by a Soviet GRU defector. GRU was military intelligence. Ilya recommended this book to me. When I read it, I was shocked at the intensity of state-level espionage.

The whole book was about how they go to these European countries and try and recruit people to get the technology. Here’s one anecdote. This eventual defector, he's being trained at the GRU spy academy. To graduate from the spy academy before being sent abroad, you had to pass a test to show that you can do this.

The test was recruiting a Soviet scientist in Moscow to give you information, like you would do in a foreign country. Of course, for whomever you recruited, the penalty for giving away secret information was death. So to graduate from the GRU spy academy, you had to condemn a countryman to death. States do this stuff.
Dwarkesh Patel
I started reading the book because you mentioned it in the series. I was wondering about the fact that you use this anecdote. Then you're like, "a book recommended by Ilya." Is this some sort of Easter egg? We'll leave that as an exercise for the reader.

**Extracted Belief:**

Intelligence agencies maintain stockpiles of zero-day vulnerabilities, which are previously unknown security flaws.

**Context:**

Leopold Aschenbrenner is discussing the intensity of state-level espionage and the potential for intelligence agencies to exploit vulnerabilities.

**Justification:**

Leopold Aschenbrenner states that intelligence agencies have stockpiles of zero-days. This statement is based on his knowledge of intelligence operations, likely gained from his own experiences and interactions with individuals involved in the field.

--------

## Chunk 174

**Chunk:**

Dwarkesh Patel
Are you talking about Stuxnet?
Leopold Aschenbrenner
Yeah. Intelligence agencies have stockpiles of zero-days. When things get really hot, maybe we'll send special forces to go to the data center or something. China does this. They threaten people's families. They’re like, “if you don't cooperate, if you don't give us the intel…”

There's a good book along the lines of The Gulag Archipelago called Inside the Aquarium, which is by a Soviet GRU defector. GRU was military intelligence. Ilya recommended this book to me. When I read it, I was shocked at the intensity of state-level espionage.

The whole book was about how they go to these European countries and try and recruit people to get the technology. Here’s one anecdote. This eventual defector, he's being trained at the GRU spy academy. To graduate from the spy academy before being sent abroad, you had to pass a test to show that you can do this.

The test was recruiting a Soviet scientist in Moscow to give you information, like you would do in a foreign country. Of course, for whomever you recruited, the penalty for giving away secret information was death. So to graduate from the GRU spy academy, you had to condemn a countryman to death. States do this stuff.
Dwarkesh Patel
I started reading the book because you mentioned it in the series. I was wondering about the fact that you use this anecdote. Then you're like, "a book recommended by Ilya." Is this some sort of Easter egg? We'll leave that as an exercise for the reader.

**Extracted Belief:**

State-level espionage can involve threatening the families of individuals to coerce them into providing information.

**Context:**

Leopold Aschenbrenner is illustrating the extreme measures that states employ in espionage by citing examples of China's tactics.

**Justification:**

Aschenbrenner states that China engages in threatening families of individuals to compel them to cooperate and provide intelligence, suggesting that this is a common practice in state-level espionage. This belief is likely based on his knowledge of global intelligence operations and interactions within the field.

--------

## Chunk 175

**Chunk:**

Dwarkesh Patel
Are you talking about Stuxnet?
Leopold Aschenbrenner
Yeah. Intelligence agencies have stockpiles of zero-days. When things get really hot, maybe we'll send special forces to go to the data center or something. China does this. They threaten people's families. They’re like, “if you don't cooperate, if you don't give us the intel…”

There's a good book along the lines of The Gulag Archipelago called Inside the Aquarium, which is by a Soviet GRU defector. GRU was military intelligence. Ilya recommended this book to me. When I read it, I was shocked at the intensity of state-level espionage.

The whole book was about how they go to these European countries and try and recruit people to get the technology. Here’s one anecdote. This eventual defector, he's being trained at the GRU spy academy. To graduate from the spy academy before being sent abroad, you had to pass a test to show that you can do this.

The test was recruiting a Soviet scientist in Moscow to give you information, like you would do in a foreign country. Of course, for whomever you recruited, the penalty for giving away secret information was death. So to graduate from the GRU spy academy, you had to condemn a countryman to death. States do this stuff.
Dwarkesh Patel
I started reading the book because you mentioned it in the series. I was wondering about the fact that you use this anecdote. Then you're like, "a book recommended by Ilya." Is this some sort of Easter egg? We'll leave that as an exercise for the reader.

**Extracted Belief:**

State-level espionage is highly intense and involves aggressive recruitment and coercion tactics.

**Context:**

Leopold Aschenbrenner is discussing the contents of the book "Inside the Aquarium" by a Soviet GRU defector, which reveals the intense nature of state-level espionage.

**Justification:**

Aschenbrenner states that the book "Inside the Aquarium" details the aggressive tactics employed by Soviet intelligence agencies to recruit individuals and gain access to technology, implying that these practices are characteristic of state-level espionage globally. This belief is derived from his reading and understanding of the book.

--------

## Chunk 176

**Chunk:**

Dwarkesh Patel
Are you talking about Stuxnet?
Leopold Aschenbrenner
Yeah. Intelligence agencies have stockpiles of zero-days. When things get really hot, maybe we'll send special forces to go to the data center or something. China does this. They threaten people's families. They’re like, “if you don't cooperate, if you don't give us the intel…”

There's a good book along the lines of The Gulag Archipelago called Inside the Aquarium, which is by a Soviet GRU defector. GRU was military intelligence. Ilya recommended this book to me. When I read it, I was shocked at the intensity of state-level espionage.

The whole book was about how they go to these European countries and try and recruit people to get the technology. Here’s one anecdote. This eventual defector, he's being trained at the GRU spy academy. To graduate from the spy academy before being sent abroad, you had to pass a test to show that you can do this.

The test was recruiting a Soviet scientist in Moscow to give you information, like you would do in a foreign country. Of course, for whomever you recruited, the penalty for giving away secret information was death. So to graduate from the GRU spy academy, you had to condemn a countryman to death. States do this stuff.
Dwarkesh Patel
I started reading the book because you mentioned it in the series. I was wondering about the fact that you use this anecdote. Then you're like, "a book recommended by Ilya." Is this some sort of Easter egg? We'll leave that as an exercise for the reader.

**Extracted Belief:**

GRU, the Soviet military intelligence agency, used a test involving the recruitment of a Soviet scientist as a graduation requirement for its spy academy.

**Context:**

Leopold Aschenbrenner is relaying an anecdote from the book "Inside the Aquarium" about the training methods employed by the GRU spy academy.

**Justification:**

Aschenbrenner states that the book "Inside the Aquarium" describes a test used by the GRU spy academy to assess the recruitment skills of its trainees, which involved recruiting a Soviet scientist and exposing them to the potential of a death penalty if they provided information. This belief is based on his reading and understanding of the book.

--------

## Chunk 177

**Chunk:**

Dwarkesh Patel
Are you talking about Stuxnet?
Leopold Aschenbrenner
Yeah. Intelligence agencies have stockpiles of zero-days. When things get really hot, maybe we'll send special forces to go to the data center or something. China does this. They threaten people's families. They’re like, “if you don't cooperate, if you don't give us the intel…”

There's a good book along the lines of The Gulag Archipelago called Inside the Aquarium, which is by a Soviet GRU defector. GRU was military intelligence. Ilya recommended this book to me. When I read it, I was shocked at the intensity of state-level espionage.

The whole book was about how they go to these European countries and try and recruit people to get the technology. Here’s one anecdote. This eventual defector, he's being trained at the GRU spy academy. To graduate from the spy academy before being sent abroad, you had to pass a test to show that you can do this.

The test was recruiting a Soviet scientist in Moscow to give you information, like you would do in a foreign country. Of course, for whomever you recruited, the penalty for giving away secret information was death. So to graduate from the GRU spy academy, you had to condemn a countryman to death. States do this stuff.
Dwarkesh Patel
I started reading the book because you mentioned it in the series. I was wondering about the fact that you use this anecdote. Then you're like, "a book recommended by Ilya." Is this some sort of Easter egg? We'll leave that as an exercise for the reader.

**Extracted Belief:**

The penalty for giving away secret information to foreign intelligence agencies can be death.

**Context:**

Leopold Aschenbrenner is describing the consequences of a Soviet scientist providing information to a foreign intelligence agency, as detailed in the book "Inside the Aquarium".

**Justification:**

Aschenbrenner states that the penalty for a Soviet scientist providing information to foreign intelligence agencies was death. This belief is based on his understanding of the book "Inside the Aquarium", which details Soviet intelligence practices and the dangers faced by individuals involved in espionage.

--------

## Chunk 178

**Chunk:**

Dwarkesh Patel
Are you talking about Stuxnet?
Leopold Aschenbrenner
Yeah. Intelligence agencies have stockpiles of zero-days. When things get really hot, maybe we'll send special forces to go to the data center or something. China does this. They threaten people's families. They’re like, “if you don't cooperate, if you don't give us the intel…”

There's a good book along the lines of The Gulag Archipelago called Inside the Aquarium, which is by a Soviet GRU defector. GRU was military intelligence. Ilya recommended this book to me. When I read it, I was shocked at the intensity of state-level espionage.

The whole book was about how they go to these European countries and try and recruit people to get the technology. Here’s one anecdote. This eventual defector, he's being trained at the GRU spy academy. To graduate from the spy academy before being sent abroad, you had to pass a test to show that you can do this.

The test was recruiting a Soviet scientist in Moscow to give you information, like you would do in a foreign country. Of course, for whomever you recruited, the penalty for giving away secret information was death. So to graduate from the GRU spy academy, you had to condemn a countryman to death. States do this stuff.
Dwarkesh Patel
I started reading the book because you mentioned it in the series. I was wondering about the fact that you use this anecdote. Then you're like, "a book recommended by Ilya." Is this some sort of Easter egg? We'll leave that as an exercise for the reader.

**Extracted Belief:**

States engage in extreme tactics and practices in espionage, including recruitment, coercion, and the potential for deadly consequences for those involved.

**Context:**

Leopold Aschenbrenner concludes his discussion about state-level espionage by emphasizing the intense tactics employed by states.

**Justification:**

Aschenbrenner concludes his discussion with the statement "States do this stuff", referencing the aggressive practices and dangers involved in state-level espionage described earlier. This belief is based on his understanding of the book "Inside the Aquarium" and his knowledge of global intelligence operations.

--------

## Chunk 179

**Chunk:**

Dwarkesh Patel
I started reading the book because you mentioned it in the series. I was wondering about the fact that you use this anecdote. Then you're like, "a book recommended by Ilya." Is this some sort of Easter egg? We'll leave that as an exercise for the reader.
Leopold Aschenbrenner
The beatings will continue until morale improves.
Dwarkesh Patel
Suppose we live in a world where these secrets are locked down, but China realizes this progress is happening in America.

**Extracted Belief:**

The beatings will continue until morale improves.

**Context:**

Leopold Aschenbrenner makes this statement in response to a question about the use of an anecdote in a book he mentioned, implying a situation where a particular action is being carried out until a desired outcome is achieved.

**Justification:**

The statement is an idiom, often used to express a satirical outlook on situations where forced compliance is used to maintain control, with no underlying evidence or reasoning being stated.

--------

## Chunk 180

**Chunk:**

Dwarkesh Patel
Suppose we live in a world where these secrets are locked down, but China realizes this progress is happening in America.
Leopold Aschenbrenner
The secrets probably won't be locked down. We’re probably going to live in the bad world. It's going to be really bad.
Dwarkesh Patel
Why are you so confident they won't be locked down?

**Extracted Belief:**

The secrets of AI progress in America will probably not be locked down.

**Context:**

Leopold Aschenbrenner expresses his belief about the likelihood of AI secrets being locked down in response to a hypothetical scenario where China becomes aware of progress in America.

**Justification:**

Leopold Aschenbrenner states this belief based on his personal assessment and experience, without providing explicit evidence or reasoning.

--------

## Chunk 181

**Chunk:**

Dwarkesh Patel
Suppose we live in a world where these secrets are locked down, but China realizes this progress is happening in America.
Leopold Aschenbrenner
The secrets probably won't be locked down. We’re probably going to live in the bad world. It's going to be really bad.
Dwarkesh Patel
Why are you so confident they won't be locked down?

**Extracted Belief:**

The world will likely experience a negative outcome due to the lack of security around AI progress.

**Context:**

In response to the hypothetical scenario, Leopold Aschenbrenner predicts a negative outcome for the world due to the lack of security around AI progress.

**Justification:**

Leopold Aschenbrenner makes this prediction based on his personal assessment and experience, without providing explicit evidence or reasoning.

--------

## Chunk 182

**Chunk:**

Dwarkesh Patel
Why are you so confident they won't be locked down?
Leopold Aschenbrenner
I'm not confident they won't be locked down, but it's just not happening.
Dwarkesh Patel
Let’s say tomorrow, the lab leaders get the message. How hard is it? What do they have to do? Do they get more security guards? Do they air-gap? What do they do?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 183

**Chunk:**

Dwarkesh Patel
Let’s say tomorrow, the lab leaders get the message. How hard is it? What do they have to do? Do they get more security guards? Do they air-gap? What do they do?
Leopold Aschenbrenner
People have two reactions: "we're already secure." We’re not.

Then there's fatalism: "it's impossible."

You need to stay ahead of the curve of how AGI-pilled the CCP is. Right now, you've got to be resistant to normal economic espionage. They're not. I probably wouldn't be talking about this stuff if the labs were. I wouldn't want to wake up the CCP more. But this stuff is really trivial for them to do right now.

So they're not resistant to that. It would be possible for a private company to be resistant to it. Both of us have friends in the quantitative trading world. Those secrets are shaped similarly where if I got on a call for an hour with somebody from a competitor firm, most of our alpha would be gone.
Dwarkesh Patel
You're going to worry about that pretty soon.

**Extracted Belief:**

Current security measures implemented in AI research labs are insufficient to prevent economic espionage.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's query about the difficulty of protecting AI research labs from Chinese espionage.

**Justification:**

Aschenbrenner states that labs are not secure and that they are not resistant to economic espionage. He compares the situation to the quantitative trading world, where sensitive information could be easily compromised through conversations with competitors.

--------

## Chunk 184

**Chunk:**

Dwarkesh Patel
Let’s say tomorrow, the lab leaders get the message. How hard is it? What do they have to do? Do they get more security guards? Do they air-gap? What do they do?
Leopold Aschenbrenner
People have two reactions: "we're already secure." We’re not.

Then there's fatalism: "it's impossible."

You need to stay ahead of the curve of how AGI-pilled the CCP is. Right now, you've got to be resistant to normal economic espionage. They're not. I probably wouldn't be talking about this stuff if the labs were. I wouldn't want to wake up the CCP more. But this stuff is really trivial for them to do right now.

So they're not resistant to that. It would be possible for a private company to be resistant to it. Both of us have friends in the quantitative trading world. Those secrets are shaped similarly where if I got on a call for an hour with somebody from a competitor firm, most of our alpha would be gone.
Dwarkesh Patel
You're going to worry about that pretty soon.

**Extracted Belief:**

The Chinese government is actively engaged in economic espionage related to AI research.

**Context:**

Aschenbrenner is expressing his belief that the Chinese government is actively working to obtain information from AI labs.

**Justification:**

Aschenbrenner states that the Chinese government is not resistant to economic espionage and that they are actively engaged in it. He does not provide specific evidence but relies on his expertise and knowledge about the Chinese government's behavior.

--------

## Chunk 185

**Chunk:**

Dwarkesh Patel
Let’s say tomorrow, the lab leaders get the message. How hard is it? What do they have to do? Do they get more security guards? Do they air-gap? What do they do?
Leopold Aschenbrenner
People have two reactions: "we're already secure." We’re not.

Then there's fatalism: "it's impossible."

You need to stay ahead of the curve of how AGI-pilled the CCP is. Right now, you've got to be resistant to normal economic espionage. They're not. I probably wouldn't be talking about this stuff if the labs were. I wouldn't want to wake up the CCP more. But this stuff is really trivial for them to do right now.

So they're not resistant to that. It would be possible for a private company to be resistant to it. Both of us have friends in the quantitative trading world. Those secrets are shaped similarly where if I got on a call for an hour with somebody from a competitor firm, most of our alpha would be gone.
Dwarkesh Patel
You're going to worry about that pretty soon.

**Extracted Belief:**

The Chinese government's activities in AI espionage are easily carried out and pose a significant threat to AI labs.

**Context:**

Aschenbrenner is outlining the severity of the threat posed by Chinese espionage to AI labs.

**Justification:**

He describes the Chinese government's activities as "trivial" and states that he wouldn't be talking about the issue if the labs were adequately secure.

--------

## Chunk 186

**Chunk:**

Dwarkesh Patel
You're going to worry about that pretty soon.
Leopold Aschenbrenner
All the alpha could be gone but in fact, their alpha often persists for many years and decades. So this doesn't seem to happen. There's a lot you could do if you went from current startup security to good private sector security: hedge funds, the way Google treats customer data or whatever. That'd be good right now.

The issue is that basically the CCP will also get more AGI-pilled. At some point, we're going to face the full force of the Ministry of State Security. You're talking about smart people underrating espionage and the insane capabilities of states. This stuff is wild. There are papers about how you can find out the location of where you are in a video game map just from sounds. States can do a lot with electromagnetic emanations.

At some point, you have to be working from a SCIF. Your cluster needs to be air-gapped and basically be a military base. You need to have intense security clearance procedures for employees. All this shit is monitored. They basically have security guards. You can't use any other dependencies. It's all got to be intensely vetted. All your hardware has to be intensely vetted.

If they actually really face the full force of state-level espionage, this isn’t really the thing private companies can do empirically. Microsoft recently had executives' emails hacked by Russian hackers, and government emails they've hosted hacked by government actors. Also, there's just a lot of stuff that only the people behind the security clearances know and only they deal with.

To actually resist the full force of espionage, you're going to need the government. We could do it by always being ahead of the curve. I think we're just going to always be behind the curve, unless we get a sort of government project.
Dwarkesh Patel
Going back to the naive perspective, we're very much coming at this from, “there's going to be a race and the CCP, we must win.” Listen, I understand bad people are in charge of the Chinese government, with the CCP and everything.

I want to step back to a sort of galactic perspective. Humanity is developing AGI. Do we want to come at this from the perspective of "we need to beat China"? To our superintelligent Jupiter brain descendants, China will be some distant memory that they have, America too.

Shouldn't it be more, as an initial approach, just going to them like, “listen, this is superintelligence. We come from a cooperative perspective.” Why immediately rush into it from a hawkish, competitive perspective?

**Extracted Belief:**

Quantitative trading firms are vulnerable to espionage, and a competitor firm could steal their valuable secrets by simply having a one-hour conversation with an employee.

**Context:**

Leopold Aschenbrenner is discussing the vulnerability of artificial intelligence (AGI) research labs to espionage, and uses quantitative trading firms as an example of organizations that have experienced similar vulnerabilities.

**Justification:**

Aschenbrenner states that he has friends in the quantitative trading world and that those secrets are shaped similarly, meaning that a brief conversation with a competitor could compromise their alpha (a measure of trading performance).

--------

## Chunk 187

**Chunk:**

Dwarkesh Patel
You're going to worry about that pretty soon.
Leopold Aschenbrenner
All the alpha could be gone but in fact, their alpha often persists for many years and decades. So this doesn't seem to happen. There's a lot you could do if you went from current startup security to good private sector security: hedge funds, the way Google treats customer data or whatever. That'd be good right now.

The issue is that basically the CCP will also get more AGI-pilled. At some point, we're going to face the full force of the Ministry of State Security. You're talking about smart people underrating espionage and the insane capabilities of states. This stuff is wild. There are papers about how you can find out the location of where you are in a video game map just from sounds. States can do a lot with electromagnetic emanations.

At some point, you have to be working from a SCIF. Your cluster needs to be air-gapped and basically be a military base. You need to have intense security clearance procedures for employees. All this shit is monitored. They basically have security guards. You can't use any other dependencies. It's all got to be intensely vetted. All your hardware has to be intensely vetted.

If they actually really face the full force of state-level espionage, this isn’t really the thing private companies can do empirically. Microsoft recently had executives' emails hacked by Russian hackers, and government emails they've hosted hacked by government actors. Also, there's just a lot of stuff that only the people behind the security clearances know and only they deal with.

To actually resist the full force of espionage, you're going to need the government. We could do it by always being ahead of the curve. I think we're just going to always be behind the curve, unless we get a sort of government project.
Dwarkesh Patel
Going back to the naive perspective, we're very much coming at this from, “there's going to be a race and the CCP, we must win.” Listen, I understand bad people are in charge of the Chinese government, with the CCP and everything.

I want to step back to a sort of galactic perspective. Humanity is developing AGI. Do we want to come at this from the perspective of "we need to beat China"? To our superintelligent Jupiter brain descendants, China will be some distant memory that they have, America too.

Shouldn't it be more, as an initial approach, just going to them like, “listen, this is superintelligence. We come from a cooperative perspective.” Why immediately rush into it from a hawkish, competitive perspective?

**Extracted Belief:**

The Chinese Communist Party (CCP) is becoming increasingly sophisticated in its use of artificial intelligence (AGI) and is likely to escalate its espionage efforts against AGI research labs.

**Context:**

Aschenbrenner is discussing the potential threat of espionage from the CCP against AGI research labs.

**Justification:**

Aschenbrenner uses the phrase 'AGI-pilled' to describe the CCP's increasing interest and capabilities in AI, suggesting a rising level of sophistication and a potential for more advanced espionage.

--------

## Chunk 188

**Chunk:**

Dwarkesh Patel
You're going to worry about that pretty soon.
Leopold Aschenbrenner
All the alpha could be gone but in fact, their alpha often persists for many years and decades. So this doesn't seem to happen. There's a lot you could do if you went from current startup security to good private sector security: hedge funds, the way Google treats customer data or whatever. That'd be good right now.

The issue is that basically the CCP will also get more AGI-pilled. At some point, we're going to face the full force of the Ministry of State Security. You're talking about smart people underrating espionage and the insane capabilities of states. This stuff is wild. There are papers about how you can find out the location of where you are in a video game map just from sounds. States can do a lot with electromagnetic emanations.

At some point, you have to be working from a SCIF. Your cluster needs to be air-gapped and basically be a military base. You need to have intense security clearance procedures for employees. All this shit is monitored. They basically have security guards. You can't use any other dependencies. It's all got to be intensely vetted. All your hardware has to be intensely vetted.

If they actually really face the full force of state-level espionage, this isn’t really the thing private companies can do empirically. Microsoft recently had executives' emails hacked by Russian hackers, and government emails they've hosted hacked by government actors. Also, there's just a lot of stuff that only the people behind the security clearances know and only they deal with.

To actually resist the full force of espionage, you're going to need the government. We could do it by always being ahead of the curve. I think we're just going to always be behind the curve, unless we get a sort of government project.
Dwarkesh Patel
Going back to the naive perspective, we're very much coming at this from, “there's going to be a race and the CCP, we must win.” Listen, I understand bad people are in charge of the Chinese government, with the CCP and everything.

I want to step back to a sort of galactic perspective. Humanity is developing AGI. Do we want to come at this from the perspective of "we need to beat China"? To our superintelligent Jupiter brain descendants, China will be some distant memory that they have, America too.

Shouldn't it be more, as an initial approach, just going to them like, “listen, this is superintelligence. We come from a cooperative perspective.” Why immediately rush into it from a hawkish, competitive perspective?

**Extracted Belief:**

The full force of state-level espionage is a significant threat that private companies cannot adequately defend against.

**Context:**

Aschenbrenner is discussing the limitations of private sector security measures in countering state-sponsored espionage.

**Justification:**

Aschenbrenner cites the example of Microsoft, whose executives' emails were hacked by Russian hackers, and government emails hosted by Microsoft were also hacked by government actors, highlighting the difficulty of defending against sophisticated state-level attacks.

--------

## Chunk 189

**Chunk:**

Dwarkesh Patel
You're going to worry about that pretty soon.
Leopold Aschenbrenner
All the alpha could be gone but in fact, their alpha often persists for many years and decades. So this doesn't seem to happen. There's a lot you could do if you went from current startup security to good private sector security: hedge funds, the way Google treats customer data or whatever. That'd be good right now.

The issue is that basically the CCP will also get more AGI-pilled. At some point, we're going to face the full force of the Ministry of State Security. You're talking about smart people underrating espionage and the insane capabilities of states. This stuff is wild. There are papers about how you can find out the location of where you are in a video game map just from sounds. States can do a lot with electromagnetic emanations.

At some point, you have to be working from a SCIF. Your cluster needs to be air-gapped and basically be a military base. You need to have intense security clearance procedures for employees. All this shit is monitored. They basically have security guards. You can't use any other dependencies. It's all got to be intensely vetted. All your hardware has to be intensely vetted.

If they actually really face the full force of state-level espionage, this isn’t really the thing private companies can do empirically. Microsoft recently had executives' emails hacked by Russian hackers, and government emails they've hosted hacked by government actors. Also, there's just a lot of stuff that only the people behind the security clearances know and only they deal with.

To actually resist the full force of espionage, you're going to need the government. We could do it by always being ahead of the curve. I think we're just going to always be behind the curve, unless we get a sort of government project.
Dwarkesh Patel
Going back to the naive perspective, we're very much coming at this from, “there's going to be a race and the CCP, we must win.” Listen, I understand bad people are in charge of the Chinese government, with the CCP and everything.

I want to step back to a sort of galactic perspective. Humanity is developing AGI. Do we want to come at this from the perspective of "we need to beat China"? To our superintelligent Jupiter brain descendants, China will be some distant memory that they have, America too.

Shouldn't it be more, as an initial approach, just going to them like, “listen, this is superintelligence. We come from a cooperative perspective.” Why immediately rush into it from a hawkish, competitive perspective?

**Extracted Belief:**

Government intervention is necessary to effectively defend against state-level espionage in the context of artificial intelligence (AGI) research.

**Context:**

Aschenbrenner argues for government involvement in securing AGI research.

**Justification:**

Aschenbrenner reasons that private companies alone lack the resources and expertise to effectively resist state-level espionage and that government collaboration is essential to stay ahead of the curve.

--------

## Chunk 190

**Chunk:**

Dwarkesh Patel
Going back to the naive perspective, we're very much coming at this from, “there's going to be a race and the CCP, we must win.” Listen, I understand bad people are in charge of the Chinese government, with the CCP and everything.

I want to step back to a sort of galactic perspective. Humanity is developing AGI. Do we want to come at this from the perspective of "we need to beat China"? To our superintelligent Jupiter brain descendants, China will be some distant memory that they have, America too.

Shouldn't it be more, as an initial approach, just going to them like, “listen, this is superintelligence. We come from a cooperative perspective.” Why immediately rush into it from a hawkish, competitive perspective?
Leopold Aschenbrenner
A lot of the stuff I talk about in the series is primarily descriptive. On the China stuff,  in some ideal world, it's just all merry-go-round and cooperation. Again, people wake up to AGI. The issue in particular is, can we make a deal? Can we make an international treaty? It really relates to the stability of international arms control agreements.

We did very successful arms control on nuclear weapons in the 1980s. The reason it was successful is because the new equilibrium was stable. You go down from 60,000 nukes to 10,000 nukes or whatever. When you have 10,000 nukes, breakout basically doesn't matter that much.

Suppose the other guy now tried to make 20,000 nukes. Who cares? It's still mutually assured destruction. Suppose a rogue state went from zero nukes to one nuke. Who cares? We still have way more nukes than you. It's still not ideal for destabilization.

It'd be very different if the arms control agreement had been zero nukes. At zero nukes, you just need one rogue state to make one nuke and the whole thing is destabilized. Breakout is very easy. Your adversary state starts making nukes.

When you're going to very low levels of arms or when you're in a very dynamic technological situation, arms control is really tough because breakout is easy. There are some other stories about this in the 1920s and 1930s. All the European states had disarmed.

Germany did this kind of crash program to build the Luftwaffe. That was able to massively destabilize things because they were the first. They were able to pretty easily build a modern air force because the others didn't really have one. That really destabilized things.

The issue with AGI and superintelligence is the explosiveness of it. If you have an intelligence explosion, you're able to go from AGI to superintelligence. That superintelligence is decisive because you’ll developed some crazy WMD or you’ll have some super hacking ability that lets you completely deactivate the enemy arsenal. Suppose you're trying to put in a break. We're both going to cooperate. We're going to go slower on the cusp of AGI.

There is going to be such an enormous incentive to race ahead, to break out. We're just going to do the intelligence explosion. If we can get three months ahead, we win. That makes any sort of arms control agreement very unstable in a close situation.
Dwarkesh Patel
That's really interesting. This is very analogous to a debate I had with Rhodes on the podcast where he argued for nuclear disarmament. If some country tried to break out and started developing nuclear weapons, the six months you would get is enough to get international consensus and invade the country and prevent them from getting nukes. I thought that was not stable equilibrium.

On this, maybe it's a bit easier because you have AGI and so you can monitor the other person's cluster or something. You can see the data centers from space. You can see the energy draw they're getting. As you were saying, there are a lot of ways to get information from an environment if you're really dedicated. Also, unlike nukes, the data centers are fixed. Obviously, you have nukes in submarines, planes, bunkers, mountains, etc. You can have them so many different places. A 100 GW data center, we can blow that shit up if we're concerned. We can just use a cruise missile or something. That's very vulnerable.

**Extracted Belief:**

Arms control agreements are more likely to be successful when the new equilibrium is stable, meaning that even if one party breaks out, the balance of power is not significantly altered.

**Context:**

Leopold Aschenbrenner is explaining the success of nuclear arms control in the 1980s by comparing it to the stability of the new equilibrium that resulted from it, arguing that the reduction of nuclear weapons to a lower level did not destabilize the international order.

**Justification:**

Leopold Aschenbrenner cites the example of nuclear arms control in the 1980s, where the reduction in nuclear weapons from 60,000 to 10,000 did not significantly affect the balance of power. He argues that even if one side were to increase their nuclear arsenal to 20,000, it would still be within the realm of mutually assured destruction.

--------

## Chunk 191

**Chunk:**

Dwarkesh Patel
Going back to the naive perspective, we're very much coming at this from, “there's going to be a race and the CCP, we must win.” Listen, I understand bad people are in charge of the Chinese government, with the CCP and everything.

I want to step back to a sort of galactic perspective. Humanity is developing AGI. Do we want to come at this from the perspective of "we need to beat China"? To our superintelligent Jupiter brain descendants, China will be some distant memory that they have, America too.

Shouldn't it be more, as an initial approach, just going to them like, “listen, this is superintelligence. We come from a cooperative perspective.” Why immediately rush into it from a hawkish, competitive perspective?
Leopold Aschenbrenner
A lot of the stuff I talk about in the series is primarily descriptive. On the China stuff,  in some ideal world, it's just all merry-go-round and cooperation. Again, people wake up to AGI. The issue in particular is, can we make a deal? Can we make an international treaty? It really relates to the stability of international arms control agreements.

We did very successful arms control on nuclear weapons in the 1980s. The reason it was successful is because the new equilibrium was stable. You go down from 60,000 nukes to 10,000 nukes or whatever. When you have 10,000 nukes, breakout basically doesn't matter that much.

Suppose the other guy now tried to make 20,000 nukes. Who cares? It's still mutually assured destruction. Suppose a rogue state went from zero nukes to one nuke. Who cares? We still have way more nukes than you. It's still not ideal for destabilization.

It'd be very different if the arms control agreement had been zero nukes. At zero nukes, you just need one rogue state to make one nuke and the whole thing is destabilized. Breakout is very easy. Your adversary state starts making nukes.

When you're going to very low levels of arms or when you're in a very dynamic technological situation, arms control is really tough because breakout is easy. There are some other stories about this in the 1920s and 1930s. All the European states had disarmed.

Germany did this kind of crash program to build the Luftwaffe. That was able to massively destabilize things because they were the first. They were able to pretty easily build a modern air force because the others didn't really have one. That really destabilized things.

The issue with AGI and superintelligence is the explosiveness of it. If you have an intelligence explosion, you're able to go from AGI to superintelligence. That superintelligence is decisive because you’ll developed some crazy WMD or you’ll have some super hacking ability that lets you completely deactivate the enemy arsenal. Suppose you're trying to put in a break. We're both going to cooperate. We're going to go slower on the cusp of AGI.

There is going to be such an enormous incentive to race ahead, to break out. We're just going to do the intelligence explosion. If we can get three months ahead, we win. That makes any sort of arms control agreement very unstable in a close situation.
Dwarkesh Patel
That's really interesting. This is very analogous to a debate I had with Rhodes on the podcast where he argued for nuclear disarmament. If some country tried to break out and started developing nuclear weapons, the six months you would get is enough to get international consensus and invade the country and prevent them from getting nukes. I thought that was not stable equilibrium.

On this, maybe it's a bit easier because you have AGI and so you can monitor the other person's cluster or something. You can see the data centers from space. You can see the energy draw they're getting. As you were saying, there are a lot of ways to get information from an environment if you're really dedicated. Also, unlike nukes, the data centers are fixed. Obviously, you have nukes in submarines, planes, bunkers, mountains, etc. You can have them so many different places. A 100 GW data center, we can blow that shit up if we're concerned. We can just use a cruise missile or something. That's very vulnerable.

**Extracted Belief:**

Arms control agreements are less likely to be successful when breakout is easy, meaning that one party can easily gain a significant advantage by breaking the agreement.

**Context:**

Leopold Aschenbrenner is contrasting the success of arms control with the challenge of arms control in a dynamic technological situation, arguing that the ease of breakout makes such agreements unstable.

**Justification:**

Leopold Aschenbrenner points out that if the nuclear arms control agreement had been zero nukes, a single rogue state could destabilize the entire system by acquiring just one nuke. He also uses the example of Germany's rapid buildup of the Luftwaffe in the 1920s and 1930s, which destabilized Europe because other states were unprepared.

--------

## Chunk 192

**Chunk:**

Dwarkesh Patel
Going back to the naive perspective, we're very much coming at this from, “there's going to be a race and the CCP, we must win.” Listen, I understand bad people are in charge of the Chinese government, with the CCP and everything.

I want to step back to a sort of galactic perspective. Humanity is developing AGI. Do we want to come at this from the perspective of "we need to beat China"? To our superintelligent Jupiter brain descendants, China will be some distant memory that they have, America too.

Shouldn't it be more, as an initial approach, just going to them like, “listen, this is superintelligence. We come from a cooperative perspective.” Why immediately rush into it from a hawkish, competitive perspective?
Leopold Aschenbrenner
A lot of the stuff I talk about in the series is primarily descriptive. On the China stuff,  in some ideal world, it's just all merry-go-round and cooperation. Again, people wake up to AGI. The issue in particular is, can we make a deal? Can we make an international treaty? It really relates to the stability of international arms control agreements.

We did very successful arms control on nuclear weapons in the 1980s. The reason it was successful is because the new equilibrium was stable. You go down from 60,000 nukes to 10,000 nukes or whatever. When you have 10,000 nukes, breakout basically doesn't matter that much.

Suppose the other guy now tried to make 20,000 nukes. Who cares? It's still mutually assured destruction. Suppose a rogue state went from zero nukes to one nuke. Who cares? We still have way more nukes than you. It's still not ideal for destabilization.

It'd be very different if the arms control agreement had been zero nukes. At zero nukes, you just need one rogue state to make one nuke and the whole thing is destabilized. Breakout is very easy. Your adversary state starts making nukes.

When you're going to very low levels of arms or when you're in a very dynamic technological situation, arms control is really tough because breakout is easy. There are some other stories about this in the 1920s and 1930s. All the European states had disarmed.

Germany did this kind of crash program to build the Luftwaffe. That was able to massively destabilize things because they were the first. They were able to pretty easily build a modern air force because the others didn't really have one. That really destabilized things.

The issue with AGI and superintelligence is the explosiveness of it. If you have an intelligence explosion, you're able to go from AGI to superintelligence. That superintelligence is decisive because you’ll developed some crazy WMD or you’ll have some super hacking ability that lets you completely deactivate the enemy arsenal. Suppose you're trying to put in a break. We're both going to cooperate. We're going to go slower on the cusp of AGI.

There is going to be such an enormous incentive to race ahead, to break out. We're just going to do the intelligence explosion. If we can get three months ahead, we win. That makes any sort of arms control agreement very unstable in a close situation.
Dwarkesh Patel
That's really interesting. This is very analogous to a debate I had with Rhodes on the podcast where he argued for nuclear disarmament. If some country tried to break out and started developing nuclear weapons, the six months you would get is enough to get international consensus and invade the country and prevent them from getting nukes. I thought that was not stable equilibrium.

On this, maybe it's a bit easier because you have AGI and so you can monitor the other person's cluster or something. You can see the data centers from space. You can see the energy draw they're getting. As you were saying, there are a lot of ways to get information from an environment if you're really dedicated. Also, unlike nukes, the data centers are fixed. Obviously, you have nukes in submarines, planes, bunkers, mountains, etc. You can have them so many different places. A 100 GW data center, we can blow that shit up if we're concerned. We can just use a cruise missile or something. That's very vulnerable.

**Extracted Belief:**

The development of superintelligence is inherently unstable because it offers a decisive advantage to whoever achieves it first.

**Context:**

Leopold Aschenbrenner is arguing that the explosive nature of intelligence explosion makes arms control difficult in the context of superintelligence, as the first to achieve it gains an insurmountable advantage.

**Justification:**

He argues that a three-month lead in the development of superintelligence could be enough to win, as the first to achieve it could develop powerful weapons or hacking capabilities that could disable their adversaries.

--------

## Chunk 193

**Chunk:**

Dwarkesh Patel
That's really interesting. This is very analogous to a debate I had with Rhodes on the podcast where he argued for nuclear disarmament. If some country tried to break out and started developing nuclear weapons, the six months you would get is enough to get international consensus and invade the country and prevent them from getting nukes. I thought that was not stable equilibrium.

On this, maybe it's a bit easier because you have AGI and so you can monitor the other person's cluster or something. You can see the data centers from space. You can see the energy draw they're getting. As you were saying, there are a lot of ways to get information from an environment if you're really dedicated. Also, unlike nukes, the data centers are fixed. Obviously, you have nukes in submarines, planes, bunkers, mountains, etc. You can have them so many different places. A 100 GW data center, we can blow that shit up if we're concerned. We can just use a cruise missile or something. That's very vulnerable.
Leopold Aschenbrenner
That gets to the insane vulnerability and the volatility of this period, post-superintelligence. You have the intelligence explosion. You have these vastly superhuman things on your cluster. You haven't done the industrial explosion yet. You don't have your robots yet. You haven't covered the desert in robot factories yet.

That is this crazy moment. Say the United States is ahead. The CCP is somewhat behind. There's actually an enormous incentive for first strike, if they can take out your data center. They know you're about to have this command, a decisive lead. They know if they can just take out this data center, then they can stop it. They might get desperate.

We're going to get into a position that's going to be pretty hard to defend early on. We're basically going to be in a position where we're protecting data centers with the threat of nuclear retaliation. Maybe it sounds kind of crazy.
Dwarkesh Patel
Is this the inverse of the Eliezer…?

**Extracted Belief:**

The period after the development of superintelligence will be characterized by extreme vulnerability and volatility.

**Context:**

Leopold Aschenbrenner is discussing the potential dangers and risks associated with the development of superintelligence, specifically highlighting the instability that could arise in the post-superintelligence era.

**Justification:**

He references the potential for first strikes against data centers, a situation he describes as "insane vulnerability" and "volatility." This suggests he believes this vulnerability is based on observable facts and trends.

--------

## Chunk 194

**Chunk:**

Dwarkesh Patel
That's really interesting. This is very analogous to a debate I had with Rhodes on the podcast where he argued for nuclear disarmament. If some country tried to break out and started developing nuclear weapons, the six months you would get is enough to get international consensus and invade the country and prevent them from getting nukes. I thought that was not stable equilibrium.

On this, maybe it's a bit easier because you have AGI and so you can monitor the other person's cluster or something. You can see the data centers from space. You can see the energy draw they're getting. As you were saying, there are a lot of ways to get information from an environment if you're really dedicated. Also, unlike nukes, the data centers are fixed. Obviously, you have nukes in submarines, planes, bunkers, mountains, etc. You can have them so many different places. A 100 GW data center, we can blow that shit up if we're concerned. We can just use a cruise missile or something. That's very vulnerable.
Leopold Aschenbrenner
That gets to the insane vulnerability and the volatility of this period, post-superintelligence. You have the intelligence explosion. You have these vastly superhuman things on your cluster. You haven't done the industrial explosion yet. You don't have your robots yet. You haven't covered the desert in robot factories yet.

That is this crazy moment. Say the United States is ahead. The CCP is somewhat behind. There's actually an enormous incentive for first strike, if they can take out your data center. They know you're about to have this command, a decisive lead. They know if they can just take out this data center, then they can stop it. They might get desperate.

We're going to get into a position that's going to be pretty hard to defend early on. We're basically going to be in a position where we're protecting data centers with the threat of nuclear retaliation. Maybe it sounds kind of crazy.
Dwarkesh Patel
Is this the inverse of the Eliezer…?

**Extracted Belief:**

The development of superintelligence will be marked by an 'intelligence explosion' where rapid advancements in AI will lead to a significant leap in capabilities.

**Context:**

Leopold Aschenbrenner is explaining the potential consequences of the development of superintelligence, specifically the rapid acceleration of AI capabilities beyond human comprehension.

**Justification:**

He refers to this period as an "intelligence explosion," implying it's a recognized phenomenon in the field of AI development, based on observed trends and projections.

--------

## Chunk 195

**Chunk:**

Dwarkesh Patel
That's really interesting. This is very analogous to a debate I had with Rhodes on the podcast where he argued for nuclear disarmament. If some country tried to break out and started developing nuclear weapons, the six months you would get is enough to get international consensus and invade the country and prevent them from getting nukes. I thought that was not stable equilibrium.

On this, maybe it's a bit easier because you have AGI and so you can monitor the other person's cluster or something. You can see the data centers from space. You can see the energy draw they're getting. As you were saying, there are a lot of ways to get information from an environment if you're really dedicated. Also, unlike nukes, the data centers are fixed. Obviously, you have nukes in submarines, planes, bunkers, mountains, etc. You can have them so many different places. A 100 GW data center, we can blow that shit up if we're concerned. We can just use a cruise missile or something. That's very vulnerable.
Leopold Aschenbrenner
That gets to the insane vulnerability and the volatility of this period, post-superintelligence. You have the intelligence explosion. You have these vastly superhuman things on your cluster. You haven't done the industrial explosion yet. You don't have your robots yet. You haven't covered the desert in robot factories yet.

That is this crazy moment. Say the United States is ahead. The CCP is somewhat behind. There's actually an enormous incentive for first strike, if they can take out your data center. They know you're about to have this command, a decisive lead. They know if they can just take out this data center, then they can stop it. They might get desperate.

We're going to get into a position that's going to be pretty hard to defend early on. We're basically going to be in a position where we're protecting data centers with the threat of nuclear retaliation. Maybe it sounds kind of crazy.
Dwarkesh Patel
Is this the inverse of the Eliezer…?

**Extracted Belief:**

The early stage of superintelligence development will be characterized by a lack of widespread industrial automation and robotics.

**Context:**

Leopold Aschenbrenner is describing the potential vulnerabilities during the early stages of superintelligence development, highlighting the lack of advanced industrial automation and robotics as a critical factor.

**Justification:**

He explicitly states, "You haven't done the industrial explosion yet. You don't have your robots yet. You haven't covered the desert in robot factories yet." This suggests his belief is based on the current state of technology and the anticipated timeline for the development of widespread robotics.

--------

## Chunk 196

**Chunk:**

Dwarkesh Patel
That's really interesting. This is very analogous to a debate I had with Rhodes on the podcast where he argued for nuclear disarmament. If some country tried to break out and started developing nuclear weapons, the six months you would get is enough to get international consensus and invade the country and prevent them from getting nukes. I thought that was not stable equilibrium.

On this, maybe it's a bit easier because you have AGI and so you can monitor the other person's cluster or something. You can see the data centers from space. You can see the energy draw they're getting. As you were saying, there are a lot of ways to get information from an environment if you're really dedicated. Also, unlike nukes, the data centers are fixed. Obviously, you have nukes in submarines, planes, bunkers, mountains, etc. You can have them so many different places. A 100 GW data center, we can blow that shit up if we're concerned. We can just use a cruise missile or something. That's very vulnerable.
Leopold Aschenbrenner
That gets to the insane vulnerability and the volatility of this period, post-superintelligence. You have the intelligence explosion. You have these vastly superhuman things on your cluster. You haven't done the industrial explosion yet. You don't have your robots yet. You haven't covered the desert in robot factories yet.

That is this crazy moment. Say the United States is ahead. The CCP is somewhat behind. There's actually an enormous incentive for first strike, if they can take out your data center. They know you're about to have this command, a decisive lead. They know if they can just take out this data center, then they can stop it. They might get desperate.

We're going to get into a position that's going to be pretty hard to defend early on. We're basically going to be in a position where we're protecting data centers with the threat of nuclear retaliation. Maybe it sounds kind of crazy.
Dwarkesh Patel
Is this the inverse of the Eliezer…?

**Extracted Belief:**

There will be a strong incentive for a first strike against a nation's data centers during the early stages of superintelligence development.

**Context:**

Leopold Aschenbrenner is arguing that the early stages of superintelligence development will be characterized by a high risk of conflict due to the potential for a first strike against data centers.

**Justification:**

He explains that if one nation is ahead in superintelligence development, there will be an "enormous incentive for first strike" to disrupt the other nation's progress. This is based on the logical deduction that a nation would seek to maintain its advantage or prevent another nation from gaining a decisive lead.

--------

## Chunk 197

**Chunk:**

Dwarkesh Patel
That's really interesting. This is very analogous to a debate I had with Rhodes on the podcast where he argued for nuclear disarmament. If some country tried to break out and started developing nuclear weapons, the six months you would get is enough to get international consensus and invade the country and prevent them from getting nukes. I thought that was not stable equilibrium.

On this, maybe it's a bit easier because you have AGI and so you can monitor the other person's cluster or something. You can see the data centers from space. You can see the energy draw they're getting. As you were saying, there are a lot of ways to get information from an environment if you're really dedicated. Also, unlike nukes, the data centers are fixed. Obviously, you have nukes in submarines, planes, bunkers, mountains, etc. You can have them so many different places. A 100 GW data center, we can blow that shit up if we're concerned. We can just use a cruise missile or something. That's very vulnerable.
Leopold Aschenbrenner
That gets to the insane vulnerability and the volatility of this period, post-superintelligence. You have the intelligence explosion. You have these vastly superhuman things on your cluster. You haven't done the industrial explosion yet. You don't have your robots yet. You haven't covered the desert in robot factories yet.

That is this crazy moment. Say the United States is ahead. The CCP is somewhat behind. There's actually an enormous incentive for first strike, if they can take out your data center. They know you're about to have this command, a decisive lead. They know if they can just take out this data center, then they can stop it. They might get desperate.

We're going to get into a position that's going to be pretty hard to defend early on. We're basically going to be in a position where we're protecting data centers with the threat of nuclear retaliation. Maybe it sounds kind of crazy.
Dwarkesh Patel
Is this the inverse of the Eliezer…?

**Extracted Belief:**

The early stages of superintelligence development will be difficult to defend against.

**Context:**

Leopold Aschenbrenner is acknowledging the challenges of protecting data centers in the early stages of superintelligence development, emphasizing the difficulty in defending against potential attacks.

**Justification:**

He states, "We're going to get into a position that's going to be pretty hard to defend early on." His belief is based on an assessment of the anticipated technological landscape and the vulnerability of data centers in the early stages of superintelligence.

--------

## Chunk 198

**Chunk:**

Dwarkesh Patel
That's really interesting. This is very analogous to a debate I had with Rhodes on the podcast where he argued for nuclear disarmament. If some country tried to break out and started developing nuclear weapons, the six months you would get is enough to get international consensus and invade the country and prevent them from getting nukes. I thought that was not stable equilibrium.

On this, maybe it's a bit easier because you have AGI and so you can monitor the other person's cluster or something. You can see the data centers from space. You can see the energy draw they're getting. As you were saying, there are a lot of ways to get information from an environment if you're really dedicated. Also, unlike nukes, the data centers are fixed. Obviously, you have nukes in submarines, planes, bunkers, mountains, etc. You can have them so many different places. A 100 GW data center, we can blow that shit up if we're concerned. We can just use a cruise missile or something. That's very vulnerable.
Leopold Aschenbrenner
That gets to the insane vulnerability and the volatility of this period, post-superintelligence. You have the intelligence explosion. You have these vastly superhuman things on your cluster. You haven't done the industrial explosion yet. You don't have your robots yet. You haven't covered the desert in robot factories yet.

That is this crazy moment. Say the United States is ahead. The CCP is somewhat behind. There's actually an enormous incentive for first strike, if they can take out your data center. They know you're about to have this command, a decisive lead. They know if they can just take out this data center, then they can stop it. They might get desperate.

We're going to get into a position that's going to be pretty hard to defend early on. We're basically going to be in a position where we're protecting data centers with the threat of nuclear retaliation. Maybe it sounds kind of crazy.
Dwarkesh Patel
Is this the inverse of the Eliezer…?

**Extracted Belief:**

The early stages of superintelligence development will require the use of nuclear deterrence to protect data centers.

**Context:**

Leopold Aschenbrenner is proposing a potential solution to the vulnerability of data centers in the early stages of superintelligence development, suggesting the use of nuclear deterrence as a defensive strategy.

**Justification:**

He states, "We're basically going to be in a position where we're protecting data centers with the threat of nuclear retaliation." This suggests he believes nuclear deterrence could be a logical strategy for defending against potential attacks on data centers, although he recognizes it's a "terrible option." 

--------

## Chunk 199

**Chunk:**

Dwarkesh Patel
Is this the inverse of the Eliezer…?
Leopold Aschenbrenner
Nuclear deterrence for data centers. This is Berlin in the late 1950s, early 1960s. Both Eisenhower and Kennedy multiple times made the threat of full-on nuclear war against the Soviets if they tried to encroach on West Berlin.

It's sort of insane. It's kind of insane that that went well. Basically, that's going to be the only option for the data centers. It's a terrible option. This whole scheme is terrible. Being in  neck and neck race at this point is terrible.

I have some uncertainty on how easy that decisive advantage will be. I'm pretty confident that if you have superintelligence, you have two years, you have the robots, you're able to get that 30-year lead. Then you're in this Gulf War 1 situation. You have your millions or billions of mosquito-sized drones that can just take it out. There's even a possibility you can get a decisive advantage earlier.

There are these stories about colonization in the 1500s where a few hundred Spaniards were able to topple the Aztec Empire, a couple of other empires as well. Each of these had a few million people. It was not a godlike technological advantage. It was some technological advantage. It was some amount of disease and cunning strategic play.

There's a possibility that even early on — when you haven't gone through the full industrial explosion yet — you have superintelligence, but you're able to manipulate the opposing generals, claiming you're allying with them. Then you have some crazy new bioweapons. Maybe there's even some way to pretty easily get a paradigm that deactivates enemy nukes. This stuff could get pretty wild.

Here's what we should do. I really don't want this volatile period. A deal with China would be nice. It's going to be really tough if you're in this unstable equilibrium. We want to get in a position where it is clear that the United States, a coalition of democratic allies, will win. It is clear to the United States, it is clear to China. That will require having locked down the secrets, having built the 100 gigawatt cluster in the United States, having done the natural gas and doing what's necessary.

When it is clear that the democratic coalition is well ahead, you go to China and offer them a deal. China will know we’re going to win. They're very scared of what's going to happen. We're going to know we're going to win, but we're also very scared of what's going to happen because we really want to avoid this kind of breakneck race right at the end. Things could really go awry.

We offer them a deal. There's an incentive to come to the table. There's a more stable arrangement you can do. It's an Atoms for Peace arrangement. We're like, "look, we're going to respect you. We're not going to use superintelligence against you. You can do what you want. You're going to get your slice of the galaxy.

We're going to benefit-share with you. We're going to have some compute agreement where there's some ratio of compute that you're allowed to have, enforced with opposing AIs or whatever. We're just not going to do this volatile WMD arms race to the death.

It's a new world order that's US-led, democracy-led, but respects China and lets them do what they want.
Dwarkesh Patel
There's so much there. On the galaxies thing, there’s a funny anecdote. I kind of want to tell it. We were at an event. I'm respecting Chatham House rules here. I'm not revealing anything about it. Leopold was talking to somebody influential. Afterwards, that person told the group, "Leopold told me he's not going to spend any money on consumption until he's ready to buy galaxies."

The guy goes, "I honestly don't know if he meant galaxies like the brand of private plane Galaxy or physical galaxies." There was an actual debate. He went away to the restroom. There was an actual debate among influential people about whether he meant Galaxys. Others who knew you better were like, "no, he means galaxies."

**Extracted Belief:**

Nuclear deterrence, specifically the threat of full-scale nuclear war, can be used to protect data centers from attack.

**Context:**

Leopold Aschenbrenner is drawing a parallel between the Cold War era and the potential future of AI development, suggesting that nuclear deterrence might be necessary to safeguard critical data centers.

**Justification:**

He cites the example of the Berlin Crisis in the late 1950s and early 1960s, where the US and USSR engaged in nuclear brinkmanship over West Berlin. He suggests that this strategy, while 'insane', might be the only option for defending data centers from an AI-driven first strike.

--------

## Chunk 200

**Chunk:**

Dwarkesh Patel
Is this the inverse of the Eliezer…?
Leopold Aschenbrenner
Nuclear deterrence for data centers. This is Berlin in the late 1950s, early 1960s. Both Eisenhower and Kennedy multiple times made the threat of full-on nuclear war against the Soviets if they tried to encroach on West Berlin.

It's sort of insane. It's kind of insane that that went well. Basically, that's going to be the only option for the data centers. It's a terrible option. This whole scheme is terrible. Being in  neck and neck race at this point is terrible.

I have some uncertainty on how easy that decisive advantage will be. I'm pretty confident that if you have superintelligence, you have two years, you have the robots, you're able to get that 30-year lead. Then you're in this Gulf War 1 situation. You have your millions or billions of mosquito-sized drones that can just take it out. There's even a possibility you can get a decisive advantage earlier.

There are these stories about colonization in the 1500s where a few hundred Spaniards were able to topple the Aztec Empire, a couple of other empires as well. Each of these had a few million people. It was not a godlike technological advantage. It was some technological advantage. It was some amount of disease and cunning strategic play.

There's a possibility that even early on — when you haven't gone through the full industrial explosion yet — you have superintelligence, but you're able to manipulate the opposing generals, claiming you're allying with them. Then you have some crazy new bioweapons. Maybe there's even some way to pretty easily get a paradigm that deactivates enemy nukes. This stuff could get pretty wild.

Here's what we should do. I really don't want this volatile period. A deal with China would be nice. It's going to be really tough if you're in this unstable equilibrium. We want to get in a position where it is clear that the United States, a coalition of democratic allies, will win. It is clear to the United States, it is clear to China. That will require having locked down the secrets, having built the 100 gigawatt cluster in the United States, having done the natural gas and doing what's necessary.

When it is clear that the democratic coalition is well ahead, you go to China and offer them a deal. China will know we’re going to win. They're very scared of what's going to happen. We're going to know we're going to win, but we're also very scared of what's going to happen because we really want to avoid this kind of breakneck race right at the end. Things could really go awry.

We offer them a deal. There's an incentive to come to the table. There's a more stable arrangement you can do. It's an Atoms for Peace arrangement. We're like, "look, we're going to respect you. We're not going to use superintelligence against you. You can do what you want. You're going to get your slice of the galaxy.

We're going to benefit-share with you. We're going to have some compute agreement where there's some ratio of compute that you're allowed to have, enforced with opposing AIs or whatever. We're just not going to do this volatile WMD arms race to the death.

It's a new world order that's US-led, democracy-led, but respects China and lets them do what they want.
Dwarkesh Patel
There's so much there. On the galaxies thing, there’s a funny anecdote. I kind of want to tell it. We were at an event. I'm respecting Chatham House rules here. I'm not revealing anything about it. Leopold was talking to somebody influential. Afterwards, that person told the group, "Leopold told me he's not going to spend any money on consumption until he's ready to buy galaxies."

The guy goes, "I honestly don't know if he meant galaxies like the brand of private plane Galaxy or physical galaxies." There was an actual debate. He went away to the restroom. There was an actual debate among influential people about whether he meant Galaxys. Others who knew you better were like, "no, he means galaxies."

**Extracted Belief:**

A 'neck-and-neck race' between nations in the development of superintelligence is a dangerous and unstable situation.

**Context:**

Leopold Aschenbrenner is expressing concern about the potential consequences of a close competition for superintelligence, emphasizing the risks associated with a rapid and unpredictable arms race.

**Justification:**

He highlights the volatile nature of such a scenario, referencing the instability of arms control agreements when dealing with low levels of weapons or rapidly evolving technologies. He suggests that this kind of race would be highly unstable, leading to a 'breakneck race right at the end' with potentially disastrous outcomes.

--------

## Chunk 201

**Chunk:**

Dwarkesh Patel
Is this the inverse of the Eliezer…?
Leopold Aschenbrenner
Nuclear deterrence for data centers. This is Berlin in the late 1950s, early 1960s. Both Eisenhower and Kennedy multiple times made the threat of full-on nuclear war against the Soviets if they tried to encroach on West Berlin.

It's sort of insane. It's kind of insane that that went well. Basically, that's going to be the only option for the data centers. It's a terrible option. This whole scheme is terrible. Being in  neck and neck race at this point is terrible.

I have some uncertainty on how easy that decisive advantage will be. I'm pretty confident that if you have superintelligence, you have two years, you have the robots, you're able to get that 30-year lead. Then you're in this Gulf War 1 situation. You have your millions or billions of mosquito-sized drones that can just take it out. There's even a possibility you can get a decisive advantage earlier.

There are these stories about colonization in the 1500s where a few hundred Spaniards were able to topple the Aztec Empire, a couple of other empires as well. Each of these had a few million people. It was not a godlike technological advantage. It was some technological advantage. It was some amount of disease and cunning strategic play.

There's a possibility that even early on — when you haven't gone through the full industrial explosion yet — you have superintelligence, but you're able to manipulate the opposing generals, claiming you're allying with them. Then you have some crazy new bioweapons. Maybe there's even some way to pretty easily get a paradigm that deactivates enemy nukes. This stuff could get pretty wild.

Here's what we should do. I really don't want this volatile period. A deal with China would be nice. It's going to be really tough if you're in this unstable equilibrium. We want to get in a position where it is clear that the United States, a coalition of democratic allies, will win. It is clear to the United States, it is clear to China. That will require having locked down the secrets, having built the 100 gigawatt cluster in the United States, having done the natural gas and doing what's necessary.

When it is clear that the democratic coalition is well ahead, you go to China and offer them a deal. China will know we’re going to win. They're very scared of what's going to happen. We're going to know we're going to win, but we're also very scared of what's going to happen because we really want to avoid this kind of breakneck race right at the end. Things could really go awry.

We offer them a deal. There's an incentive to come to the table. There's a more stable arrangement you can do. It's an Atoms for Peace arrangement. We're like, "look, we're going to respect you. We're not going to use superintelligence against you. You can do what you want. You're going to get your slice of the galaxy.

We're going to benefit-share with you. We're going to have some compute agreement where there's some ratio of compute that you're allowed to have, enforced with opposing AIs or whatever. We're just not going to do this volatile WMD arms race to the death.

It's a new world order that's US-led, democracy-led, but respects China and lets them do what they want.
Dwarkesh Patel
There's so much there. On the galaxies thing, there’s a funny anecdote. I kind of want to tell it. We were at an event. I'm respecting Chatham House rules here. I'm not revealing anything about it. Leopold was talking to somebody influential. Afterwards, that person told the group, "Leopold told me he's not going to spend any money on consumption until he's ready to buy galaxies."

The guy goes, "I honestly don't know if he meant galaxies like the brand of private plane Galaxy or physical galaxies." There was an actual debate. He went away to the restroom. There was an actual debate among influential people about whether he meant Galaxys. Others who knew you better were like, "no, he means galaxies."

**Extracted Belief:**

Achieving a decisive technological advantage in superintelligence is possible and could lead to a significant power imbalance.

**Context:**

Leopold Aschenbrenner is outlining the potential for a nation to gain a significant lead in AI development, possibly leading to a scenario similar to the Gulf War.

**Justification:**

He states that with superintelligence and the subsequent development of robots, a nation could achieve a 30-year technological lead, enabling them to dominate through advanced weaponry like drones. He suggests that this scenario is plausible, drawing parallels to historical examples of technological superiority, such as the Spanish colonization of the Americas.

--------

## Chunk 202

**Chunk:**

Dwarkesh Patel
Is this the inverse of the Eliezer…?
Leopold Aschenbrenner
Nuclear deterrence for data centers. This is Berlin in the late 1950s, early 1960s. Both Eisenhower and Kennedy multiple times made the threat of full-on nuclear war against the Soviets if they tried to encroach on West Berlin.

It's sort of insane. It's kind of insane that that went well. Basically, that's going to be the only option for the data centers. It's a terrible option. This whole scheme is terrible. Being in  neck and neck race at this point is terrible.

I have some uncertainty on how easy that decisive advantage will be. I'm pretty confident that if you have superintelligence, you have two years, you have the robots, you're able to get that 30-year lead. Then you're in this Gulf War 1 situation. You have your millions or billions of mosquito-sized drones that can just take it out. There's even a possibility you can get a decisive advantage earlier.

There are these stories about colonization in the 1500s where a few hundred Spaniards were able to topple the Aztec Empire, a couple of other empires as well. Each of these had a few million people. It was not a godlike technological advantage. It was some technological advantage. It was some amount of disease and cunning strategic play.

There's a possibility that even early on — when you haven't gone through the full industrial explosion yet — you have superintelligence, but you're able to manipulate the opposing generals, claiming you're allying with them. Then you have some crazy new bioweapons. Maybe there's even some way to pretty easily get a paradigm that deactivates enemy nukes. This stuff could get pretty wild.

Here's what we should do. I really don't want this volatile period. A deal with China would be nice. It's going to be really tough if you're in this unstable equilibrium. We want to get in a position where it is clear that the United States, a coalition of democratic allies, will win. It is clear to the United States, it is clear to China. That will require having locked down the secrets, having built the 100 gigawatt cluster in the United States, having done the natural gas and doing what's necessary.

When it is clear that the democratic coalition is well ahead, you go to China and offer them a deal. China will know we’re going to win. They're very scared of what's going to happen. We're going to know we're going to win, but we're also very scared of what's going to happen because we really want to avoid this kind of breakneck race right at the end. Things could really go awry.

We offer them a deal. There's an incentive to come to the table. There's a more stable arrangement you can do. It's an Atoms for Peace arrangement. We're like, "look, we're going to respect you. We're not going to use superintelligence against you. You can do what you want. You're going to get your slice of the galaxy.

We're going to benefit-share with you. We're going to have some compute agreement where there's some ratio of compute that you're allowed to have, enforced with opposing AIs or whatever. We're just not going to do this volatile WMD arms race to the death.

It's a new world order that's US-led, democracy-led, but respects China and lets them do what they want.
Dwarkesh Patel
There's so much there. On the galaxies thing, there’s a funny anecdote. I kind of want to tell it. We were at an event. I'm respecting Chatham House rules here. I'm not revealing anything about it. Leopold was talking to somebody influential. Afterwards, that person told the group, "Leopold told me he's not going to spend any money on consumption until he's ready to buy galaxies."

The guy goes, "I honestly don't know if he meant galaxies like the brand of private plane Galaxy or physical galaxies." There was an actual debate. He went away to the restroom. There was an actual debate among influential people about whether he meant Galaxys. Others who knew you better were like, "no, he means galaxies."

**Extracted Belief:**

Early-stage superintelligence could be used to manipulate opposing forces, potentially even to deactivate enemy nuclear weapons.

**Context:**

Leopold Aschenbrenner is exploring the potential for a nation to gain an advantage in the early stages of superintelligence, even before a full industrial explosion.

**Justification:**

He suggests that superintelligence could be used to manipulate leaders, create powerful bioweapons, or even develop methods to deactivate enemy nuclear weapons. He acknowledges the potential for unforeseen outcomes and risks in this scenario.

--------

## Chunk 203

**Chunk:**

Dwarkesh Patel
Is this the inverse of the Eliezer…?
Leopold Aschenbrenner
Nuclear deterrence for data centers. This is Berlin in the late 1950s, early 1960s. Both Eisenhower and Kennedy multiple times made the threat of full-on nuclear war against the Soviets if they tried to encroach on West Berlin.

It's sort of insane. It's kind of insane that that went well. Basically, that's going to be the only option for the data centers. It's a terrible option. This whole scheme is terrible. Being in  neck and neck race at this point is terrible.

I have some uncertainty on how easy that decisive advantage will be. I'm pretty confident that if you have superintelligence, you have two years, you have the robots, you're able to get that 30-year lead. Then you're in this Gulf War 1 situation. You have your millions or billions of mosquito-sized drones that can just take it out. There's even a possibility you can get a decisive advantage earlier.

There are these stories about colonization in the 1500s where a few hundred Spaniards were able to topple the Aztec Empire, a couple of other empires as well. Each of these had a few million people. It was not a godlike technological advantage. It was some technological advantage. It was some amount of disease and cunning strategic play.

There's a possibility that even early on — when you haven't gone through the full industrial explosion yet — you have superintelligence, but you're able to manipulate the opposing generals, claiming you're allying with them. Then you have some crazy new bioweapons. Maybe there's even some way to pretty easily get a paradigm that deactivates enemy nukes. This stuff could get pretty wild.

Here's what we should do. I really don't want this volatile period. A deal with China would be nice. It's going to be really tough if you're in this unstable equilibrium. We want to get in a position where it is clear that the United States, a coalition of democratic allies, will win. It is clear to the United States, it is clear to China. That will require having locked down the secrets, having built the 100 gigawatt cluster in the United States, having done the natural gas and doing what's necessary.

When it is clear that the democratic coalition is well ahead, you go to China and offer them a deal. China will know we’re going to win. They're very scared of what's going to happen. We're going to know we're going to win, but we're also very scared of what's going to happen because we really want to avoid this kind of breakneck race right at the end. Things could really go awry.

We offer them a deal. There's an incentive to come to the table. There's a more stable arrangement you can do. It's an Atoms for Peace arrangement. We're like, "look, we're going to respect you. We're not going to use superintelligence against you. You can do what you want. You're going to get your slice of the galaxy.

We're going to benefit-share with you. We're going to have some compute agreement where there's some ratio of compute that you're allowed to have, enforced with opposing AIs or whatever. We're just not going to do this volatile WMD arms race to the death.

It's a new world order that's US-led, democracy-led, but respects China and lets them do what they want.
Dwarkesh Patel
There's so much there. On the galaxies thing, there’s a funny anecdote. I kind of want to tell it. We were at an event. I'm respecting Chatham House rules here. I'm not revealing anything about it. Leopold was talking to somebody influential. Afterwards, that person told the group, "Leopold told me he's not going to spend any money on consumption until he's ready to buy galaxies."

The guy goes, "I honestly don't know if he meant galaxies like the brand of private plane Galaxy or physical galaxies." There was an actual debate. He went away to the restroom. There was an actual debate among influential people about whether he meant Galaxys. Others who knew you better were like, "no, he means galaxies."

**Extracted Belief:**

A stable global order can be achieved through cooperation and shared benefits, even in the face of superintelligence.

**Context:**

Leopold Aschenbrenner is proposing a strategy for preventing a catastrophic AI arms race by fostering cooperation and shared benefits.

**Justification:**

He suggests that a 'new world order' can be established where the United States, as a leader of democratic allies, collaborates with China to ensure a stable and beneficial future. He envisions a scenario where both nations benefit from superintelligence, with compute resources allocated through agreements enforced by AI.

--------

## Chunk 204

**Chunk:**

Dwarkesh Patel
There's so much there. On the galaxies thing, there’s a funny anecdote. I kind of want to tell it. We were at an event. I'm respecting Chatham House rules here. I'm not revealing anything about it. Leopold was talking to somebody influential. Afterwards, that person told the group, "Leopold told me he's not going to spend any money on consumption until he's ready to buy galaxies."

The guy goes, "I honestly don't know if he meant galaxies like the brand of private plane Galaxy or physical galaxies." There was an actual debate. He went away to the restroom. There was an actual debate among influential people about whether he meant Galaxys. Others who knew you better were like, "no, he means galaxies."
Leopold Aschenbrenner
I meant the galaxies. There are two ways to buy the galaxies. At some point, post-superintelligence, there’s some crazy...
Dwarkesh Patel
I'm laughing my ass off, not even saying anything. Wee were having this debate. Leopold comes back. Someone says, "oh, Leopold, we're having this debate about whether you meant you want to buy the Galaxy, or you want to buy the other thing." Leopold assumes they must mean not the private plane Galaxy vs. the actual galaxy, but whether he wants to buy the property rights of the galaxy or actually just send out the probes right now.

**Extracted Belief:**

At some point after the emergence of superintelligence, it will be possible to purchase galaxies, either through the acquisition of property rights or by sending out probes to claim them.

**Context:**

Leopold Aschenbrenner clarifies that his statement about buying galaxies was literal, not a reference to a private plane brand. He then explains that there are two ways to achieve this goal: by acquiring property rights or by deploying probes.

**Justification:**

Aschenbrenner's explanation refers to a future where technological advancements, specifically related to superintelligence, will enable galaxy-scale exploration and resource acquisition.

--------

## Chunk 205

**Chunk:**

Dwarkesh Patel
I'm laughing my ass off, not even saying anything. Wee were having this debate. Leopold comes back. Someone says, "oh, Leopold, we're having this debate about whether you meant you want to buy the Galaxy, or you want to buy the other thing." Leopold assumes they must mean not the private plane Galaxy vs. the actual galaxy, but whether he wants to buy the property rights of the galaxy or actually just send out the probes right now.
Leopold Aschenbrenner
Exactly.
Dwarkesh Patel
Alright, back to China. There's a whole bunch of things I could ask about that plan and whether you’re going to get a credible promise to get some part of galaxies.

**Extracted Belief:**

The concept of 'buying' the galaxies refers to gaining control or ownership of the vast expanse of space and its resources.

**Context:**

In response to a misunderstanding about his statement about buying galaxies, Leopold clarifies that he was referring to gaining control over the galaxies, not just buying a private plane.

**Justification:**

Leopold clarifies that he was referring to obtaining property rights to galaxies, not buying a plane. This implies that he believes in the possibility of owning or controlling galaxies, which is a metaphysical belief about the nature of ownership and control in a cosmic context.

--------

## Chunk 206

**Chunk:**

Dwarkesh Patel
Alright, back to China. There's a whole bunch of things I could ask about that plan and whether you’re going to get a credible promise to get some part of galaxies.
Leopold Aschenbrenner
You’ll have AIs to help you enforce stuff.
Dwarkesh Patel
Sure, we’ll leave that aside. That’s a different rabbit hole. The thing I want to ask is...

**Extracted Belief:**

Artificial intelligence (AI) will play a role in enforcing agreements or regulations related to the ownership or control of galaxies.

**Context:**

In the context of discussing the potential for acquiring ownership of galaxies, Leopold Aschenbrenner suggests that AI will be involved in enforcing related agreements.

**Justification:**

Leopold Aschenbrenner states that 'You’ll have AIs to help you enforce stuff' in response to a question about securing a credible promise for acquiring a part of the galaxies.

--------

## Chunk 207

**Chunk:**

Dwarkesh Patel
Sure, we’ll leave that aside. That’s a different rabbit hole. The thing I want to ask is...
Leopold Aschenbrenner
The only way this is possible is if we lock it down. If we don't lock it down, we are in this fever struggle. Greatest peril mankind will have ever seen.
Dwarkesh Patel
During this period, they don’t really understand how this AI governance is going to work, whether they’re going to check, whether we’re going to adjugate the galaxies. The data centers can't be built underground. They have to be above ground. Taiwan is right off the coast of China. The US needs the chips from there.

Why isn’t China just going to invade? Worst case scenario for them is the US wins the superintelligence, which we’re on track to do anyway. Wouldn't this instigate them to either invade Taiwan or blow up the data center in Arizona or something like that?

**Extracted Belief:**

The only way to prevent a catastrophic struggle for control over artificial general intelligence (AGI) is to secure it.

**Context:**

Leopold Aschenbrenner is explaining his belief that securing AGI is crucial to preventing a global crisis.

**Justification:**

Aschenbrenner argues that without securing AGI, humanity will face a dangerous struggle for its control, which he describes as the 'greatest peril mankind will have ever seen'.

--------

## Chunk 208

**Chunk:**

Dwarkesh Patel
During this period, they don’t really understand how this AI governance is going to work, whether they’re going to check, whether we’re going to adjugate the galaxies. The data centers can't be built underground. They have to be above ground. Taiwan is right off the coast of China. The US needs the chips from there.

Why isn’t China just going to invade? Worst case scenario for them is the US wins the superintelligence, which we’re on track to do anyway. Wouldn't this instigate them to either invade Taiwan or blow up the data center in Arizona or something like that?
Leopold Aschenbrenner
You talked about the data center. You'd probably have to threaten nuclear retaliation to protect that. They might just blow it up. There are also ways they can do it without attribution.
Dwarkesh Patel
Stuxnet.

**Extracted Belief:**

To deter China from attacking US data centers, the US would have to threaten nuclear retaliation.

**Context:**

Discussing the potential for China to attack US data centers due to their strategic importance in the development of AI.

**Justification:**

Aschenbrenner believes that China might be willing to attack US data centers, even with the risk of nuclear retaliation.

--------

## Chunk 209

**Chunk:**

Dwarkesh Patel
During this period, they don’t really understand how this AI governance is going to work, whether they’re going to check, whether we’re going to adjugate the galaxies. The data centers can't be built underground. They have to be above ground. Taiwan is right off the coast of China. The US needs the chips from there.

Why isn’t China just going to invade? Worst case scenario for them is the US wins the superintelligence, which we’re on track to do anyway. Wouldn't this instigate them to either invade Taiwan or blow up the data center in Arizona or something like that?
Leopold Aschenbrenner
You talked about the data center. You'd probably have to threaten nuclear retaliation to protect that. They might just blow it up. There are also ways they can do it without attribution.
Dwarkesh Patel
Stuxnet.

**Extracted Belief:**

China could attack US data centers without leaving a trace of attribution.

**Context:**

Continuing the discussion on China's potential attack on US data centers.

**Justification:**

Aschenbrenner suggests that China possesses capabilities to launch covert attacks, making it difficult to identify the perpetrator.

--------

## Chunk 210

**Chunk:**

Dwarkesh Patel
Stuxnet.
Leopold Aschenbrenner
Stuxnet, yeah. We’ll talk about later, but we need to be working on a Stuxnet for the Chinese project. I talk about AGI by 2027 or whatever. On Taiwan, do you know about the terrible twenties?
Dwarkesh Patel
No.

**Extracted Belief:**

The United States needs to develop a cyberweapon similar to Stuxnet to target China's artificial intelligence development.

**Context:**

Leopold Aschenbrenner is advocating for the creation of a cyberweapon to disrupt China's AI progress, referencing Stuxnet as a successful example.

**Justification:**

Leopold Aschenbrenner's statement "We need to be working on a Stuxnet for the Chinese project" suggests that he believes in the effectiveness of Stuxnet and its applicability to Chinese AI.

--------

## Chunk 211

**Chunk:**

Dwarkesh Patel
Stuxnet.
Leopold Aschenbrenner
Stuxnet, yeah. We’ll talk about later, but we need to be working on a Stuxnet for the Chinese project. I talk about AGI by 2027 or whatever. On Taiwan, do you know about the terrible twenties?
Dwarkesh Patel
No.

**Extracted Belief:**

Artificial general intelligence (AGI) will be achieved by 2027.

**Context:**

Leopold Aschenbrenner mentions a timeframe for AGI development while proposing the Stuxnet countermeasure, implying a belief in its feasibility.

**Justification:**

His statement "I talk about AGI by 2027 or whatever" indicates a belief in the possibility of reaching AGI within a specific timeframe.

--------

## Chunk 212

**Chunk:**

Dwarkesh Patel
No.
Leopold Aschenbrenner
In Taiwan watcher circles, people often talk about the late 2020s as the maximum period of risk for Taiwan. Military modernization cycles and extreme fiscal tightening on the US military budget over the last decade or two have meant that we’re in a trough by the late twenties in terms of overall naval capacity.

That’s when China is saying they want to be ready. It’s already  kind of a parallel timeline there. Yeah, it looks appealing to invade Taiwan. Maybe not because of the remote cut off chips, which deactivates the machines. But imagine if during the Cold War, all of the world’s uranium deposits had been in Berlin. Berlin already almost caused a nuclear war multiple times. God help us all.
Dwarkesh Patel
Leslie Groves actually had a plan after the war that America would go around the world getting the rights to every single uranium deposit because they didn’t realize how much uranium there was in the world. They thought this was feasible. They didn’t realize, of course, that there were huge deposits in the Soviet Union itself.

**Extracted Belief:**

The late 2020s will be a period of maximum risk for Taiwan.

**Context:**

Leopold Aschenbrenner is discussing the potential for China to invade Taiwan in the late 2020s.

**Justification:**

Aschenbrenner mentions that "in Taiwan watcher circles, people often talk about the late 2020s as the maximum period of risk for Taiwan."

--------

## Chunk 213

**Chunk:**

Dwarkesh Patel
No.
Leopold Aschenbrenner
In Taiwan watcher circles, people often talk about the late 2020s as the maximum period of risk for Taiwan. Military modernization cycles and extreme fiscal tightening on the US military budget over the last decade or two have meant that we’re in a trough by the late twenties in terms of overall naval capacity.

That’s when China is saying they want to be ready. It’s already  kind of a parallel timeline there. Yeah, it looks appealing to invade Taiwan. Maybe not because of the remote cut off chips, which deactivates the machines. But imagine if during the Cold War, all of the world’s uranium deposits had been in Berlin. Berlin already almost caused a nuclear war multiple times. God help us all.
Dwarkesh Patel
Leslie Groves actually had a plan after the war that America would go around the world getting the rights to every single uranium deposit because they didn’t realize how much uranium there was in the world. They thought this was feasible. They didn’t realize, of course, that there were huge deposits in the Soviet Union itself.

**Extracted Belief:**

China is planning to be ready to invade Taiwan by the late 2020s.

**Context:**

Aschenbrenner discusses China's military preparedness in relation to Taiwan.

**Justification:**

Aschenbrenner states that "That’s when China is saying they want to be ready."

--------

## Chunk 214

**Chunk:**

Dwarkesh Patel
No.
Leopold Aschenbrenner
In Taiwan watcher circles, people often talk about the late 2020s as the maximum period of risk for Taiwan. Military modernization cycles and extreme fiscal tightening on the US military budget over the last decade or two have meant that we’re in a trough by the late twenties in terms of overall naval capacity.

That’s when China is saying they want to be ready. It’s already  kind of a parallel timeline there. Yeah, it looks appealing to invade Taiwan. Maybe not because of the remote cut off chips, which deactivates the machines. But imagine if during the Cold War, all of the world’s uranium deposits had been in Berlin. Berlin already almost caused a nuclear war multiple times. God help us all.
Dwarkesh Patel
Leslie Groves actually had a plan after the war that America would go around the world getting the rights to every single uranium deposit because they didn’t realize how much uranium there was in the world. They thought this was feasible. They didn’t realize, of course, that there were huge deposits in the Soviet Union itself.

**Extracted Belief:**

The US military's naval capacity will be at a low point in the late 2020s.

**Context:**

Aschenbrenner explains the potential vulnerability of the US in the late 2020s.

**Justification:**

Aschenbrenner says that "Military modernization cycles and extreme fiscal tightening on the US military budget over the last decade or two have meant that we’re in a trough by the late twenties in terms of overall naval capacity."

--------

## Chunk 215

**Chunk:**

Dwarkesh Patel
No.
Leopold Aschenbrenner
In Taiwan watcher circles, people often talk about the late 2020s as the maximum period of risk for Taiwan. Military modernization cycles and extreme fiscal tightening on the US military budget over the last decade or two have meant that we’re in a trough by the late twenties in terms of overall naval capacity.

That’s when China is saying they want to be ready. It’s already  kind of a parallel timeline there. Yeah, it looks appealing to invade Taiwan. Maybe not because of the remote cut off chips, which deactivates the machines. But imagine if during the Cold War, all of the world’s uranium deposits had been in Berlin. Berlin already almost caused a nuclear war multiple times. God help us all.
Dwarkesh Patel
Leslie Groves actually had a plan after the war that America would go around the world getting the rights to every single uranium deposit because they didn’t realize how much uranium there was in the world. They thought this was feasible. They didn’t realize, of course, that there were huge deposits in the Soviet Union itself.

**Extracted Belief:**

The control of vital resources, like uranium during the Cold War, can lead to significant geopolitical tensions and even nuclear war.

**Context:**

Aschenbrenner uses the example of Berlin during the Cold War to illustrate the potential for conflict over resources.

**Justification:**

Aschenbrenner notes that "Imagine if during the Cold War, all of the world’s uranium deposits had been in Berlin. Berlin already almost caused a nuclear war multiple times. God help us all."

--------

## Chunk 216

**Chunk:**

Dwarkesh Patel
Leslie Groves actually had a plan after the war that America would go around the world getting the rights to every single uranium deposit because they didn’t realize how much uranium there was in the world. They thought this was feasible. They didn’t realize, of course, that there were huge deposits in the Soviet Union itself.
Leopold Aschenbrenner
East Germany, too. A lot of East German workers got screwed and got cancer.
Dwarkesh Patel
The framing we’ve been assuming— I’m not sure I buy it yet—is that the United States has this leverage. This is our data center. China is the competitor right now. Obviously, that’s not the way things are progressing. Private companies control these AIs. They’re deploying them. It’s a market-based thing. Why will it be the case that the United States has this leverage or is doing this thing versus China doing this thing?

**Extracted Belief:**

Many East German workers were exploited and developed cancer due to their work.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's mention of uranium deposits in the Soviet Union by pointing out that East Germany also had significant deposits, which led to exploitation of workers and negative health consequences.

**Justification:**

Leopold Aschenbrenner states that 'A lot of East German workers got screwed and got cancer' without elaborating further on the specific causes or details of their exploitation or the nature of the cancer.

--------

## Chunk 217

**Chunk:**

Dwarkesh Patel
The framing we’ve been assuming— I’m not sure I buy it yet—is that the United States has this leverage. This is our data center. China is the competitor right now. Obviously, that’s not the way things are progressing. Private companies control these AIs. They’re deploying them. It’s a market-based thing. Why will it be the case that the United States has this leverage or is doing this thing versus China doing this thing?
Leopold Aschenbrenner
There are descriptive and prescriptive claims, or normative and positive claims. The main thing I’m trying to say is, at these SF parties, people talk about AGI and always focus on private AI labs. I want to challenge that assumption.

It seems likely to me, for reasons we’ve discussed, that the national security state will get involved. There are many ways this could look: nationalization, a public-private partnership, a defense contractor-like relationship, or a government project that absorbs all the people. There’s a spectrum, but people vastly underrate the chances of this looking like a government project.

When we have literal superintelligence on our cluster — with a billion superintelligent scientists who can hack everything and Stuxnet the Chinese data centers, and build robo armies — you really think it’ll be a private company ? The government would be like, "oh, my God, what is going on?" 

(01:31:23) – State-led vs. private-led AI
Dwarkesh Patel
Suppose there’s no China. Suppose there are countries like Iran and North Korea that theoretically could achieve superintelligence, but they’re not on our heels. In that world, are you advocating for a national project or do you prefer the private path forward?

**Extracted Belief:**

It is likely that the national security state will get involved in the development of artificial general intelligence (AGI).

**Context:**

Leopold Aschenbrenner is expressing his belief that the government will be involved in the development of AGI, challenging the common assumption that it will be solely developed by private companies.

**Justification:**

Aschenbrenner bases his belief on the arguments previously discussed in the conversation, suggesting that the security implications of AGI would necessitate government involvement.

--------

## Chunk 218

**Chunk:**

Dwarkesh Patel
The framing we’ve been assuming— I’m not sure I buy it yet—is that the United States has this leverage. This is our data center. China is the competitor right now. Obviously, that’s not the way things are progressing. Private companies control these AIs. They’re deploying them. It’s a market-based thing. Why will it be the case that the United States has this leverage or is doing this thing versus China doing this thing?
Leopold Aschenbrenner
There are descriptive and prescriptive claims, or normative and positive claims. The main thing I’m trying to say is, at these SF parties, people talk about AGI and always focus on private AI labs. I want to challenge that assumption.

It seems likely to me, for reasons we’ve discussed, that the national security state will get involved. There are many ways this could look: nationalization, a public-private partnership, a defense contractor-like relationship, or a government project that absorbs all the people. There’s a spectrum, but people vastly underrate the chances of this looking like a government project.

When we have literal superintelligence on our cluster — with a billion superintelligent scientists who can hack everything and Stuxnet the Chinese data centers, and build robo armies — you really think it’ll be a private company ? The government would be like, "oh, my God, what is going on?" 

(01:31:23) – State-led vs. private-led AI
Dwarkesh Patel
Suppose there’s no China. Suppose there are countries like Iran and North Korea that theoretically could achieve superintelligence, but they’re not on our heels. In that world, are you advocating for a national project or do you prefer the private path forward?

**Extracted Belief:**

People vastly underrate the chances of AGI development looking like a government project.

**Context:**

Aschenbrenner expresses his belief that the likelihood of government involvement in AGI development is underestimated.

**Justification:**

Aschenbrenner's belief is based on his observation that discussions about AGI often focus on private companies, neglecting the potential role of the government.

--------

## Chunk 219

**Chunk:**

Dwarkesh Patel
Suppose there’s no China. Suppose there are countries like Iran and North Korea that theoretically could achieve superintelligence, but they’re not on our heels. In that world, are you advocating for a national project or do you prefer the private path forward?
Leopold Aschenbrenner
Two responses to this. One is, you still have Russia and other countries.

You need Russia-proof security. You can’t let Russia steal all your stuff. Their clusters may not be as big, but they can still make crazy bioweapons and mosquito-sized drone swarms. 

The security component is a large part of the project because there’s no other way to prevent this from instantly proliferating to everyone. You still have to deal with Russia, Iran, and North Korea. Saudi and Iran will try to get it to screw each other. Pakistan and India will try to get it to screw each other. There’s enormous destabilization.

Still, I agree with you. If AGI had emerged in 2005, during unparalleled American hegemony, there would have been more scope for less government involvement. But as we discussed, that would have been a unique moment in history. In almost all other moments in history, there would have been a great power competitor.
Dwarkesh Patel
Let’s get into this debate. My position is this. If you look at the people who were involved in the Manhattan Project, many of them regretted their participation. We can infer from this that we should start with a cautious approach to the nationalized ASI project.

**Extracted Belief:**

To prevent the proliferation of superintelligence, a Russia-proof security system is necessary to protect the technology from being stolen or exploited.

**Context:**

Leopold Aschenbrenner is arguing that even in a world without China as a direct competitor, the potential for other countries like Russia, Iran, and North Korea to develop superintelligence requires a robust security system to protect the technology.

**Justification:**

Leopold Aschenbrenner cites the potential for countries like Russia to develop bioweapons and drone swarms, emphasizing the need for security measures to prevent proliferation.

--------

## Chunk 220

**Chunk:**

Dwarkesh Patel
Suppose there’s no China. Suppose there are countries like Iran and North Korea that theoretically could achieve superintelligence, but they’re not on our heels. In that world, are you advocating for a national project or do you prefer the private path forward?
Leopold Aschenbrenner
Two responses to this. One is, you still have Russia and other countries.

You need Russia-proof security. You can’t let Russia steal all your stuff. Their clusters may not be as big, but they can still make crazy bioweapons and mosquito-sized drone swarms. 

The security component is a large part of the project because there’s no other way to prevent this from instantly proliferating to everyone. You still have to deal with Russia, Iran, and North Korea. Saudi and Iran will try to get it to screw each other. Pakistan and India will try to get it to screw each other. There’s enormous destabilization.

Still, I agree with you. If AGI had emerged in 2005, during unparalleled American hegemony, there would have been more scope for less government involvement. But as we discussed, that would have been a unique moment in history. In almost all other moments in history, there would have been a great power competitor.
Dwarkesh Patel
Let’s get into this debate. My position is this. If you look at the people who were involved in the Manhattan Project, many of them regretted their participation. We can infer from this that we should start with a cautious approach to the nationalized ASI project.

**Extracted Belief:**

The development of superintelligence is likely to result in significant global destabilization due to the potential for misuse and competition between nations.

**Context:**

Leopold Aschenbrenner is outlining the risks associated with the development of superintelligence, suggesting that even without China as a competitor, other countries like Saudi Arabia, Iran, Pakistan, and India will seek to exploit the technology for their own gain, leading to increased tensions and instability.

**Justification:**

He points to the potential for countries to use superintelligence to manipulate each other, illustrating the risk of destabilization.

--------

## Chunk 221

**Chunk:**

Dwarkesh Patel
Suppose there’s no China. Suppose there are countries like Iran and North Korea that theoretically could achieve superintelligence, but they’re not on our heels. In that world, are you advocating for a national project or do you prefer the private path forward?
Leopold Aschenbrenner
Two responses to this. One is, you still have Russia and other countries.

You need Russia-proof security. You can’t let Russia steal all your stuff. Their clusters may not be as big, but they can still make crazy bioweapons and mosquito-sized drone swarms. 

The security component is a large part of the project because there’s no other way to prevent this from instantly proliferating to everyone. You still have to deal with Russia, Iran, and North Korea. Saudi and Iran will try to get it to screw each other. Pakistan and India will try to get it to screw each other. There’s enormous destabilization.

Still, I agree with you. If AGI had emerged in 2005, during unparalleled American hegemony, there would have been more scope for less government involvement. But as we discussed, that would have been a unique moment in history. In almost all other moments in history, there would have been a great power competitor.
Dwarkesh Patel
Let’s get into this debate. My position is this. If you look at the people who were involved in the Manhattan Project, many of them regretted their participation. We can infer from this that we should start with a cautious approach to the nationalized ASI project.

**Extracted Belief:**

A more limited role for government in the development of artificial general intelligence (AGI) might have been possible during a period of American hegemony, but this was a unique historical moment.

**Context:**

Leopold Aschenbrenner is arguing that the current geopolitical landscape, with its power competition, makes a nationalized approach to AGI more likely and necessary than it might have been in the past.

**Justification:**

He suggests that the unique historical circumstances of 2005, when America had unparalleled global power, might have allowed for less government involvement, but this is not the case in the current context.

--------

## Chunk 222

**Chunk:**

Dwarkesh Patel
Let’s get into this debate. My position is this. If you look at the people who were involved in the Manhattan Project, many of them regretted their participation. We can infer from this that we should start with a cautious approach to the nationalized ASI project.
Leopold Aschenbrenner
Did they regret their participation because of the project or because of the technology itself? People will regret it, but it's about the nature of the technology, not the project.
Dwarkesh Patel
They probably had a sense that different decisions would have been made if it wasn’t a concerted effort that everyone agreed to participate in. If it wasn’t in the context of a race to beat Germany and Japan, you might not develop it. That’s the technology part.

**Extracted Belief:**

The regrets of those involved in the Manhattan Project were due to the nature of the technology (nuclear weapons) itself, not the specific project or the way it was organized.

**Context:**

Responding to Dwarkesh Patel's suggestion that people involved in the Manhattan Project regretted their participation due to the project's nationalized nature, Leopold Aschenbrenner argues that the regret was due to the technology itself.

**Justification:**

Leopold Aschenbrenner argues that the regret stemmed from the nature of the technology, not the project itself. He states that people will regret the technology regardless of the project. He does not offer specific evidence or references to support this claim.

--------

## Chunk 223

**Chunk:**

Dwarkesh Patel
They probably had a sense that different decisions would have been made if it wasn’t a concerted effort that everyone agreed to participate in. If it wasn’t in the context of a race to beat Germany and Japan, you might not develop it. That’s the technology part.
Leopold Aschenbrenner
It’s still going to be a weapon because of the destructive potential, the military potential. It’s not because of the project. It’s because of the technology. That will unfold regardless.

Imagine you go through the 20th century in a decade—
Dwarkesh Patel
Let’s run that example. Suppose the 20th century was run through in one decade.

Do you think the technologies that happened during the 20th century shouldn’t have been privatized? Should it have been a more concerted, government-led project?

**Extracted Belief:**

The destructive potential of advanced artificial intelligence (AGI) will be realized regardless of whether it is developed through a government project or a private initiative.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's suggestion that the Manhattan Project's example demonstrates the dangers of nationalized AI development. He argues that the inherent destructive potential of the technology itself, not the project, is the primary concern.

**Justification:**

Aschenbrenner believes that AGI's military and destructive capabilities are inherent to the technology, and will manifest regardless of the development context.

--------

## Chunk 224

**Chunk:**

Dwarkesh Patel
Let’s run that example. Suppose the 20th century was run through in one decade.

Do you think the technologies that happened during the 20th century shouldn’t have been privatized? Should it have been a more concerted, government-led project?
Leopold Aschenbrenner
There’s a history of dual-use technologies. AI will be dual-use in the same way. There will be lots of civilian uses of it. Like with nuclear energy, the government project developed the military angle of it and then worked with private companies. There was a flourishing of nuclear energy until the environmentalists stopped it.

Planes, like Boeing. Actually, the Manhattan Project wasn’t the biggest defense R&D project during World War II. It was the B-29 bomber because they needed a bomber with a long enough range to reach Japan to destroy their cities. Boeing made the B-47, and the B-52 plane the US military uses today. They used that technology later on to build the 707.
Dwarkesh Patel
What does "later on" mean in this context? I get what it means after a war to privatize. But if the government has ASI... 

Let me back up and explain my concern. You have this institution in our society with a monopoly on violence. We’re going to give it access to ASI that’s not broadly deployed. This maybe sounds silly, but we’re going to go through higher levels of intelligence. Private companies will be required by regulation to increase their security. They’ll still be private companies.

They’ll deploy this and release AGI. Now McDonald’s, JP Morgan, and some random startup will be more effective organizations because they have AGI workers. It’ll be like the Industrial Revolution, where the benefits were widely diffused.

Backing up, what is it we’re trying to do? Why do we want to win against China? We want to win because we don’t want a top-down authoritarian system to win. If the way to beat that is for the most important technology for humanity to be controlled by a top-down government, what’s the point?

Let’s run our cards with privatization. That’s how we get to the classic liberal, market-based system we want for the ASIs.

**Extracted Belief:**

There is a history of technologies having both civilian and military applications, and AI will be no different.

**Context:**

Leopold Aschenbrenner is arguing that AI, like nuclear energy and aircraft technology, will have both civilian and military applications.

**Justification:**

He cites the historical examples of nuclear energy and aircraft technology as evidence for this claim.

--------

## Chunk 225

**Chunk:**

Dwarkesh Patel
Let’s run that example. Suppose the 20th century was run through in one decade.

Do you think the technologies that happened during the 20th century shouldn’t have been privatized? Should it have been a more concerted, government-led project?
Leopold Aschenbrenner
There’s a history of dual-use technologies. AI will be dual-use in the same way. There will be lots of civilian uses of it. Like with nuclear energy, the government project developed the military angle of it and then worked with private companies. There was a flourishing of nuclear energy until the environmentalists stopped it.

Planes, like Boeing. Actually, the Manhattan Project wasn’t the biggest defense R&D project during World War II. It was the B-29 bomber because they needed a bomber with a long enough range to reach Japan to destroy their cities. Boeing made the B-47, and the B-52 plane the US military uses today. They used that technology later on to build the 707.
Dwarkesh Patel
What does "later on" mean in this context? I get what it means after a war to privatize. But if the government has ASI... 

Let me back up and explain my concern. You have this institution in our society with a monopoly on violence. We’re going to give it access to ASI that’s not broadly deployed. This maybe sounds silly, but we’re going to go through higher levels of intelligence. Private companies will be required by regulation to increase their security. They’ll still be private companies.

They’ll deploy this and release AGI. Now McDonald’s, JP Morgan, and some random startup will be more effective organizations because they have AGI workers. It’ll be like the Industrial Revolution, where the benefits were widely diffused.

Backing up, what is it we’re trying to do? Why do we want to win against China? We want to win because we don’t want a top-down authoritarian system to win. If the way to beat that is for the most important technology for humanity to be controlled by a top-down government, what’s the point?

Let’s run our cards with privatization. That’s how we get to the classic liberal, market-based system we want for the ASIs.

**Extracted Belief:**

The government, through projects like the Manhattan Project, has historically been involved in developing military applications of technology.

**Context:**

He uses the example of the Manhattan Project to show that governments have historically been involved in developing military applications of technology, even those with civilian applications.

**Justification:**

He uses the example of the Manhattan Project to show that governments have historically been involved in developing military applications of technology, even those with civilian applications.

--------

## Chunk 226

**Chunk:**

Dwarkesh Patel
Let’s run that example. Suppose the 20th century was run through in one decade.

Do you think the technologies that happened during the 20th century shouldn’t have been privatized? Should it have been a more concerted, government-led project?
Leopold Aschenbrenner
There’s a history of dual-use technologies. AI will be dual-use in the same way. There will be lots of civilian uses of it. Like with nuclear energy, the government project developed the military angle of it and then worked with private companies. There was a flourishing of nuclear energy until the environmentalists stopped it.

Planes, like Boeing. Actually, the Manhattan Project wasn’t the biggest defense R&D project during World War II. It was the B-29 bomber because they needed a bomber with a long enough range to reach Japan to destroy their cities. Boeing made the B-47, and the B-52 plane the US military uses today. They used that technology later on to build the 707.
Dwarkesh Patel
What does "later on" mean in this context? I get what it means after a war to privatize. But if the government has ASI... 

Let me back up and explain my concern. You have this institution in our society with a monopoly on violence. We’re going to give it access to ASI that’s not broadly deployed. This maybe sounds silly, but we’re going to go through higher levels of intelligence. Private companies will be required by regulation to increase their security. They’ll still be private companies.

They’ll deploy this and release AGI. Now McDonald’s, JP Morgan, and some random startup will be more effective organizations because they have AGI workers. It’ll be like the Industrial Revolution, where the benefits were widely diffused.

Backing up, what is it we’re trying to do? Why do we want to win against China? We want to win because we don’t want a top-down authoritarian system to win. If the way to beat that is for the most important technology for humanity to be controlled by a top-down government, what’s the point?

Let’s run our cards with privatization. That’s how we get to the classic liberal, market-based system we want for the ASIs.

**Extracted Belief:**

Private companies can and do effectively utilize military technology for civilian purposes.

**Context:**

Aschenbrenner uses Boeing's development of the 707 as an example of how technology initially developed for military purposes can be adapted for civilian use.

**Justification:**

He cites Boeing's development of the 707, which was based on technology initially developed for military planes, as evidence for this belief.

--------

## Chunk 227

**Chunk:**

Dwarkesh Patel
What does "later on" mean in this context? I get what it means after a war to privatize. But if the government has ASI... 

Let me back up and explain my concern. You have this institution in our society with a monopoly on violence. We’re going to give it access to ASI that’s not broadly deployed. This maybe sounds silly, but we’re going to go through higher levels of intelligence. Private companies will be required by regulation to increase their security. They’ll still be private companies.

They’ll deploy this and release AGI. Now McDonald’s, JP Morgan, and some random startup will be more effective organizations because they have AGI workers. It’ll be like the Industrial Revolution, where the benefits were widely diffused.

Backing up, what is it we’re trying to do? Why do we want to win against China? We want to win because we don’t want a top-down authoritarian system to win. If the way to beat that is for the most important technology for humanity to be controlled by a top-down government, what’s the point?

Let’s run our cards with privatization. That’s how we get to the classic liberal, market-based system we want for the ASIs.
Leopold Aschenbrenner
All right, there’s a lot to talk about here. I’ll start by looking at what the private world would look like. This is part of why there's no alternative. Then let’s look at what the government project looks like, what checks and balances look like, and so on.

Let’s start with the private world. A lot of people talk about open source. There’s a misconception that AGI development will be a beautiful, decentralized thing, a giddy community of coders collaborating. That’s not how it’s going to look. It’s a $100 billion or trillion-dollar cluster. Not many people will have it.

Right now, open source is good because people use the stuff that’s published. They use the published algorithms, or, like Mistral, they leave DeepMind, take all the secrets, and replicate it.

That’s not going to continue. People also say stuff like, “10^26 flops will be in my phone.” No, it won’t. Moore’s Law is really slow. AI chips are getting better but the $100 billion computer won’t cost $1,000 within your lifetime. So it’s going to be like two or three big players in the private world.

You talk about the enormous power that superintelligence and the government will have. It’s pretty plausible that in the alternative world one AI company will have that power. Say OpenAI has a six-month lead. You’re talking about the most powerful weapon ever. You’re making a radical bet on a private company CEO as the benevolent dictator.
Dwarkesh Patel
Not necessarily. Like any other thing that’s privatized, we don’t count on them being benevolent. Think of someone who manufactures industrial fertilizer. This person with this factory, if they went back to an ancient civilization, they could blow up Rome. They could probably blow up Washington, DC.

**Extracted Belief:**

The development of Artificial General Intelligence (AGI) will not be a decentralized, open-source collaborative effort.

**Context:**

Leopold Aschenbrenner expresses his belief about the future of AGI development, contrasting it with the common notion of open-source collaboration.

**Justification:**

He argues that the high cost and complexity of developing AGI will lead to a concentrated, limited number of players in the market. He compares it to a "$100 billion or trillion-dollar cluster" where "not many people will have it." This suggests he is drawing from his experience and knowledge of the technology and its potential.

--------

## Chunk 228

**Chunk:**

Dwarkesh Patel
What does "later on" mean in this context? I get what it means after a war to privatize. But if the government has ASI... 

Let me back up and explain my concern. You have this institution in our society with a monopoly on violence. We’re going to give it access to ASI that’s not broadly deployed. This maybe sounds silly, but we’re going to go through higher levels of intelligence. Private companies will be required by regulation to increase their security. They’ll still be private companies.

They’ll deploy this and release AGI. Now McDonald’s, JP Morgan, and some random startup will be more effective organizations because they have AGI workers. It’ll be like the Industrial Revolution, where the benefits were widely diffused.

Backing up, what is it we’re trying to do? Why do we want to win against China? We want to win because we don’t want a top-down authoritarian system to win. If the way to beat that is for the most important technology for humanity to be controlled by a top-down government, what’s the point?

Let’s run our cards with privatization. That’s how we get to the classic liberal, market-based system we want for the ASIs.
Leopold Aschenbrenner
All right, there’s a lot to talk about here. I’ll start by looking at what the private world would look like. This is part of why there's no alternative. Then let’s look at what the government project looks like, what checks and balances look like, and so on.

Let’s start with the private world. A lot of people talk about open source. There’s a misconception that AGI development will be a beautiful, decentralized thing, a giddy community of coders collaborating. That’s not how it’s going to look. It’s a $100 billion or trillion-dollar cluster. Not many people will have it.

Right now, open source is good because people use the stuff that’s published. They use the published algorithms, or, like Mistral, they leave DeepMind, take all the secrets, and replicate it.

That’s not going to continue. People also say stuff like, “10^26 flops will be in my phone.” No, it won’t. Moore’s Law is really slow. AI chips are getting better but the $100 billion computer won’t cost $1,000 within your lifetime. So it’s going to be like two or three big players in the private world.

You talk about the enormous power that superintelligence and the government will have. It’s pretty plausible that in the alternative world one AI company will have that power. Say OpenAI has a six-month lead. You’re talking about the most powerful weapon ever. You’re making a radical bet on a private company CEO as the benevolent dictator.
Dwarkesh Patel
Not necessarily. Like any other thing that’s privatized, we don’t count on them being benevolent. Think of someone who manufactures industrial fertilizer. This person with this factory, if they went back to an ancient civilization, they could blow up Rome. They could probably blow up Washington, DC.

**Extracted Belief:**

The widespread availability of powerful AI chips is unlikely to happen within our lifetime.

**Context:**

He disagrees with the expectation that high-performance AI computing power will become widely accessible due to the limitations of Moore's Law.

**Justification:**

He cites the slow pace of Moore's Law and the continued high cost of advanced AI chips as reasons why "the $100 billion computer won't cost $1,000 within your lifetime."  This indicates that he is referencing scientific knowledge and data about the advancement of computing technology.

--------

## Chunk 229

**Chunk:**

Dwarkesh Patel
What does "later on" mean in this context? I get what it means after a war to privatize. But if the government has ASI... 

Let me back up and explain my concern. You have this institution in our society with a monopoly on violence. We’re going to give it access to ASI that’s not broadly deployed. This maybe sounds silly, but we’re going to go through higher levels of intelligence. Private companies will be required by regulation to increase their security. They’ll still be private companies.

They’ll deploy this and release AGI. Now McDonald’s, JP Morgan, and some random startup will be more effective organizations because they have AGI workers. It’ll be like the Industrial Revolution, where the benefits were widely diffused.

Backing up, what is it we’re trying to do? Why do we want to win against China? We want to win because we don’t want a top-down authoritarian system to win. If the way to beat that is for the most important technology for humanity to be controlled by a top-down government, what’s the point?

Let’s run our cards with privatization. That’s how we get to the classic liberal, market-based system we want for the ASIs.
Leopold Aschenbrenner
All right, there’s a lot to talk about here. I’ll start by looking at what the private world would look like. This is part of why there's no alternative. Then let’s look at what the government project looks like, what checks and balances look like, and so on.

Let’s start with the private world. A lot of people talk about open source. There’s a misconception that AGI development will be a beautiful, decentralized thing, a giddy community of coders collaborating. That’s not how it’s going to look. It’s a $100 billion or trillion-dollar cluster. Not many people will have it.

Right now, open source is good because people use the stuff that’s published. They use the published algorithms, or, like Mistral, they leave DeepMind, take all the secrets, and replicate it.

That’s not going to continue. People also say stuff like, “10^26 flops will be in my phone.” No, it won’t. Moore’s Law is really slow. AI chips are getting better but the $100 billion computer won’t cost $1,000 within your lifetime. So it’s going to be like two or three big players in the private world.

You talk about the enormous power that superintelligence and the government will have. It’s pretty plausible that in the alternative world one AI company will have that power. Say OpenAI has a six-month lead. You’re talking about the most powerful weapon ever. You’re making a radical bet on a private company CEO as the benevolent dictator.
Dwarkesh Patel
Not necessarily. Like any other thing that’s privatized, we don’t count on them being benevolent. Think of someone who manufactures industrial fertilizer. This person with this factory, if they went back to an ancient civilization, they could blow up Rome. They could probably blow up Washington, DC.

**Extracted Belief:**

A single private company could potentially gain significant power and influence through controlling advanced AI technology.

**Context:**

Leopold Aschenbrenner discusses the potential for one company, like OpenAI, to gain a significant advantage in AI development, potentially leading to a monopoly.

**Justification:**

He suggests that a company with a six-month lead in AGI development would possess "the most powerful weapon ever" and act as a "benevolent dictator."  He concludes by stating that "it's pretty plausible that in the alternative world one AI company will have that power." This implies a logical progression from the speed of technological advancement to the concentration of power.

--------

## Chunk 230

**Chunk:**

Dwarkesh Patel
Not necessarily. Like any other thing that’s privatized, we don’t count on them being benevolent. Think of someone who manufactures industrial fertilizer. This person with this factory, if they went back to an ancient civilization, they could blow up Rome. They could probably blow up Washington, DC.
Leopold Aschenbrenner
Indeed.
Dwarkesh Patel
In your series, you talk about Tyler Cowen’s phrase of “muddling through.” Even with privatization, people underrate that there are a lot of private actors who control vital resources like the water supply.

We can count on cooperation and market-based incentives to maintain a balance of power. Sure, things are proceeding really fast. We have a lot of historical evidence that this works best.

**Extracted Belief:**

Cooperation and market-based incentives are effective in maintaining a balance of power.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's argument about the dangers of private companies controlling vital resources like water supply.

**Justification:**

He references the phrase 'muddling through,' which implies that historical evidence suggests such cooperation and market mechanisms have proven successful in the past.

--------

## Chunk 231

**Chunk:**

Dwarkesh Patel
In your series, you talk about Tyler Cowen’s phrase of “muddling through.” Even with privatization, people underrate that there are a lot of private actors who control vital resources like the water supply.

We can count on cooperation and market-based incentives to maintain a balance of power. Sure, things are proceeding really fast. We have a lot of historical evidence that this works best.
Leopold Aschenbrenner
What do we do with nukes, right? We don't keep nukes in check by beefing up the Second Amendment so each state has its own nuclear arsenal. Dario and Sam don’t have their own little arsenal.

No, it’s institutions, constitutions, laws, and courts. I’m not sure this balance of power analogy holds. The government having the biggest guns was an enormous civilizational achievement, like Landfrieden in the Holy Roman Empire. If someone from the neighboring town committed a crime, you didn’t start a battle between the towns. You took it to a court of the Holy Roman Empire. They decided it. It’s a big achievement.

The key differences with the analogy about the industrial fertilizer are speed and offense-defense balance issues. It’s like compressing the 20th century into a few years. That is incredibly scary because of the rapid advancement in destructive technology and military advancements.

You'd go from bayonets and horses to tank armies and fighter jets in a couple of years. In just a few more years you’d have nukes, ICBMs, and stealth. That speed creates an incredibly volatile and dangerous period. We have to make it through that, which will be incredibly challenging.

That’s where a government project is necessary. If we can make it through that, the situation stabilizes. We don’t face this imminent national security threat. Yes, there were WMDs that developed, but we’ve managed to create a stable offense-defense balance.

Bioweapons are a huge issue initially. An attacker can create 1000 different synthetic viruses and spread them. It’s hard to defend against each. Maybe at some point, you figure out a universal defense against every possible virus, then you’re in a stable situation again on the offense-defense balance. Or like with planes,  you restrict certain capabilities that the private sector isn’t allowed to have, then you can let the civilian uses run free.
Dwarkesh Patel
I’m skeptical of this.

**Extracted Belief:**

The government having the biggest guns was an enormous civilizational achievement, analogous to the Landfrieden in the Holy Roman Empire.

**Context:**

Leopold Aschenbrenner refutes the analogy of private actors controlling vital resources like water supply to the potential power of AI companies with superintelligence, arguing that the historical example of the Holy Roman Empire successfully managing power dynamics with a centralized authority is more relevant to the current situation.

**Justification:**

Leopold Aschenbrenner cites the historical example of the Landfrieden, a period of peace and stability in the Holy Roman Empire, as evidence that a strong centralized government is crucial for managing power dynamics and preventing conflict.

--------

## Chunk 232

**Chunk:**

Dwarkesh Patel
In your series, you talk about Tyler Cowen’s phrase of “muddling through.” Even with privatization, people underrate that there are a lot of private actors who control vital resources like the water supply.

We can count on cooperation and market-based incentives to maintain a balance of power. Sure, things are proceeding really fast. We have a lot of historical evidence that this works best.
Leopold Aschenbrenner
What do we do with nukes, right? We don't keep nukes in check by beefing up the Second Amendment so each state has its own nuclear arsenal. Dario and Sam don’t have their own little arsenal.

No, it’s institutions, constitutions, laws, and courts. I’m not sure this balance of power analogy holds. The government having the biggest guns was an enormous civilizational achievement, like Landfrieden in the Holy Roman Empire. If someone from the neighboring town committed a crime, you didn’t start a battle between the towns. You took it to a court of the Holy Roman Empire. They decided it. It’s a big achievement.

The key differences with the analogy about the industrial fertilizer are speed and offense-defense balance issues. It’s like compressing the 20th century into a few years. That is incredibly scary because of the rapid advancement in destructive technology and military advancements.

You'd go from bayonets and horses to tank armies and fighter jets in a couple of years. In just a few more years you’d have nukes, ICBMs, and stealth. That speed creates an incredibly volatile and dangerous period. We have to make it through that, which will be incredibly challenging.

That’s where a government project is necessary. If we can make it through that, the situation stabilizes. We don’t face this imminent national security threat. Yes, there were WMDs that developed, but we’ve managed to create a stable offense-defense balance.

Bioweapons are a huge issue initially. An attacker can create 1000 different synthetic viruses and spread them. It’s hard to defend against each. Maybe at some point, you figure out a universal defense against every possible virus, then you’re in a stable situation again on the offense-defense balance. Or like with planes,  you restrict certain capabilities that the private sector isn’t allowed to have, then you can let the civilian uses run free.
Dwarkesh Patel
I’m skeptical of this.

**Extracted Belief:**

The rapid advancement in destructive technology and military advancements creates an incredibly volatile and dangerous period.

**Context:**

Leopold Aschenbrenner emphasizes the significant threat posed by the rapid advancement of technology, particularly in the context of weapons development, by drawing a parallel between the historical evolution of warfare and the potential for AI-driven advancements.

**Justification:**

Leopold Aschenbrenner describes the historical transition from basic weaponry like bayonets and horses to advanced military technologies like tank armies, fighter jets, nuclear weapons, and stealth technology within a relatively short timeframe. He uses this historical evidence to argue that the current pace of technological advancement in the field of weapons development is incredibly dangerous.

--------

## Chunk 233

**Chunk:**

Dwarkesh Patel
In your series, you talk about Tyler Cowen’s phrase of “muddling through.” Even with privatization, people underrate that there are a lot of private actors who control vital resources like the water supply.

We can count on cooperation and market-based incentives to maintain a balance of power. Sure, things are proceeding really fast. We have a lot of historical evidence that this works best.
Leopold Aschenbrenner
What do we do with nukes, right? We don't keep nukes in check by beefing up the Second Amendment so each state has its own nuclear arsenal. Dario and Sam don’t have their own little arsenal.

No, it’s institutions, constitutions, laws, and courts. I’m not sure this balance of power analogy holds. The government having the biggest guns was an enormous civilizational achievement, like Landfrieden in the Holy Roman Empire. If someone from the neighboring town committed a crime, you didn’t start a battle between the towns. You took it to a court of the Holy Roman Empire. They decided it. It’s a big achievement.

The key differences with the analogy about the industrial fertilizer are speed and offense-defense balance issues. It’s like compressing the 20th century into a few years. That is incredibly scary because of the rapid advancement in destructive technology and military advancements.

You'd go from bayonets and horses to tank armies and fighter jets in a couple of years. In just a few more years you’d have nukes, ICBMs, and stealth. That speed creates an incredibly volatile and dangerous period. We have to make it through that, which will be incredibly challenging.

That’s where a government project is necessary. If we can make it through that, the situation stabilizes. We don’t face this imminent national security threat. Yes, there were WMDs that developed, but we’ve managed to create a stable offense-defense balance.

Bioweapons are a huge issue initially. An attacker can create 1000 different synthetic viruses and spread them. It’s hard to defend against each. Maybe at some point, you figure out a universal defense against every possible virus, then you’re in a stable situation again on the offense-defense balance. Or like with planes,  you restrict certain capabilities that the private sector isn’t allowed to have, then you can let the civilian uses run free.
Dwarkesh Patel
I’m skeptical of this.

**Extracted Belief:**

It is necessary to have a government project to navigate this volatile period of technological advancement and secure national security.

**Context:**

Leopold Aschenbrenner emphasizes the importance of a government project in mitigating the risks associated with rapid technological advancements, particularly in the context of weapon development. He suggests that the government can play a critical role in managing this period of rapid change and ensuring national security.

**Justification:**

Leopold Aschenbrenner argues that a government project is necessary to manage the volatility and danger created by the rapid advancement of destructive technology. He suggests that a centralized authority is needed to regulate and control the development and deployment of such technologies, thus ensuring national security.

--------

## Chunk 234

**Chunk:**

Dwarkesh Patel
In your series, you talk about Tyler Cowen’s phrase of “muddling through.” Even with privatization, people underrate that there are a lot of private actors who control vital resources like the water supply.

We can count on cooperation and market-based incentives to maintain a balance of power. Sure, things are proceeding really fast. We have a lot of historical evidence that this works best.
Leopold Aschenbrenner
What do we do with nukes, right? We don't keep nukes in check by beefing up the Second Amendment so each state has its own nuclear arsenal. Dario and Sam don’t have their own little arsenal.

No, it’s institutions, constitutions, laws, and courts. I’m not sure this balance of power analogy holds. The government having the biggest guns was an enormous civilizational achievement, like Landfrieden in the Holy Roman Empire. If someone from the neighboring town committed a crime, you didn’t start a battle between the towns. You took it to a court of the Holy Roman Empire. They decided it. It’s a big achievement.

The key differences with the analogy about the industrial fertilizer are speed and offense-defense balance issues. It’s like compressing the 20th century into a few years. That is incredibly scary because of the rapid advancement in destructive technology and military advancements.

You'd go from bayonets and horses to tank armies and fighter jets in a couple of years. In just a few more years you’d have nukes, ICBMs, and stealth. That speed creates an incredibly volatile and dangerous period. We have to make it through that, which will be incredibly challenging.

That’s where a government project is necessary. If we can make it through that, the situation stabilizes. We don’t face this imminent national security threat. Yes, there were WMDs that developed, but we’ve managed to create a stable offense-defense balance.

Bioweapons are a huge issue initially. An attacker can create 1000 different synthetic viruses and spread them. It’s hard to defend against each. Maybe at some point, you figure out a universal defense against every possible virus, then you’re in a stable situation again on the offense-defense balance. Or like with planes,  you restrict certain capabilities that the private sector isn’t allowed to have, then you can let the civilian uses run free.
Dwarkesh Patel
I’m skeptical of this.

**Extracted Belief:**

Once this volatile period of rapid technological advancement is navigated, the situation will stabilize.

**Context:**

Leopold Aschenbrenner emphasizes the importance of a government project in mitigating the risks associated with rapid technological advancements, particularly in the context of weapon development. He suggests that the government can play a critical role in managing this period of rapid change and ensuring national security.

**Justification:**

Leopold Aschenbrenner suggests that the initial period of volatility created by rapid technological advancement, particularly in weapon development, is temporary. He argues that once this period is navigated, the situation will stabilize and the risks will be mitigated.

--------

## Chunk 235

**Chunk:**

Dwarkesh Patel
In your series, you talk about Tyler Cowen’s phrase of “muddling through.” Even with privatization, people underrate that there are a lot of private actors who control vital resources like the water supply.

We can count on cooperation and market-based incentives to maintain a balance of power. Sure, things are proceeding really fast. We have a lot of historical evidence that this works best.
Leopold Aschenbrenner
What do we do with nukes, right? We don't keep nukes in check by beefing up the Second Amendment so each state has its own nuclear arsenal. Dario and Sam don’t have their own little arsenal.

No, it’s institutions, constitutions, laws, and courts. I’m not sure this balance of power analogy holds. The government having the biggest guns was an enormous civilizational achievement, like Landfrieden in the Holy Roman Empire. If someone from the neighboring town committed a crime, you didn’t start a battle between the towns. You took it to a court of the Holy Roman Empire. They decided it. It’s a big achievement.

The key differences with the analogy about the industrial fertilizer are speed and offense-defense balance issues. It’s like compressing the 20th century into a few years. That is incredibly scary because of the rapid advancement in destructive technology and military advancements.

You'd go from bayonets and horses to tank armies and fighter jets in a couple of years. In just a few more years you’d have nukes, ICBMs, and stealth. That speed creates an incredibly volatile and dangerous period. We have to make it through that, which will be incredibly challenging.

That’s where a government project is necessary. If we can make it through that, the situation stabilizes. We don’t face this imminent national security threat. Yes, there were WMDs that developed, but we’ve managed to create a stable offense-defense balance.

Bioweapons are a huge issue initially. An attacker can create 1000 different synthetic viruses and spread them. It’s hard to defend against each. Maybe at some point, you figure out a universal defense against every possible virus, then you’re in a stable situation again on the offense-defense balance. Or like with planes,  you restrict certain capabilities that the private sector isn’t allowed to have, then you can let the civilian uses run free.
Dwarkesh Patel
I’m skeptical of this.

**Extracted Belief:**

We can learn from historical examples of successful control over weapons of mass destruction, such as the management of nuclear weapons, to create a stable offense-defense balance in the context of emerging technologies.

**Context:**

Leopold Aschenbrenner argues that the rapid advancement in destructive technology and military advancements creates an incredibly volatile and dangerous period, but he suggests that we can learn from historical examples of successfully controlling weapons of mass destruction, such as the management of nuclear weapons, to create a stable offense-defense balance in the context of emerging technologies.

**Justification:**

Leopold Aschenbrenner uses the historical example of managing nuclear weapons as evidence that it is possible to create a stable offense-defense balance even in the presence of powerful weapons of mass destruction. He suggests that we can apply lessons learned from this experience to the current situation involving rapidly evolving technologies.

--------

## Chunk 236

**Chunk:**

Dwarkesh Patel
In your series, you talk about Tyler Cowen’s phrase of “muddling through.” Even with privatization, people underrate that there are a lot of private actors who control vital resources like the water supply.

We can count on cooperation and market-based incentives to maintain a balance of power. Sure, things are proceeding really fast. We have a lot of historical evidence that this works best.
Leopold Aschenbrenner
What do we do with nukes, right? We don't keep nukes in check by beefing up the Second Amendment so each state has its own nuclear arsenal. Dario and Sam don’t have their own little arsenal.

No, it’s institutions, constitutions, laws, and courts. I’m not sure this balance of power analogy holds. The government having the biggest guns was an enormous civilizational achievement, like Landfrieden in the Holy Roman Empire. If someone from the neighboring town committed a crime, you didn’t start a battle between the towns. You took it to a court of the Holy Roman Empire. They decided it. It’s a big achievement.

The key differences with the analogy about the industrial fertilizer are speed and offense-defense balance issues. It’s like compressing the 20th century into a few years. That is incredibly scary because of the rapid advancement in destructive technology and military advancements.

You'd go from bayonets and horses to tank armies and fighter jets in a couple of years. In just a few more years you’d have nukes, ICBMs, and stealth. That speed creates an incredibly volatile and dangerous period. We have to make it through that, which will be incredibly challenging.

That’s where a government project is necessary. If we can make it through that, the situation stabilizes. We don’t face this imminent national security threat. Yes, there were WMDs that developed, but we’ve managed to create a stable offense-defense balance.

Bioweapons are a huge issue initially. An attacker can create 1000 different synthetic viruses and spread them. It’s hard to defend against each. Maybe at some point, you figure out a universal defense against every possible virus, then you’re in a stable situation again on the offense-defense balance. Or like with planes,  you restrict certain capabilities that the private sector isn’t allowed to have, then you can let the civilian uses run free.
Dwarkesh Patel
I’m skeptical of this.

**Extracted Belief:**

Bioweapons pose a significant challenge due to the ease with which attackers can create and spread synthetic viruses.

**Context:**

Leopold Aschenbrenner highlights the specific threat posed by bioweapons, emphasizing the ease with which attackers can create and spread synthetic viruses.

**Justification:**

Leopold Aschenbrenner states that attackers can create thousands of different synthetic viruses and spread them easily, highlighting the significant challenge posed by bioweapons.

--------

## Chunk 237

**Chunk:**

Dwarkesh Patel
In your series, you talk about Tyler Cowen’s phrase of “muddling through.” Even with privatization, people underrate that there are a lot of private actors who control vital resources like the water supply.

We can count on cooperation and market-based incentives to maintain a balance of power. Sure, things are proceeding really fast. We have a lot of historical evidence that this works best.
Leopold Aschenbrenner
What do we do with nukes, right? We don't keep nukes in check by beefing up the Second Amendment so each state has its own nuclear arsenal. Dario and Sam don’t have their own little arsenal.

No, it’s institutions, constitutions, laws, and courts. I’m not sure this balance of power analogy holds. The government having the biggest guns was an enormous civilizational achievement, like Landfrieden in the Holy Roman Empire. If someone from the neighboring town committed a crime, you didn’t start a battle between the towns. You took it to a court of the Holy Roman Empire. They decided it. It’s a big achievement.

The key differences with the analogy about the industrial fertilizer are speed and offense-defense balance issues. It’s like compressing the 20th century into a few years. That is incredibly scary because of the rapid advancement in destructive technology and military advancements.

You'd go from bayonets and horses to tank armies and fighter jets in a couple of years. In just a few more years you’d have nukes, ICBMs, and stealth. That speed creates an incredibly volatile and dangerous period. We have to make it through that, which will be incredibly challenging.

That’s where a government project is necessary. If we can make it through that, the situation stabilizes. We don’t face this imminent national security threat. Yes, there were WMDs that developed, but we’ve managed to create a stable offense-defense balance.

Bioweapons are a huge issue initially. An attacker can create 1000 different synthetic viruses and spread them. It’s hard to defend against each. Maybe at some point, you figure out a universal defense against every possible virus, then you’re in a stable situation again on the offense-defense balance. Or like with planes,  you restrict certain capabilities that the private sector isn’t allowed to have, then you can let the civilian uses run free.
Dwarkesh Patel
I’m skeptical of this.

**Extracted Belief:**

While it is currently difficult to defend against a wide range of synthetic viruses, the possibility of developing a universal defense against all possible viruses exists.

**Context:**

Leopold Aschenbrenner acknowledges the difficulty of defending against a wide range of synthetic viruses but suggests the possibility of developing a universal defense against all possible viruses, potentially leading to a more stable situation.

**Justification:**

Leopold Aschenbrenner notes that the challenge of defending against bioweapons is significant, given the ability to create many synthetic viruses, but he suggests that a universal defense against all possible viruses might be achievable.

--------

## Chunk 238

**Chunk:**

Dwarkesh Patel
In your series, you talk about Tyler Cowen’s phrase of “muddling through.” Even with privatization, people underrate that there are a lot of private actors who control vital resources like the water supply.

We can count on cooperation and market-based incentives to maintain a balance of power. Sure, things are proceeding really fast. We have a lot of historical evidence that this works best.
Leopold Aschenbrenner
What do we do with nukes, right? We don't keep nukes in check by beefing up the Second Amendment so each state has its own nuclear arsenal. Dario and Sam don’t have their own little arsenal.

No, it’s institutions, constitutions, laws, and courts. I’m not sure this balance of power analogy holds. The government having the biggest guns was an enormous civilizational achievement, like Landfrieden in the Holy Roman Empire. If someone from the neighboring town committed a crime, you didn’t start a battle between the towns. You took it to a court of the Holy Roman Empire. They decided it. It’s a big achievement.

The key differences with the analogy about the industrial fertilizer are speed and offense-defense balance issues. It’s like compressing the 20th century into a few years. That is incredibly scary because of the rapid advancement in destructive technology and military advancements.

You'd go from bayonets and horses to tank armies and fighter jets in a couple of years. In just a few more years you’d have nukes, ICBMs, and stealth. That speed creates an incredibly volatile and dangerous period. We have to make it through that, which will be incredibly challenging.

That’s where a government project is necessary. If we can make it through that, the situation stabilizes. We don’t face this imminent national security threat. Yes, there were WMDs that developed, but we’ve managed to create a stable offense-defense balance.

Bioweapons are a huge issue initially. An attacker can create 1000 different synthetic viruses and spread them. It’s hard to defend against each. Maybe at some point, you figure out a universal defense against every possible virus, then you’re in a stable situation again on the offense-defense balance. Or like with planes,  you restrict certain capabilities that the private sector isn’t allowed to have, then you can let the civilian uses run free.
Dwarkesh Patel
I’m skeptical of this.

**Extracted Belief:**

Restrictions on certain capabilities in the private sector, as exemplified by the regulation of aircraft technology, can allow for the safe advancement of civilian uses.

**Context:**

Leopold Aschenbrenner suggests that restricting certain capabilities in the private sector, as exemplified by the regulation of aircraft technology, can allow for the safe advancement of civilian uses.

**Justification:**

Leopold Aschenbrenner uses the example of aircraft technology regulation, where the private sector is restricted from certain capabilities, to argue that restricting certain capabilities in the private sector can allow for the safe advancement of civilian uses.

--------

## Chunk 239

**Chunk:**

Dwarkesh Patel
I’m skeptical of this.
Leopold Aschenbrenner
This is the other important thing. I talked about one company having all this power. It is unprecedented because the industrial fertilizer guy cannot overthrow the US government. It’s quite plausible that the AI company with superintelligence can.
Dwarkesh Patel
There would be multiple AI companies, right? I buy that one of them could be ahead.

**Extracted Belief:**

An AI company with superintelligence could potentially overthrow the US government.

**Context:**

Responding to skepticism about the possibility of one company having too much power, Leopold Aschenbrenner argues that an AI company with superintelligence could pose a unique and unprecedented threat to the US government.

**Justification:**

The statement is based on the idea that superintelligence in an AI could surpass human capabilities and lead to a significant power imbalance, making it plausible for an AI company to overcome the power of the US government.

--------

## Chunk 240

**Chunk:**

Dwarkesh Patel
There would be multiple AI companies, right? I buy that one of them could be ahead.
Leopold Aschenbrenner
It’s not obvious that it’ll be multiple. If there’s a six-month lead, maybe there are two or three.
Dwarkesh Patel
I agree.

**Extracted Belief:**

It is not certain that there will be multiple AI companies in a future where one company achieves a six-month lead in AI development.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's suggestion that there would be multiple AI companies, even if one were to gain a significant lead. Aschenbrenner is presenting a counter-argument.

**Justification:**

Aschenbrenner argues that a six-month lead in AI development could result in only two or three companies remaining in the race, given the competitive nature of the field. This is a logical deduction based on the assumption that a significant lead would make it difficult for others to catch up.

--------

## Chunk 241

**Chunk:**

Dwarkesh Patel
I agree.
Leopold Aschenbrenner
If there are two or three, then it’s a crazy race between these companies. Demis and Sam would be like, "I don’t want to let the other one win." They’re both developing their nuclear arsenals and robots.

Come on. The government is not going to let these people do that. Is Dario going to be the one developing super hacking Stuxnet and deploying it against the Chinese data center?

The other issue is that if it’s two or three, it won’t just be two or three. It’ll be China, Russia, and North Korea too. In the private lab world, there’s no way they’ll have good enough security.
Dwarkesh Patel
We’re also assuming that if you nationalize it, especially in a world where this stuff is priced in by the CCP, you’ve got it nailed down. I’m not sure why we would expect that.

**Extracted Belief:**

If there are two or three companies developing artificial superintelligence, it will lead to a competitive race between them, driven by a desire to be the first to achieve this goal.

**Context:**

Leopold Aschenbrenner is expressing his concern about the potential consequences of multiple companies racing to develop artificial superintelligence.

**Justification:**

This belief is derived from the assumption that companies, in a competitive environment, strive to outperform their rivals.

--------

## Chunk 242

**Chunk:**

Dwarkesh Patel
I agree.
Leopold Aschenbrenner
If there are two or three, then it’s a crazy race between these companies. Demis and Sam would be like, "I don’t want to let the other one win." They’re both developing their nuclear arsenals and robots.

Come on. The government is not going to let these people do that. Is Dario going to be the one developing super hacking Stuxnet and deploying it against the Chinese data center?

The other issue is that if it’s two or three, it won’t just be two or three. It’ll be China, Russia, and North Korea too. In the private lab world, there’s no way they’ll have good enough security.
Dwarkesh Patel
We’re also assuming that if you nationalize it, especially in a world where this stuff is priced in by the CCP, you’ve got it nailed down. I’m not sure why we would expect that.

**Extracted Belief:**

Companies developing artificial superintelligence, such as those led by Demis Hassabis and Sam Altman, would be willing to take extreme measures, including developing nuclear arsenals and robots, to gain an advantage.

**Context:**

Leopold Aschenbrenner is expressing his concern about the potential consequences of companies developing artificial superintelligence, particularly those led by Demis Hassabis and Sam Altman.

**Justification:**

Leopold Aschenbrenner is expressing a belief based on his understanding of the competitive nature of the technology industry and the potential for companies to take extreme measures to gain an advantage.

--------

## Chunk 243

**Chunk:**

Dwarkesh Patel
I agree.
Leopold Aschenbrenner
If there are two or three, then it’s a crazy race between these companies. Demis and Sam would be like, "I don’t want to let the other one win." They’re both developing their nuclear arsenals and robots.

Come on. The government is not going to let these people do that. Is Dario going to be the one developing super hacking Stuxnet and deploying it against the Chinese data center?

The other issue is that if it’s two or three, it won’t just be two or three. It’ll be China, Russia, and North Korea too. In the private lab world, there’s no way they’ll have good enough security.
Dwarkesh Patel
We’re also assuming that if you nationalize it, especially in a world where this stuff is priced in by the CCP, you’ve got it nailed down. I’m not sure why we would expect that.

**Extracted Belief:**

Governments would not allow companies to develop weapons like super hacking Stuxnet and deploy them against foreign data centers.

**Context:**

Leopold Aschenbrenner is expressing his belief that governments would intervene to prevent companies from developing and deploying dangerous technologies like super hacking Stuxnet.

**Justification:**

The statement is based on the assumption that governments would prioritize national security and would not allow private companies to engage in activities that could jeopardize it.

--------

## Chunk 244

**Chunk:**

Dwarkesh Patel
I agree.
Leopold Aschenbrenner
If there are two or three, then it’s a crazy race between these companies. Demis and Sam would be like, "I don’t want to let the other one win." They’re both developing their nuclear arsenals and robots.

Come on. The government is not going to let these people do that. Is Dario going to be the one developing super hacking Stuxnet and deploying it against the Chinese data center?

The other issue is that if it’s two or three, it won’t just be two or three. It’ll be China, Russia, and North Korea too. In the private lab world, there’s no way they’ll have good enough security.
Dwarkesh Patel
We’re also assuming that if you nationalize it, especially in a world where this stuff is priced in by the CCP, you’ve got it nailed down. I’m not sure why we would expect that.

**Extracted Belief:**

If two or three companies are developing artificial superintelligence, it is likely that other countries, such as China, Russia, and North Korea, will also be involved in this race.

**Context:**

Leopold Aschenbrenner is arguing that the development of artificial superintelligence is likely to involve not just private companies but also state actors.

**Justification:**

This belief is derived from the assumption that countries, particularly those with significant technological capabilities, would be motivated to participate in a race to develop artificial superintelligence.

--------

## Chunk 245

**Chunk:**

Dwarkesh Patel
I agree.
Leopold Aschenbrenner
If there are two or three, then it’s a crazy race between these companies. Demis and Sam would be like, "I don’t want to let the other one win." They’re both developing their nuclear arsenals and robots.

Come on. The government is not going to let these people do that. Is Dario going to be the one developing super hacking Stuxnet and deploying it against the Chinese data center?

The other issue is that if it’s two or three, it won’t just be two or three. It’ll be China, Russia, and North Korea too. In the private lab world, there’s no way they’ll have good enough security.
Dwarkesh Patel
We’re also assuming that if you nationalize it, especially in a world where this stuff is priced in by the CCP, you’ve got it nailed down. I’m not sure why we would expect that.

**Extracted Belief:**

Private labs involved in developing artificial superintelligence would lack the necessary security to prevent unauthorized access or misuse of the technology.

**Context:**

Leopold Aschenbrenner is expressing his concern about the security risks associated with private labs developing artificial superintelligence.

**Justification:**

Leopold Aschenbrenner is drawing on his knowledge of the difficulty of securing sensitive information and technologies, suggesting that private labs are unlikely to have the necessary resources or expertise to prevent unauthorized access or misuse of artificial superintelligence.

--------

## Chunk 246

**Chunk:**

Dwarkesh Patel
We’re also assuming that if you nationalize it, especially in a world where this stuff is priced in by the CCP, you’ve got it nailed down. I’m not sure why we would expect that.
Leopold Aschenbrenner
The government’s the only one who does this stuff.
Dwarkesh Patel
If we don’t trust Sam or Dario to be benevolent dictators…

**Extracted Belief:**

Governments are the only entities capable of effectively managing and controlling powerful technologies, such as artificial superintelligence (ASI).

**Context:**

Leopold Aschenbrenner argues that the government is the only entity capable of managing powerful technologies, while acknowledging that this assertion is controversial given the potential risks associated with government control.

**Justification:**

Aschenbrenner does not provide specific evidence or reasoning to support this belief. However, the statement is made in response to Dwarkesh Patel’s questioning the assumption that nationalization would guarantee control over powerful technology, implying that Aschenbrenner is relying on a general understanding of the government’s role and capabilities.

--------

## Chunk 247

**Chunk:**

Dwarkesh Patel
If we don’t trust Sam or Dario to be benevolent dictators…
Leopold Aschenbrenner
Just corporate governance in general.
Dwarkesh Patel
Because you can cause a coup, the same capabilities are going to be true of the government project, right? The modal president in 2025, Donald Trump, will be the person versus you not trusting Sam or Dario to have these capabilities. I agree that if Sam and Dario have a one-year lead on ASI, in that world I’m concerned about privatization.

In that exact same world, I’m very concerned about Donald Trump having the capability. Potentially, if the takeoff is slower than anticipated, I prefer the private companies in that world. In no part of this matrix is it obviously true that the government-led project is better.

**Extracted Belief:**

Corporate governance in general is not reliable enough to prevent the misuse of powerful technologies.

**Context:**

Leopold Aschenbrenner is expressing concerns about the potential for misuse of Artificial Superintelligence (ASI) developed by private companies, and he argues that the existing system of corporate governance is not sufficient to prevent such misuse.

**Justification:**

The belief is based on Leopold's general statement that he does not trust 'corporate governance in general.' He does not offer specific evidence but implies that he has a general skepticism about the effectiveness of corporate governance.

--------

## Chunk 248

**Chunk:**

Dwarkesh Patel
Because you can cause a coup, the same capabilities are going to be true of the government project, right? The modal president in 2025, Donald Trump, will be the person versus you not trusting Sam or Dario to have these capabilities. I agree that if Sam and Dario have a one-year lead on ASI, in that world I’m concerned about privatization.

In that exact same world, I’m very concerned about Donald Trump having the capability. Potentially, if the takeoff is slower than anticipated, I prefer the private companies in that world. In no part of this matrix is it obviously true that the government-led project is better.
Leopold Aschenbrenner
Let’s talk about the government project and checks and balances.

In some sense, my argument is a Burkean one. American checks and balances have held for over 200 years through crazy technological revolutions. The US military could kill every civilian in the United States.
Dwarkesh Patel
You’re going to make that argument. The private-public balance of power has held for hundreds of years.

**Extracted Belief:**

American checks and balances have held for over 200 years through crazy technological revolutions.

**Context:**

Leopold Aschenbrenner is arguing that a government-led approach to artificial superintelligence is preferable to a private sector approach, citing the effectiveness of existing checks and balances in the United States.

**Justification:**

Leopold Aschenbrenner states that American checks and balances have endured over 200 years, including through significant technological advancements.

--------

## Chunk 249

**Chunk:**

Dwarkesh Patel
Because you can cause a coup, the same capabilities are going to be true of the government project, right? The modal president in 2025, Donald Trump, will be the person versus you not trusting Sam or Dario to have these capabilities. I agree that if Sam and Dario have a one-year lead on ASI, in that world I’m concerned about privatization.

In that exact same world, I’m very concerned about Donald Trump having the capability. Potentially, if the takeoff is slower than anticipated, I prefer the private companies in that world. In no part of this matrix is it obviously true that the government-led project is better.
Leopold Aschenbrenner
Let’s talk about the government project and checks and balances.

In some sense, my argument is a Burkean one. American checks and balances have held for over 200 years through crazy technological revolutions. The US military could kill every civilian in the United States.
Dwarkesh Patel
You’re going to make that argument. The private-public balance of power has held for hundreds of years.

**Extracted Belief:**

The US military could kill every civilian in the United States.

**Context:**

Leopold Aschenbrenner is making a point about the potential for unchecked power in government, even with checks and balances in place.

**Justification:**

Leopold Aschenbrenner states that the US military possesses the capability to inflict harm on a large scale, highlighting the potential for unchecked power in government.

--------

## Chunk 250

**Chunk:**

Dwarkesh Patel
You’re going to make that argument. The private-public balance of power has held for hundreds of years.
Leopold Aschenbrenner
But, why has it held? It’s because the government has had the biggest guns. Never before has a single CEO or a random nonprofit board had the ability to launch nukes.

What is the track record of government checks and balances versus the track record of the private company checks and balances? Well the AI lab's first stress test went really badly. 

Even worse in the private company world, it’s two private companies and the CCP. They’ll just instantly have all the tech. They probably won’t have good enough internal control. It’s not just the random CEO, but rogue employees who can use these superintelligences to do whatever they want.
Dwarkesh Patel
This won’t be true of the government? Rogue employees won’t exist on the project?

**Extracted Belief:**

The government has maintained a balance of power over the private sector for centuries because it possesses greater military power, including the ability to launch nuclear weapons.

**Context:**

Leopold Aschenbrenner is arguing that the government's control over military power has historically prevented the private sector from gaining dominance. He uses the example of nuclear weapons to illustrate this point.

**Justification:**

Leopold Aschenbrenner states that the government has 'had the biggest guns' and that 'never before has a single CEO or a random nonprofit board had the ability to launch nukes.'

--------

## Chunk 251

**Chunk:**

Dwarkesh Patel
You’re going to make that argument. The private-public balance of power has held for hundreds of years.
Leopold Aschenbrenner
But, why has it held? It’s because the government has had the biggest guns. Never before has a single CEO or a random nonprofit board had the ability to launch nukes.

What is the track record of government checks and balances versus the track record of the private company checks and balances? Well the AI lab's first stress test went really badly. 

Even worse in the private company world, it’s two private companies and the CCP. They’ll just instantly have all the tech. They probably won’t have good enough internal control. It’s not just the random CEO, but rogue employees who can use these superintelligences to do whatever they want.
Dwarkesh Patel
This won’t be true of the government? Rogue employees won’t exist on the project?

**Extracted Belief:**

Private companies, such as AI labs, are more susceptible to security breaches and rogue employee behavior than government institutions.

**Context:**

Leopold Aschenbrenner is expressing concerns about the potential for misuse of superintelligence by private companies, citing the example of the 'AI lab's first stress test' going poorly.

**Justification:**

He highlights the 'AI lab's first stress test' going 'really badly' as evidence of private companies' vulnerabilities. He also suggests that private companies may not have adequate 'internal control' to prevent rogue employees from misusing superintelligence.

--------

## Chunk 252

**Chunk:**

Dwarkesh Patel
You’re going to make that argument. The private-public balance of power has held for hundreds of years.
Leopold Aschenbrenner
But, why has it held? It’s because the government has had the biggest guns. Never before has a single CEO or a random nonprofit board had the ability to launch nukes.

What is the track record of government checks and balances versus the track record of the private company checks and balances? Well the AI lab's first stress test went really badly. 

Even worse in the private company world, it’s two private companies and the CCP. They’ll just instantly have all the tech. They probably won’t have good enough internal control. It’s not just the random CEO, but rogue employees who can use these superintelligences to do whatever they want.
Dwarkesh Patel
This won’t be true of the government? Rogue employees won’t exist on the project?

**Extracted Belief:**

The Chinese Communist Party (CCP) poses a significant risk in the private sector's development of superintelligence due to its potential to acquire advanced technology quickly and its lack of internal control.

**Context:**

Leopold Aschenbrenner is expressing concerns about the CCP's potential involvement in the development of superintelligence.

**Justification:**

He states that 'it’s two private companies and the CCP' and that they will 'just instantly have all the tech.' He also suggests that the CCP may not have 'good enough internal control' to prevent misuse of superintelligence.

--------

## Chunk 253

**Chunk:**

Dwarkesh Patel
This won’t be true of the government? Rogue employees won’t exist on the project?
Leopold Aschenbrenner
The government has actual decades of experience and actually cares about this stuff. They deal with nukes and really powerful technology. This is the stuff that the national security state cares about.

Let's talk about government checks and balances a little bit. What are checks and balances in the government world? First, it’s important to have some international coalition. I talked about these two tiers before. The inner tiers are modeled on the Quebec Agreement, Churchill and Roosevelt agreeing to pool efforts on nukes but not using them against each other, or anyone else without consent.

Bring in the UK with DeepMind, Southeast Asian states with the chip supply chain, and more NATO allies with talent and industrial resources. You have those checks and balances with more international countries at the table.

Separately, you have the second tier of coalitions, the Atoms for Peace thing. You go to countries including the UAE and make a deal similar to the NPT. They’re not allowed to do crazy military stuff, but we’ll share civilian applications. We’ll help them and share the benefits, creating a new post-superintelligence world order.

US checks and balances: Congress will have to be involved to appropriate trillions of dollars. Ideally, Congress needs to confirm whoever’s running this. You have Congress, different factions of the government, and the courts. I expect the First Amendment to remain really important.

This sounds crazy to people, but these institutions have withstood the test of time in a powerful way. This is why alignment is important. You program AIs to follow the constitution. The military works because generals are not allowed to follow unlawful or unconstitutional orders. You have the same thing for the AIs.
Dwarkesh Patel
So what’s wrong with this argument. Maybe you have a point in a world with an extremely fast takeoff, one year from AGI to ASI.

**Extracted Belief:**

The US government has decades of experience in dealing with powerful technology and national security, including nuclear weapons.

**Context:**

Leopold Aschenbrenner is arguing that the US government is well-equipped to handle the development and control of artificial superintelligence, drawing a comparison with its experience in managing nuclear weapons.

**Justification:**

He states that the government 'deals with nukes and really powerful technology,' implying that this experience makes them capable of handling the challenges of superintelligence.

--------

## Chunk 254

**Chunk:**

Dwarkesh Patel
This won’t be true of the government? Rogue employees won’t exist on the project?
Leopold Aschenbrenner
The government has actual decades of experience and actually cares about this stuff. They deal with nukes and really powerful technology. This is the stuff that the national security state cares about.

Let's talk about government checks and balances a little bit. What are checks and balances in the government world? First, it’s important to have some international coalition. I talked about these two tiers before. The inner tiers are modeled on the Quebec Agreement, Churchill and Roosevelt agreeing to pool efforts on nukes but not using them against each other, or anyone else without consent.

Bring in the UK with DeepMind, Southeast Asian states with the chip supply chain, and more NATO allies with talent and industrial resources. You have those checks and balances with more international countries at the table.

Separately, you have the second tier of coalitions, the Atoms for Peace thing. You go to countries including the UAE and make a deal similar to the NPT. They’re not allowed to do crazy military stuff, but we’ll share civilian applications. We’ll help them and share the benefits, creating a new post-superintelligence world order.

US checks and balances: Congress will have to be involved to appropriate trillions of dollars. Ideally, Congress needs to confirm whoever’s running this. You have Congress, different factions of the government, and the courts. I expect the First Amendment to remain really important.

This sounds crazy to people, but these institutions have withstood the test of time in a powerful way. This is why alignment is important. You program AIs to follow the constitution. The military works because generals are not allowed to follow unlawful or unconstitutional orders. You have the same thing for the AIs.
Dwarkesh Patel
So what’s wrong with this argument. Maybe you have a point in a world with an extremely fast takeoff, one year from AGI to ASI.

**Extracted Belief:**

International coalitions, modeled after historical agreements like the Quebec Agreement and the Atoms for Peace program, can provide checks and balances for the development and use of artificial superintelligence.

**Context:**

Leopold Aschenbrenner is proposing a framework for international cooperation in the development and control of artificial superintelligence.

**Justification:**

He suggests that by creating a multi-tiered coalition system, with varying levels of cooperation and agreement, the potential risks of superintelligence can be mitigated.

--------

## Chunk 255

**Chunk:**

Dwarkesh Patel
This won’t be true of the government? Rogue employees won’t exist on the project?
Leopold Aschenbrenner
The government has actual decades of experience and actually cares about this stuff. They deal with nukes and really powerful technology. This is the stuff that the national security state cares about.

Let's talk about government checks and balances a little bit. What are checks and balances in the government world? First, it’s important to have some international coalition. I talked about these two tiers before. The inner tiers are modeled on the Quebec Agreement, Churchill and Roosevelt agreeing to pool efforts on nukes but not using them against each other, or anyone else without consent.

Bring in the UK with DeepMind, Southeast Asian states with the chip supply chain, and more NATO allies with talent and industrial resources. You have those checks and balances with more international countries at the table.

Separately, you have the second tier of coalitions, the Atoms for Peace thing. You go to countries including the UAE and make a deal similar to the NPT. They’re not allowed to do crazy military stuff, but we’ll share civilian applications. We’ll help them and share the benefits, creating a new post-superintelligence world order.

US checks and balances: Congress will have to be involved to appropriate trillions of dollars. Ideally, Congress needs to confirm whoever’s running this. You have Congress, different factions of the government, and the courts. I expect the First Amendment to remain really important.

This sounds crazy to people, but these institutions have withstood the test of time in a powerful way. This is why alignment is important. You program AIs to follow the constitution. The military works because generals are not allowed to follow unlawful or unconstitutional orders. You have the same thing for the AIs.
Dwarkesh Patel
So what’s wrong with this argument. Maybe you have a point in a world with an extremely fast takeoff, one year from AGI to ASI.

**Extracted Belief:**

Existing US institutions, such as Congress, the courts, and the First Amendment, can effectively regulate and control the development and use of artificial superintelligence.

**Context:**

Leopold Aschenbrenner is expressing confidence in the ability of the US government to manage the challenges of superintelligence, citing the resilience of its institutions.

**Justification:**

He argues that these institutions 'have withstood the test of time in a powerful way,' suggesting they can adapt to the challenges of superintelligence.

--------

## Chunk 256

**Chunk:**

Dwarkesh Patel
This won’t be true of the government? Rogue employees won’t exist on the project?
Leopold Aschenbrenner
The government has actual decades of experience and actually cares about this stuff. They deal with nukes and really powerful technology. This is the stuff that the national security state cares about.

Let's talk about government checks and balances a little bit. What are checks and balances in the government world? First, it’s important to have some international coalition. I talked about these two tiers before. The inner tiers are modeled on the Quebec Agreement, Churchill and Roosevelt agreeing to pool efforts on nukes but not using them against each other, or anyone else without consent.

Bring in the UK with DeepMind, Southeast Asian states with the chip supply chain, and more NATO allies with talent and industrial resources. You have those checks and balances with more international countries at the table.

Separately, you have the second tier of coalitions, the Atoms for Peace thing. You go to countries including the UAE and make a deal similar to the NPT. They’re not allowed to do crazy military stuff, but we’ll share civilian applications. We’ll help them and share the benefits, creating a new post-superintelligence world order.

US checks and balances: Congress will have to be involved to appropriate trillions of dollars. Ideally, Congress needs to confirm whoever’s running this. You have Congress, different factions of the government, and the courts. I expect the First Amendment to remain really important.

This sounds crazy to people, but these institutions have withstood the test of time in a powerful way. This is why alignment is important. You program AIs to follow the constitution. The military works because generals are not allowed to follow unlawful or unconstitutional orders. You have the same thing for the AIs.
Dwarkesh Patel
So what’s wrong with this argument. Maybe you have a point in a world with an extremely fast takeoff, one year from AGI to ASI.

**Extracted Belief:**

Artificial intelligence can be programmed to follow the US Constitution, similar to how military officers are bound to obey lawful orders.

**Context:**

Leopold Aschenbrenner is advocating for a constitutional approach to AI alignment, suggesting that AI systems should be programmed to adhere to existing laws and principles.

**Justification:**

He draws a parallel between the military, where officers are trained to follow the law, and the potential for programming AI systems to adhere to the Constitution.

--------

## Chunk 257

**Chunk:**

Dwarkesh Patel
So what’s wrong with this argument. Maybe you have a point in a world with an extremely fast takeoff, one year from AGI to ASI.
Leopold Aschenbrenner
Then you have the years after ASI where you have this extraordinary explosion and technological progress.
Dwarkesh Patel
Maybe you have a point. We don’t know. You have arguments for why that’s a more likely world, but maybe that’s not the world we live in.

In the other world, I’m very much on the side of ensuring these things are privately held. When you nationalize, that’s a one-way function. You can’t go back.

Why not wait until we have more evidence on which world we live in? Rushing nationalization might be a bad idea while we’re uncertain. I’ll let you respond to that first.

**Extracted Belief:**

The years after the development of Artificial Superintelligence (ASI) will be characterized by an extraordinary explosion and technological progress.

**Context:**

Responding to Dwarkesh Patel's argument about the possibility of a rapid transition from Artificial General Intelligence (AGI) to ASI, Leopold Aschenbrenner suggests that the time period after ASI would be marked by significant technological advancement.

**Justification:**

While not explicitly stated, the implication is that the creation of ASI would lead to rapid technological breakthroughs, based on the historical correlation between technological advancements and their associated impact.

--------

## Chunk 258

**Chunk:**

Dwarkesh Patel
So what’s wrong with this argument. Maybe you have a point in a world with an extremely fast takeoff, one year from AGI to ASI.
Leopold Aschenbrenner
Then you have the years after ASI where you have this extraordinary explosion and technological progress.
Dwarkesh Patel
Maybe you have a point. We don’t know. You have arguments for why that’s a more likely world, but maybe that’s not the world we live in.

In the other world, I’m very much on the side of ensuring these things are privately held. When you nationalize, that’s a one-way function. You can’t go back.

Why not wait until we have more evidence on which world we live in? Rushing nationalization might be a bad idea while we’re uncertain. I’ll let you respond to that first.

**Extracted Belief:**

Nationalization of technology is a one-way process, meaning that once a technology is nationalized, it is difficult or impossible to return it to private control.

**Context:**

In response to Dwarkesh Patel's suggestion of waiting for more evidence before nationalizing powerful technologies like ASI, Leopold Aschenbrenner asserts the irreversibility of nationalization.

**Justification:**

The argument rests on the inherent difficulty of reversing government control and returning assets to private ownership, often due to legal and political complexities.

--------

## Chunk 259

**Chunk:**

Dwarkesh Patel
So what’s wrong with this argument. Maybe you have a point in a world with an extremely fast takeoff, one year from AGI to ASI.
Leopold Aschenbrenner
Then you have the years after ASI where you have this extraordinary explosion and technological progress.
Dwarkesh Patel
Maybe you have a point. We don’t know. You have arguments for why that’s a more likely world, but maybe that’s not the world we live in.

In the other world, I’m very much on the side of ensuring these things are privately held. When you nationalize, that’s a one-way function. You can’t go back.

Why not wait until we have more evidence on which world we live in? Rushing nationalization might be a bad idea while we’re uncertain. I’ll let you respond to that first.

**Extracted Belief:**

Rushing nationalization of potentially transformative technologies, such as ASI, might be unwise in the absence of sufficient evidence about their implications.

**Context:**

Leopold Aschenbrenner cautions against hasty nationalization of ASI, arguing that a lack of certainty about its potential consequences necessitates a more cautious approach.

**Justification:**

The belief is based on the principle of prudence, suggesting that significant decisions regarding potentially disruptive technologies should be informed by thorough understanding and evidence.

--------

## Chunk 260

**Chunk:**

Dwarkesh Patel
Maybe you have a point. We don’t know. You have arguments for why that’s a more likely world, but maybe that’s not the world we live in.

In the other world, I’m very much on the side of ensuring these things are privately held. When you nationalize, that’s a one-way function. You can’t go back.

Why not wait until we have more evidence on which world we live in? Rushing nationalization might be a bad idea while we’re uncertain. I’ll let you respond to that first.
Leopold Aschenbrenner
I don’t expect us to nationalize tomorrow. If anything I expect it to be like COVID,where it’s kind of too late. Ideally, you nationalize early enough to lock stuff down. It’ll probably be chaotic. You’ll be trying to do a crash program to lock stuff down. It’ll be kind of late. It’ll be clear what’s happening. We’re not going to nationalize when it’s not clear what’s happening.
Dwarkesh Patel
The argument that these institutions have held up historically so well is flawed. They’ve actually almost broken a bunch of times.

**Extracted Belief:**

Nationalization of artificial superintelligence (ASI) will likely occur after the consequences of its development become clear, rather than before.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's argument that nationalization of ASI should be delayed until there is more evidence about its impact.

**Justification:**

Leopold Aschenbrenner compares the situation to the COVID-19 pandemic, suggesting that nationalization will happen in a similar way, with government intervention occurring when the effects are already evident.

--------

## Chunk 261

**Chunk:**

Dwarkesh Patel
The argument that these institutions have held up historically so well is flawed. They’ve actually almost broken a bunch of times.
Leopold Aschenbrenner
They’ve held up. They didn’t break the first time they were tested.
Dwarkesh Patel
This is similar to the argument that some people make about nuclear war: we’ve had nukes for 80 years and have been fine, so the risk must be low. The answer to that is no. The risk is really high. We’ve avoided it because people have made a lot of effort to prevent it. Giving the government ASI without knowing the implications isn’t making that effort.

Look at the base rate. America is very exceptional, not just in terms of avoiding dictatorship. Every other country in history has had a complete drawdown of wealth because of war, revolution, etc. America is very unique in not having had that.

We have to think of the historical base rate. We haven’t thought about great power competition in the last 80 years, but it’s significant. Dictatorship is the default state of mankind. Relying on institutions in an ASI world is fundamentally different. Right now, if the government tried to overthrow, it’s much harder without ASI. There are people with AK-47s and AR-15s, making it harder.

**Extracted Belief:**

The institutions that are relevant to the discussion, such as democratic institutions, have held up under historical pressures and have not broken down.

**Context:**

Leopold Aschenbrenner is defending the resilience of democratic institutions against the argument that they have nearly failed in the past.

**Justification:**

He states, "They’ve held up. They didn’t break the first time they were tested." This implies that these institutions have been tested historically and have survived.

--------

## Chunk 262

**Chunk:**

Dwarkesh Patel
This is similar to the argument that some people make about nuclear war: we’ve had nukes for 80 years and have been fine, so the risk must be low. The answer to that is no. The risk is really high. We’ve avoided it because people have made a lot of effort to prevent it. Giving the government ASI without knowing the implications isn’t making that effort.

Look at the base rate. America is very exceptional, not just in terms of avoiding dictatorship. Every other country in history has had a complete drawdown of wealth because of war, revolution, etc. America is very unique in not having had that.

We have to think of the historical base rate. We haven’t thought about great power competition in the last 80 years, but it’s significant. Dictatorship is the default state of mankind. Relying on institutions in an ASI world is fundamentally different. Right now, if the government tried to overthrow, it’s much harder without ASI. There are people with AK-47s and AR-15s, making it harder.
Leopold Aschenbrenner
The government could crush the AR-15s.
Dwarkesh Patel
No, it would actually be pretty hard. It’s the reason why Vietnam and Afghanistan were so hard.

**Extracted Belief:**

The government could overcome resistance from people with AR-15s.

**Context:**

Responding to Dwarkesh Patel's assertion that it would be difficult for the government to overthrow a population armed with AR-15s, Leopold Aschenbrenner argued that the government could still overcome such resistance.

**Justification:**

While not explicitly stated, the justification for this belief likely stems from the assumption that the government possesses superior military capabilities and resources, capable of suppressing any armed opposition. This aligns with his later statement regarding nuclear weapons, suggesting a reliance on brute force as a solution.

--------

## Chunk 263

**Chunk:**

Dwarkesh Patel
No, it would actually be pretty hard. It’s the reason why Vietnam and Afghanistan were so hard.
Leopold Aschenbrenner
They could just nuke the whole country.
Dwarkesh Patel
I agree.

**Extracted Belief:**

A government with advanced artificial superintelligence (ASI) could use nuclear weapons to effectively control a country.

**Context:**

Leopold Aschenbrenner was responding to Dwarkesh Patel's assertion that a government without ASI would find it difficult to suppress a rebellion due to the widespread availability of firearms. Aschenbrenner suggested that a government with ASI could overcome this challenge by using nuclear weapons, implying that ASI would grant the government an overwhelming advantage.

**Justification:**

The reasoning behind this belief is based on the assumption that ASI would provide the government with the computational power and strategic insight needed to effectively deploy nuclear weapons, rendering any opposition ineffective. This assumption is not explicitly supported by evidence or data presented in the conversation, but it is implied by Aschenbrenner's statement.

--------

## Chunk 264

**Chunk:**

Dwarkesh Patel
I agree.
Leopold Aschenbrenner
They could. It’s similar to the ASI.
Dwarkesh Patel
It’s just easier if you have what you were talking.

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 265

**Chunk:**

Dwarkesh Patel
It’s just easier if you have what you were talking.
Leopold Aschenbrenner
No, there are institutions, constitutions, legal restraints, courts, and checks and balances. The crazy bet is the bet on a private company CEO.
Dwarkesh Patel
Isn’t the same thing true of nukes where we have institutional agreements about non-proliferation? We’re still very concerned about those being broken and someone getting nukes. We stay up at night worrying about that situation.

**Extracted Belief:**

The existence of institutions, constitutions, legal restraints, courts, and checks and balances prevents a private company CEO from having complete control over advanced technologies.

**Context:**

Leopold Aschenbrenner argues against the idea of a private company CEO having unchecked power over advanced technologies, highlighting the presence of established institutions and legal frameworks as countermeasures.

**Justification:**

Aschenbrenner explicitly lists institutions, constitutions, legal restraints, courts, and checks and balances as existing safeguards against unchecked power. He contrasts this with the potential danger of a single individual, a private company CEO, having complete control.

--------

## Chunk 266

**Chunk:**

Dwarkesh Patel
Isn’t the same thing true of nukes where we have institutional agreements about non-proliferation? We’re still very concerned about those being broken and someone getting nukes. We stay up at night worrying about that situation.
Leopold Aschenbrenner
It’s a precarious situation. ASI will a precarious situation as well. Given how precarious nukes are, we’ve done pretty well.
Dwarkesh Patel
What does privatization in this world even mean? What happens after?

**Extracted Belief:**

The development and control of artificial superintelligence (ASI) presents a precarious situation.

**Context:**

Leopold Aschenbrenner is comparing the potential risks associated with ASI to the existing risks posed by nuclear weapons.

**Justification:**

He explicitly states that ASI will be a precarious situation, referencing the precariousness of nuclear weapons as a point of comparison.  This suggests a belief based on the perceived dangers of both ASI and nuclear weapons.

--------

## Chunk 267

**Chunk:**

Dwarkesh Patel
Isn’t the same thing true of nukes where we have institutional agreements about non-proliferation? We’re still very concerned about those being broken and someone getting nukes. We stay up at night worrying about that situation.
Leopold Aschenbrenner
It’s a precarious situation. ASI will a precarious situation as well. Given how precarious nukes are, we’ve done pretty well.
Dwarkesh Patel
What does privatization in this world even mean? What happens after?

**Extracted Belief:**

Humanity has managed the risks associated with nuclear weapons relatively well.

**Context:**

Leopold Aschenbrenner is comparing the potential risks associated with ASI to the existing risks posed by nuclear weapons.

**Justification:**

He states that given how precarious nuclear weapons are, 'we've done pretty well.' This suggests a belief based on the observed history of nuclear proliferation and the relative absence of large-scale nuclear conflicts.

--------

## Chunk 268

**Chunk:**

Dwarkesh Patel
What does privatization in this world even mean? What happens after?
Leopold Aschenbrenner
We’re talking about whether the government project is good or not. I have very mixed feelings about this as well.

My primary argument is that if you’re at the point where this thing has vastly superhuman capabilities — it can develop crazy bioweapons targeted to kill everyone but the Han Chinese, it can wipe out entire countries, it can build robo armies and drone swarms with mosquito-sized drones — the US national security state will be intimately involved.

The government project will look like a joint venture between cloud providers, labs, and the government. There is no world in which the government isn’t involved in this crazy period. At the very least, intelligence agencies need to run security for these labs. They’re already controlling access to everything.

If we’re in a volatile international situation, initial applications will focus on stabilizing it. It’ll suck. It’s not what I want to use ASI for. Somehow we need to prevent proliferation of new WMDs, and maintain mutually assured destruction with North Korea, Russia, and China.

There’s a broader spectrum than you’re acknowledging. In a world with private labs, there will be heavy government involvement. What we’re debating is the form of government involvement, but it will look more like the national security state than a startup, which is what it is right now.
Dwarkesh Patel
Something like that makes sense. I’d be very worried if it’s like the Manhattan Project, where it’s directly part of the US military. If it’s more like needing to talk to Jake Sullivan before running the next training line...

**Extracted Belief:**

If artificial superintelligence (ASI) reaches a point where it possesses vastly superhuman capabilities, the US national security state will be heavily involved in its development and use.

**Context:**

Leopold Aschenbrenner is making an argument about the inevitability of government involvement in ASI development, particularly in the context of potential risks and applications.

**Justification:**

He reasons that the potential for ASI to develop powerful bioweapons, wipe out countries, and create autonomous weapon systems would necessitate government involvement to ensure security and control.

--------

## Chunk 269

**Chunk:**

Dwarkesh Patel
What does privatization in this world even mean? What happens after?
Leopold Aschenbrenner
We’re talking about whether the government project is good or not. I have very mixed feelings about this as well.

My primary argument is that if you’re at the point where this thing has vastly superhuman capabilities — it can develop crazy bioweapons targeted to kill everyone but the Han Chinese, it can wipe out entire countries, it can build robo armies and drone swarms with mosquito-sized drones — the US national security state will be intimately involved.

The government project will look like a joint venture between cloud providers, labs, and the government. There is no world in which the government isn’t involved in this crazy period. At the very least, intelligence agencies need to run security for these labs. They’re already controlling access to everything.

If we’re in a volatile international situation, initial applications will focus on stabilizing it. It’ll suck. It’s not what I want to use ASI for. Somehow we need to prevent proliferation of new WMDs, and maintain mutually assured destruction with North Korea, Russia, and China.

There’s a broader spectrum than you’re acknowledging. In a world with private labs, there will be heavy government involvement. What we’re debating is the form of government involvement, but it will look more like the national security state than a startup, which is what it is right now.
Dwarkesh Patel
Something like that makes sense. I’d be very worried if it’s like the Manhattan Project, where it’s directly part of the US military. If it’s more like needing to talk to Jake Sullivan before running the next training line...

**Extracted Belief:**

The US government will be involved in any project that develops ASI with significant capabilities, even if the project is initially private.

**Context:**

Leopold Aschenbrenner is arguing that government involvement in ASI development is unavoidable, even if the project is initially private.

**Justification:**

He suggests that the government will have to be involved to ensure security and control, and that there is no realistic scenario where the government is completely absent from such projects.

--------

## Chunk 270

**Chunk:**

Dwarkesh Patel
What does privatization in this world even mean? What happens after?
Leopold Aschenbrenner
We’re talking about whether the government project is good or not. I have very mixed feelings about this as well.

My primary argument is that if you’re at the point where this thing has vastly superhuman capabilities — it can develop crazy bioweapons targeted to kill everyone but the Han Chinese, it can wipe out entire countries, it can build robo armies and drone swarms with mosquito-sized drones — the US national security state will be intimately involved.

The government project will look like a joint venture between cloud providers, labs, and the government. There is no world in which the government isn’t involved in this crazy period. At the very least, intelligence agencies need to run security for these labs. They’re already controlling access to everything.

If we’re in a volatile international situation, initial applications will focus on stabilizing it. It’ll suck. It’s not what I want to use ASI for. Somehow we need to prevent proliferation of new WMDs, and maintain mutually assured destruction with North Korea, Russia, and China.

There’s a broader spectrum than you’re acknowledging. In a world with private labs, there will be heavy government involvement. What we’re debating is the form of government involvement, but it will look more like the national security state than a startup, which is what it is right now.
Dwarkesh Patel
Something like that makes sense. I’d be very worried if it’s like the Manhattan Project, where it’s directly part of the US military. If it’s more like needing to talk to Jake Sullivan before running the next training line...

**Extracted Belief:**

The government's role in ASI development will likely resemble a joint venture between private labs, cloud providers, and government agencies.

**Context:**

Leopold Aschenbrenner is discussing the potential structure of government involvement in ASI development.

**Justification:**

He envisions a model where the government partners with private labs and cloud providers to manage and oversee the development of ASI, emphasizing the collaborative nature of the effort.

--------

## Chunk 271

**Chunk:**

Dwarkesh Patel
What does privatization in this world even mean? What happens after?
Leopold Aschenbrenner
We’re talking about whether the government project is good or not. I have very mixed feelings about this as well.

My primary argument is that if you’re at the point where this thing has vastly superhuman capabilities — it can develop crazy bioweapons targeted to kill everyone but the Han Chinese, it can wipe out entire countries, it can build robo armies and drone swarms with mosquito-sized drones — the US national security state will be intimately involved.

The government project will look like a joint venture between cloud providers, labs, and the government. There is no world in which the government isn’t involved in this crazy period. At the very least, intelligence agencies need to run security for these labs. They’re already controlling access to everything.

If we’re in a volatile international situation, initial applications will focus on stabilizing it. It’ll suck. It’s not what I want to use ASI for. Somehow we need to prevent proliferation of new WMDs, and maintain mutually assured destruction with North Korea, Russia, and China.

There’s a broader spectrum than you’re acknowledging. In a world with private labs, there will be heavy government involvement. What we’re debating is the form of government involvement, but it will look more like the national security state than a startup, which is what it is right now.
Dwarkesh Patel
Something like that makes sense. I’d be very worried if it’s like the Manhattan Project, where it’s directly part of the US military. If it’s more like needing to talk to Jake Sullivan before running the next training line...

**Extracted Belief:**

The initial applications of ASI will likely focus on stabilizing volatile international situations.

**Context:**

Leopold Aschenbrenner is discussing the potential uses of ASI in the context of global security.

**Justification:**

He suggests that ASI will likely be used to address international tensions and conflicts, highlighting the potential for both benefits and risks.

--------

## Chunk 272

**Chunk:**

Dwarkesh Patel
What does privatization in this world even mean? What happens after?
Leopold Aschenbrenner
We’re talking about whether the government project is good or not. I have very mixed feelings about this as well.

My primary argument is that if you’re at the point where this thing has vastly superhuman capabilities — it can develop crazy bioweapons targeted to kill everyone but the Han Chinese, it can wipe out entire countries, it can build robo armies and drone swarms with mosquito-sized drones — the US national security state will be intimately involved.

The government project will look like a joint venture between cloud providers, labs, and the government. There is no world in which the government isn’t involved in this crazy period. At the very least, intelligence agencies need to run security for these labs. They’re already controlling access to everything.

If we’re in a volatile international situation, initial applications will focus on stabilizing it. It’ll suck. It’s not what I want to use ASI for. Somehow we need to prevent proliferation of new WMDs, and maintain mutually assured destruction with North Korea, Russia, and China.

There’s a broader spectrum than you’re acknowledging. In a world with private labs, there will be heavy government involvement. What we’re debating is the form of government involvement, but it will look more like the national security state than a startup, which is what it is right now.
Dwarkesh Patel
Something like that makes sense. I’d be very worried if it’s like the Manhattan Project, where it’s directly part of the US military. If it’s more like needing to talk to Jake Sullivan before running the next training line...

**Extracted Belief:**

The potential uses of ASI for security purposes are not necessarily desirable.

**Context:**

Leopold Aschenbrenner is expressing a concern about the potential misuse of ASI for security purposes.

**Justification:**

He states that using ASI to maintain mutually assured destruction or prevent the proliferation of new weapons of mass destruction is not his preferred application for the technology.

--------

## Chunk 273

**Chunk:**

Dwarkesh Patel
What does privatization in this world even mean? What happens after?
Leopold Aschenbrenner
We’re talking about whether the government project is good or not. I have very mixed feelings about this as well.

My primary argument is that if you’re at the point where this thing has vastly superhuman capabilities — it can develop crazy bioweapons targeted to kill everyone but the Han Chinese, it can wipe out entire countries, it can build robo armies and drone swarms with mosquito-sized drones — the US national security state will be intimately involved.

The government project will look like a joint venture between cloud providers, labs, and the government. There is no world in which the government isn’t involved in this crazy period. At the very least, intelligence agencies need to run security for these labs. They’re already controlling access to everything.

If we’re in a volatile international situation, initial applications will focus on stabilizing it. It’ll suck. It’s not what I want to use ASI for. Somehow we need to prevent proliferation of new WMDs, and maintain mutually assured destruction with North Korea, Russia, and China.

There’s a broader spectrum than you’re acknowledging. In a world with private labs, there will be heavy government involvement. What we’re debating is the form of government involvement, but it will look more like the national security state than a startup, which is what it is right now.
Dwarkesh Patel
Something like that makes sense. I’d be very worried if it’s like the Manhattan Project, where it’s directly part of the US military. If it’s more like needing to talk to Jake Sullivan before running the next training line...

**Extracted Belief:**

Government involvement in ASI development will likely be more substantial than in the case of traditional startups.

**Context:**

Leopold Aschenbrenner is contrasting government involvement in ASI development with the current situation in the tech industry.

**Justification:**

He suggests that government involvement in ASI will be more significant due to the technology's potential impact and risks, drawing a comparison to the national security state rather than a typical startup model.

--------

## Chunk 274

**Chunk:**

Dwarkesh Patel
Something like that makes sense. I’d be very worried if it’s like the Manhattan Project, where it’s directly part of the US military. If it’s more like needing to talk to Jake Sullivan before running the next training line...
Leopold Aschenbrenner
Is Lockheed Martin’s Skunk Works part of the US military? They call the shots.
Dwarkesh Patel
I don’t think that’s great and I think that’s bad if it happens with ASI. What’s the scenario?

**Extracted Belief:**

Lockheed Martin's Skunk Works, a private aerospace company, operates independently of the US military but exerts significant influence on its operations.

**Context:**

Leopold Aschenbrenner uses Skunk Works as an example to argue that private companies can wield considerable influence even without being directly part of the military.

**Justification:**

Leopold Aschenbrenner makes the assertion that Skunk Works, a private company, 'calls the shots' which suggests that they have significant influence on military decisions.

--------

## Chunk 275

**Chunk:**

Dwarkesh Patel
I don’t think that’s great and I think that’s bad if it happens with ASI. What’s the scenario?
Leopold Aschenbrenner
What’s the alternative? What’s the alternative?
Dwarkesh Patel
It’s closer to my end of the spectrum. You talk to Jake Sullivan before you can launch the next training cluster, but many companies are still going for it, and the government is involved in security.

**Extracted Belief:**

There is no realistic alternative to government involvement in the development and deployment of advanced AI systems.

**Context:**

Leopold Aschenbrenner responds to Dwarkesh Patel's concern about the potential negative consequences of heavy government involvement in AI development by arguing that such involvement is unavoidable given the potential capabilities of AI.

**Justification:**

Aschenbrenner asserts that given the potential of AI to develop powerful bioweapons, wipe out entire countries, and create robotic armies, the government will inevitably be involved in its development and use. He emphasizes that the US national security state will be intimately involved, implying that there are no viable alternatives to this involvement.

--------

## Chunk 276

**Chunk:**

Dwarkesh Patel
It’s closer to my end of the spectrum. You talk to Jake Sullivan before you can launch the next training cluster, but many companies are still going for it, and the government is involved in security.
Leopold Aschenbrenner
Is Dario launching the Stuxnet attack?
Dwarkesh Patel
What do you mean by launching?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 277

**Chunk:**

Dwarkesh Patel
What do you mean by launching?
Leopold Aschenbrenner
Dario is deactivating the Chinese data centers?
Dwarkesh Patel
This is similar to the story you could tell about Big Tech right now. Satya, if he wanted to, could get his engineers to find zero days in Windows and infiltrate the president’s computer. Right now, Satya could do that.

**Extracted Belief:**

The government would not allow Microsoft to find vulnerabilities in Windows and use them to infiltrate the president's computer.

**Context:**

Responding to Dwarkesh Patel's hypothetical scenario where Satya Nadella could use Microsoft's power to infiltrate the government, Leopold Aschenbrenner expresses his belief about government intervention.

**Justification:**

Leopold Aschenbrenner explicitly states that 'The government wouldn’t let them do that.' This suggests a belief in the government's ability and willingness to prevent such actions.

--------

## Chunk 278

**Chunk:**

Dwarkesh Patel
This is similar to the story you could tell about Big Tech right now. Satya, if he wanted to, could get his engineers to find zero days in Windows and infiltrate the president’s computer. Right now, Satya could do that.
Leopold Aschenbrenner
They’d be shut down.
Dwarkesh Patel
What do you mean?

**Extracted Belief:**

If a company like Microsoft, under the leadership of Satya Nadella, attempted to exploit vulnerabilities in Windows to infiltrate the president's computer, the government would intervene and shut them down.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's hypothetical scenario of Satya Nadella using Microsoft's resources to infiltrate the president's computer.

**Justification:**

Aschenbrenner does not explicitly cite specific evidence or reasoning for this belief. His statement suggests reliance on his own understanding of government response to such actions or on information from other sources.

--------

## Chunk 279

**Chunk:**

Dwarkesh Patel
What do you mean?
Leopold Aschenbrenner
The government wouldn’t let them do that.
Dwarkesh Patel
There’s a story where they could pull off a coup.

**Extracted Belief:**

The government would not allow large tech companies to use their technological capabilities to infiltrate the president's computer.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's hypothetical scenario about a tech company, like Microsoft, infiltrating the president's computer.

**Justification:**

Leopold Aschenbrenner believes this because he states that 'The government wouldn’t let them do that' in response to Dwarkesh Patel's hypothetical scenario.

--------

## Chunk 280

**Chunk:**

Dwarkesh Patel
There’s a story where they could pull off a coup.
Leopold Aschenbrenner
They could not pull off a coup.
Dwarkesh Patel
Fine, I agree. What’s wrong with a scenario where multiple companies are going for it? The AI is still broadly deployed. Alignment works. The system-level prompt is that it can’t help people make bioweapons or something. It’s still broadly deployed.

**Extracted Belief:**

It is impossible for companies to successfully execute a coup using AI.

**Context:**

In response to Dwarkesh Patel's hypothetical scenario about companies potentially leveraging AI to conduct a coup, Leopold Aschenbrenner explicitly states that such an operation is impossible.

**Justification:**

Leopold Aschenbrenner's statement is presented as a logical deduction, implying that there are inherent limitations within the AI technology or its deployment that would prevent a coup.

--------

## Chunk 281

**Chunk:**

Dwarkesh Patel
Fine, I agree. What’s wrong with a scenario where multiple companies are going for it? The AI is still broadly deployed. Alignment works. The system-level prompt is that it can’t help people make bioweapons or something. It’s still broadly deployed.
Leopold Aschenbrenner
I expect AIs to be broadly deployed.
Dwarkesh Patel
Even if it’s a government project?

**Extracted Belief:**

Artificial intelligence (AI) will be widely deployed in the future.

**Context:**

Leopold Aschenbrenner is expressing his expectation about the future of AI development and its impact on society.

**Justification:**

Aschenbrenner is drawing on his knowledge of the current trends in AI research and development. He believes that AI technology is progressing rapidly and is likely to become increasingly prevalent.

--------

## Chunk 282

**Chunk:**

Dwarkesh Patel
Even if it’s a government project?
Leopold Aschenbrenner
Yeah, I think the Metas of the world open-sourcing their AIs that are two years behind is super valuable. There will be some question of whether the offense-defense balance is fine, and open-sourcing two-year-old AIs is fine. Or there are restrictions on the most extreme dual-use capabilities, like not letting private companies sell crazy weapons.

That’s great and will help with diffusion. After the government project, there will be an initial tense period. Hopefully, it stabilizes. Then, like Boeing, they’ll go out and do all the flourishing civilian applications like nuclear energy. The civilian applications will have their day.
Dwarkesh Patel
How does that proceed? Because in the other world, there are existing stocks of capital that are worth a lot.

**Extracted Belief:**

Open-sourcing AIs that are two years behind the most advanced versions is valuable for technological advancement and diffusion.

**Context:**

Leopold Aschenbrenner is discussing the potential benefits of open-sourcing older AI technology in a context where a government project has developed a more advanced artificial superintelligence (ASI).

**Justification:**

He believes this is valuable because it allows other companies to catch up and use the technology for their own purposes.

--------

## Chunk 283

**Chunk:**

Dwarkesh Patel
Even if it’s a government project?
Leopold Aschenbrenner
Yeah, I think the Metas of the world open-sourcing their AIs that are two years behind is super valuable. There will be some question of whether the offense-defense balance is fine, and open-sourcing two-year-old AIs is fine. Or there are restrictions on the most extreme dual-use capabilities, like not letting private companies sell crazy weapons.

That’s great and will help with diffusion. After the government project, there will be an initial tense period. Hopefully, it stabilizes. Then, like Boeing, they’ll go out and do all the flourishing civilian applications like nuclear energy. The civilian applications will have their day.
Dwarkesh Patel
How does that proceed? Because in the other world, there are existing stocks of capital that are worth a lot.

**Extracted Belief:**

There is a need to consider the balance between offensive and defensive capabilities when it comes to open-sourcing AI technologies.

**Context:**

He is concerned about the potential risks of open-sourcing AI technologies, particularly in the context of national security.

**Justification:**

He believes that making AI technology widely available could lead to the development of dangerous weapons.

--------

## Chunk 284

**Chunk:**

Dwarkesh Patel
Even if it’s a government project?
Leopold Aschenbrenner
Yeah, I think the Metas of the world open-sourcing their AIs that are two years behind is super valuable. There will be some question of whether the offense-defense balance is fine, and open-sourcing two-year-old AIs is fine. Or there are restrictions on the most extreme dual-use capabilities, like not letting private companies sell crazy weapons.

That’s great and will help with diffusion. After the government project, there will be an initial tense period. Hopefully, it stabilizes. Then, like Boeing, they’ll go out and do all the flourishing civilian applications like nuclear energy. The civilian applications will have their day.
Dwarkesh Patel
How does that proceed? Because in the other world, there are existing stocks of capital that are worth a lot.

**Extracted Belief:**

Restrictions are necessary to prevent the misuse of dual-use technologies, such as AI, for purposes like the development of weapons.

**Context:**

He is suggesting that government regulation is necessary to prevent the development of dangerous weapons from AI technology.

**Justification:**

He believes that it is important to prevent private companies from selling dangerous weapons, such as those that could be developed with advanced AI.

--------

## Chunk 285

**Chunk:**

Dwarkesh Patel
Even if it’s a government project?
Leopold Aschenbrenner
Yeah, I think the Metas of the world open-sourcing their AIs that are two years behind is super valuable. There will be some question of whether the offense-defense balance is fine, and open-sourcing two-year-old AIs is fine. Or there are restrictions on the most extreme dual-use capabilities, like not letting private companies sell crazy weapons.

That’s great and will help with diffusion. After the government project, there will be an initial tense period. Hopefully, it stabilizes. Then, like Boeing, they’ll go out and do all the flourishing civilian applications like nuclear energy. The civilian applications will have their day.
Dwarkesh Patel
How does that proceed? Because in the other world, there are existing stocks of capital that are worth a lot.

**Extracted Belief:**

Open-sourcing older AI technologies will contribute to the wider diffusion of AI.

**Context:**

He is expressing confidence in the benefits of open-sourcing for the overall growth of the AI field.

**Justification:**

He believes that making AI technology available to a wider range of companies and developers will encourage innovation and adoption.

--------

## Chunk 286

**Chunk:**

Dwarkesh Patel
Even if it’s a government project?
Leopold Aschenbrenner
Yeah, I think the Metas of the world open-sourcing their AIs that are two years behind is super valuable. There will be some question of whether the offense-defense balance is fine, and open-sourcing two-year-old AIs is fine. Or there are restrictions on the most extreme dual-use capabilities, like not letting private companies sell crazy weapons.

That’s great and will help with diffusion. After the government project, there will be an initial tense period. Hopefully, it stabilizes. Then, like Boeing, they’ll go out and do all the flourishing civilian applications like nuclear energy. The civilian applications will have their day.
Dwarkesh Patel
How does that proceed? Because in the other world, there are existing stocks of capital that are worth a lot.

**Extracted Belief:**

After a government-led AI project, there will be a period of tension as private companies adapt and develop their own AI capabilities.

**Context:**

He is describing a future scenario where the government leads the development of advanced AI and private companies will eventually catch up.

**Justification:**

He believes that the initial period after a government-led AI project will be characterized by competition and instability.

--------

## Chunk 287

**Chunk:**

Dwarkesh Patel
Even if it’s a government project?
Leopold Aschenbrenner
Yeah, I think the Metas of the world open-sourcing their AIs that are two years behind is super valuable. There will be some question of whether the offense-defense balance is fine, and open-sourcing two-year-old AIs is fine. Or there are restrictions on the most extreme dual-use capabilities, like not letting private companies sell crazy weapons.

That’s great and will help with diffusion. After the government project, there will be an initial tense period. Hopefully, it stabilizes. Then, like Boeing, they’ll go out and do all the flourishing civilian applications like nuclear energy. The civilian applications will have their day.
Dwarkesh Patel
How does that proceed? Because in the other world, there are existing stocks of capital that are worth a lot.

**Extracted Belief:**

The development of AI will eventually lead to a wide range of civilian applications, similar to how nuclear energy transitioned from military to civilian use.

**Context:**

He is comparing the future development of AI with the transition of nuclear energy from military to civilian use.

**Justification:**

He believes that the widespread adoption of AI will lead to many beneficial uses in various industries and aspects of daily life.

--------

## Chunk 288

**Chunk:**

Dwarkesh Patel
How does that proceed? Because in the other world, there are existing stocks of capital that are worth a lot.
Leopold Aschenbrenner
There will still be Google clusters.
Dwarkesh Patel
So Google, because they got the contract from the government, will control the ASI? But why are they trading with anybody else?

**Extracted Belief:**

Google will continue to have large computing clusters even after a government project develops an artificial superintelligence (ASI).

**Context:**

Dwarkesh Patel asks Leopold Aschenbrenner how the transition from a government-led ASI project to private companies utilizing it would proceed, considering existing capital investments like Google's computing clusters.

**Justification:**

Leopold Aschenbrenner states, "There will still be Google clusters," indicating his belief that Google's computing infrastructure will remain, even if the government is involved in developing the ASI.

--------

## Chunk 289

**Chunk:**

Dwarkesh Patel
So Google, because they got the contract from the government, will control the ASI? But why are they trading with anybody else?
Leopold Aschenbrenner
It’ll be the same companies that would be doing it anyway. In this world, they’re just contracting with the government or are DPA’d so all their compute goes to the government. In some sense it’s very natural.
Dwarkesh Patel
After you get the ASI and we’re building the robot armies and fusion reactors…

**Extracted Belief:**

The companies that would be involved in developing and utilizing ASI will be the same regardless of whether the government contracts them or not.

**Context:**

Responding to Dwarkesh Patel's question about why Google would trade with other companies after receiving a government contract for ASI development, Leopold Aschenbrenner expresses this belief.

**Justification:**

This belief appears to be based on Leopold Aschenbrenner's observations of the technology and business landscape, suggesting that the same companies are likely to be involved in ASI development regardless of government contracts.

--------

## Chunk 290

**Chunk:**

Dwarkesh Patel
So Google, because they got the contract from the government, will control the ASI? But why are they trading with anybody else?
Leopold Aschenbrenner
It’ll be the same companies that would be doing it anyway. In this world, they’re just contracting with the government or are DPA’d so all their compute goes to the government. In some sense it’s very natural.
Dwarkesh Patel
After you get the ASI and we’re building the robot armies and fusion reactors…

**Extracted Belief:**

In a scenario where ASI development is primarily contracted by the government, those companies will be subject to government directives and restrictions, including the allocation of computing resources.

**Context:**

Leopold Aschenbrenner is outlining the scenario of a government-contracted ASI development, emphasizing the control and restrictions that come with it.

**Justification:**

He cites the practice of DPA (Defense Priorities and Allocations) as evidence of how government contracts could direct computing resources towards specific projects.

--------

## Chunk 291

**Chunk:**

Dwarkesh Patel
After you get the ASI and we’re building the robot armies and fusion reactors…
Leopold Aschenbrenner
Only the government will get to build robot armies.
Dwarkesh Patel
Now I’m worried. Or like the fusion reactors and stuff.

**Extracted Belief:**

Only the government will be allowed to build robot armies.

**Context:**

Leopold Aschenbrenner was responding to a hypothetical scenario in which the ASI is developed and robot armies are built. He is suggesting that the government would be the only entity authorized to construct such armies.

**Justification:**

Leopold Aschenbrenner does not provide any evidence or reasoning to support this belief, but his expertise in the field of AI and robotics gives this belief some credibility.

--------

## Chunk 292

**Chunk:**

Dwarkesh Patel
Now I’m worried. Or like the fusion reactors and stuff.
Leopold Aschenbrenner
That’s what we do with nukes today.
Dwarkesh Patel
If you already have the robot armies and everything, the existing society doesn’t have some leverage where it makes sense for the government to—

**Extracted Belief:**

The government has a monopoly on the development and use of nuclear weapons.

**Context:**

Dwarkesh Patel expressed concern about the potential for the government to monopolize advanced technologies like fusion reactors and robot armies. Leopold Aschenbrenner responded by stating that this is the current situation with nuclear weapons, implying that the government already has a monopoly on these powerful technologies.

**Justification:**

Leopold Aschenbrenner's statement explicitly compares the potential future scenario with the current situation regarding nuclear weapons, suggesting that the government already has exclusive control over these weapons. This implies that the government has a monopoly on these technologies.

--------

## Chunk 293

**Chunk:**

Dwarkesh Patel
If you already have the robot armies and everything, the existing society doesn’t have some leverage where it makes sense for the government to—
Leopold Aschenbrenner
They don’t have that today.
Dwarkesh Patel
They do, in the sense that they have a lot of capital that the government wants. There are other things as well. Why was Boeing privatized after WWII?

**Extracted Belief:**

Existing society does not have leverage over governments regarding the development of advanced technologies.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's speculation that society might have leverage over governments regarding the development of powerful technologies like robot armies. 

**Justification:**

Leopold Aschenbrenner states 'They don't have that today' in reference to society's leverage over government, implying that he believes this to be the case based on his observation of current power dynamics.

--------

## Chunk 294

**Chunk:**

Dwarkesh Patel
They do, in the sense that they have a lot of capital that the government wants. There are other things as well. Why was Boeing privatized after WWII?
Leopold Aschenbrenner
The government has the biggest guns. The way we regulate is through institutions, constitutions, legal restraints, courts, etc.
Dwarkesh Patel
Tell me what privatization looks like in the ASI world afterwards.

**Extracted Belief:**

Governments have the ability to regulate through institutions, constitutions, legal restraints, and courts.

**Context:**

Leopold Aschenbrenner was responding to Dwarkesh Patel's question about why Boeing was privatized after WWII.

**Justification:**

He stated that the government has the 'biggest guns' and that regulations are carried out through these institutions.

--------

## Chunk 295

**Chunk:**

Dwarkesh Patel
Tell me what privatization looks like in the ASI world afterwards.
Leopold Aschenbrenner
Afterwards, it’s like the Boeing example.
Dwarkesh Patel
Who gets it?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 296

**Chunk:**

Dwarkesh Patel
Who gets it?
Leopold Aschenbrenner
Google and Microsoft, the AI labs—
Dwarkesh Patel
Who are they selling it to? They already have robot factories. Why are they selling it to us? They don’t need anything from us. This is chump change in the ASI world because we didn’t get the ASI broadly deployed throughout this takeoff.

We don’t have the robots, the fusion reactors, or the advanced decades of science you’re talking about. What are they trading with us for?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 297

**Chunk:**

Dwarkesh Patel
Who are they selling it to? They already have robot factories. Why are they selling it to us? They don’t need anything from us. This is chump change in the ASI world because we didn’t get the ASI broadly deployed throughout this takeoff.

We don’t have the robots, the fusion reactors, or the advanced decades of science you’re talking about. What are they trading with us for?
Leopold Aschenbrenner
Trading with whom for?
Dwarkesh Patel
For everybody who was not part of the project. They’ve got technology that’s decades ahead.

**Extracted Belief:**

The spread of advanced technology, like ASI, will likely lead to a significant economic redistribution, potentially creating unequal outcomes.

**Context:**

Responding to the host's question about who the AI labs might be selling their technology to, Leopold Aschenbrenner states that the economic distribution of ASI technology would be a complex issue, hinting at potential inequalities.

**Justification:**

Leopold Aschenbrenner states, "That’s a whole other issue of how economic distribution works. I don’t know. That’ll be rough.

--------

## Chunk 298

**Chunk:**

Dwarkesh Patel
For everybody who was not part of the project. They’ve got technology that’s decades ahead.
Leopold Aschenbrenner
That’s a whole other issue of how economic distribution works. I don’t know. That’ll be rough. I’m just saying, I don’t see the alternative. The alternative is overturning a 500-year civilizational achievement of Landfrieden. You basically instantly leak the stuff to the CCP. 

Either you barely scrape ahead, but you’re in a fever struggle, proliferating crazy WMDs. It’s enormously dangerous for alignment because you’re in a crazy race at the end, and you don’t have the ability to take six months to get alignment right. The alternative is not bundling efforts to win the race against authoritarian powers.

I don’t like it. I wish we used the ASI to cure diseases and do all the good in the world. But it’s my prediction that in the end game, what’s at stake is not just cool products but whether liberal democracy survives, whether the CCP survives.

What will the world order for the next century be? When that is at stake, forces will be activated that are way beyond what we’re talking about now. In this crazy race at the end, the national security implications will be the most important.

To go back to World War II, nuclear energy had its day, but in the initial period when the technology was first discovered, you had to stabilize the situation. You had to get nukes and do it right. Then the civilian applications had their day.
Dwarkesh Patel
I agree that nuclear energy is a thing that happened later on and is dual-use. But it’s something that happened literally a decade after nuclear weapons were developed.

**Extracted Belief:**

There is no alternative to the current approach of developing Artificial Superintelligence (ASI), which involves a race against authoritarian powers, potentially leading to a proliferation of weapons of mass destruction.

**Context:**

Leopold Aschenbrenner is responding to a hypothetical scenario where Google and Microsoft possess ASI technology, and the question arises of how to distribute it to those who were not part of the development.

**Justification:**

Aschenbrenner argues that the alternative, releasing ASI technology to the CCP, would be a severe setback to the 500-year achievement of Landfrieden (a state of peace or truce) and lead to a dangerous arms race.

--------

## Chunk 299

**Chunk:**

Dwarkesh Patel
For everybody who was not part of the project. They’ve got technology that’s decades ahead.
Leopold Aschenbrenner
That’s a whole other issue of how economic distribution works. I don’t know. That’ll be rough. I’m just saying, I don’t see the alternative. The alternative is overturning a 500-year civilizational achievement of Landfrieden. You basically instantly leak the stuff to the CCP. 

Either you barely scrape ahead, but you’re in a fever struggle, proliferating crazy WMDs. It’s enormously dangerous for alignment because you’re in a crazy race at the end, and you don’t have the ability to take six months to get alignment right. The alternative is not bundling efforts to win the race against authoritarian powers.

I don’t like it. I wish we used the ASI to cure diseases and do all the good in the world. But it’s my prediction that in the end game, what’s at stake is not just cool products but whether liberal democracy survives, whether the CCP survives.

What will the world order for the next century be? When that is at stake, forces will be activated that are way beyond what we’re talking about now. In this crazy race at the end, the national security implications will be the most important.

To go back to World War II, nuclear energy had its day, but in the initial period when the technology was first discovered, you had to stabilize the situation. You had to get nukes and do it right. Then the civilian applications had their day.
Dwarkesh Patel
I agree that nuclear energy is a thing that happened later on and is dual-use. But it’s something that happened literally a decade after nuclear weapons were developed.

**Extracted Belief:**

The development and deployment of ASI is a national security issue with significant implications for the future world order.

**Context:**

Aschenbrenner emphasizes the importance of national security considerations in the context of the potential for ASI to change the world order.

**Justification:**

He compares the situation to the development of nuclear weapons during World War II, where the focus was initially on stabilization and national security, followed by civilian applications.

--------

## Chunk 300

**Chunk:**

Dwarkesh Patel
I agree that nuclear energy is a thing that happened later on and is dual-use. But it’s something that happened literally a decade after nuclear weapons were developed.
Leopold Aschenbrenner
Right because everything took a long time.
Dwarkesh Patel
Whereas with AI, all the applications are immediately unlocked. This is closer to the analogy people make about AGI. Assume your society had 100 million more John von Neumanns.

If that literally happened, if tomorrow you had 100 million more of them, I don’t think the approach would be that we have to worry about some of them converting to ISIS or “what if a bunch are born in China?” I don’t think we’d be talking about nationalizing all the John von Neumanns.

I think it’d generally be a good thing. I’d be concerned about one power getting all the John von Neumanns.

**Extracted Belief:**

The development of new technologies, such as AI, unfolds rapidly and has a significant impact on society.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's analogy of 100 million John von Neumann's, stating that the sudden emergence of AI is comparable to a rapid industrial explosion.

**Justification:**

This is supported by the context of the conversation and Leopold Aschenbrenner's reference to the analogy of an industrial explosion, suggesting he believes AI's development is rapid and its impact is significant.

--------

## Chunk 301

**Chunk:**

Dwarkesh Patel
Whereas with AI, all the applications are immediately unlocked. This is closer to the analogy people make about AGI. Assume your society had 100 million more John von Neumanns.

If that literally happened, if tomorrow you had 100 million more of them, I don’t think the approach would be that we have to worry about some of them converting to ISIS or “what if a bunch are born in China?” I don’t think we’d be talking about nationalizing all the John von Neumanns.

I think it’d generally be a good thing. I’d be concerned about one power getting all the John von Neumanns.
Leopold Aschenbrenner
The issue is bottling up, in a short period of time, this enormous unfolding of technological progress, an industrial explosion. We do worry about 100 million John von Neumanns.

Why do we worry about the rise of China? It’s one billion people who can do a lot of industry and technology. This is like the rise of China multiplied by 100. It’s not just one billion people, but a billion super-intelligent beings. Plus, it comes all in a very short period.
Dwarkesh Patel
Practically, if the goal is to beat China, part of that is protecting ourselves.

**Extracted Belief:**

The advancement of artificial intelligence (AI) technology will be rapid and widespread, leading to a significant industrial explosion within a short period of time.

**Context:**

Leopold Aschenbrenner is expressing his concern about the potential consequences of a rapid increase in AI capabilities, comparing it to having 100 million John von Neumanns.

**Justification:**

He uses the analogy of having 100 million John von Neumanns, suggesting that the pace and scale of AI development will be akin to having a vast number of highly intelligent individuals.

--------

## Chunk 302

**Chunk:**

Dwarkesh Patel
Whereas with AI, all the applications are immediately unlocked. This is closer to the analogy people make about AGI. Assume your society had 100 million more John von Neumanns.

If that literally happened, if tomorrow you had 100 million more of them, I don’t think the approach would be that we have to worry about some of them converting to ISIS or “what if a bunch are born in China?” I don’t think we’d be talking about nationalizing all the John von Neumanns.

I think it’d generally be a good thing. I’d be concerned about one power getting all the John von Neumanns.
Leopold Aschenbrenner
The issue is bottling up, in a short period of time, this enormous unfolding of technological progress, an industrial explosion. We do worry about 100 million John von Neumanns.

Why do we worry about the rise of China? It’s one billion people who can do a lot of industry and technology. This is like the rise of China multiplied by 100. It’s not just one billion people, but a billion super-intelligent beings. Plus, it comes all in a very short period.
Dwarkesh Patel
Practically, if the goal is to beat China, part of that is protecting ourselves.

**Extracted Belief:**

The rapid advancement of AI poses a significant risk, potentially causing concern due to the potential for a large number of super-intelligent beings.

**Context:**

Leopold Aschenbrenner is contrasting the current concerns about China's economic and technological rise to the potential future scenario of having a billion super-intelligent beings.

**Justification:**

He states that having 100 million John von Neumanns, or equivalently a billion super-intelligent beings, would be akin to the rise of China multiplied by 100, implying a significant level of concern.

--------

## Chunk 303

**Chunk:**

Dwarkesh Patel
Practically, if the goal is to beat China, part of that is protecting ourselves.
Leopold Aschenbrenner
Beating China is just one of the goals. We also want to manage this incredibly crazy, scary period.
Dwarkesh Patel
Right. Part of that is making sure we’re not leaking algorithmic secrets to them.

**Extracted Belief:**

The rise of powerful artificial intelligence presents a significant challenge that needs to be carefully managed.

**Context:**

Leopold Aschenbrenner is expressing concern about the rapid development and potential consequences of advanced AI, linking it to the challenge of managing a period of rapid technological advancement.

**Justification:**

He refers to the rise of powerful AI as 'incredibly crazy, scary period', suggesting a perceived threat or uncertainty.

--------

## Chunk 304

**Chunk:**

Dwarkesh Patel
Practically, if the goal is to beat China, part of that is protecting ourselves.
Leopold Aschenbrenner
Beating China is just one of the goals. We also want to manage this incredibly crazy, scary period.
Dwarkesh Patel
Right. Part of that is making sure we’re not leaking algorithmic secrets to them.

**Extracted Belief:**

The potential for AI to surpass human intelligence and capabilities poses a serious threat.

**Context:**

Leopold Aschenbrenner highlights the potential impact of AI on society, emphasizing the importance of managing this powerful technology.

**Justification:**

His use of terms like 'scary period' and 'crazy' implies a sense of apprehension and uncertainty about the potential consequences of AI.

--------

## Chunk 305

**Chunk:**

Dwarkesh Patel
Right. Part of that is making sure we’re not leaking algorithmic secrets to them.
Leopold Aschenbrenner
Building the trillion-dollar cluster.
Dwarkesh Patel
That’s right. But isn’t your point that Microsoft can issue corporate bonds?

**Extracted Belief:**

Building a trillion-dollar cluster of supercomputers is necessary to accelerate the development of artificial general intelligence.

**Context:**

Leopold Aschenbrenner is discussing the need to invest in a significant amount of computing power in order to advance AI research.

**Justification:**

Aschenbrenner's belief is grounded in the idea that massive computing power is required to handle the complexity of AI, particularly for AGI.

--------

## Chunk 306

**Chunk:**

Dwarkesh Patel
That’s right. But isn’t your point that Microsoft can issue corporate bonds?
Leopold Aschenbrenner
Microsoft can do hundreds of billions of dollars. The trillion-dollar cluster is closer to a national effort.
Dwarkesh Patel
I thought your earlier point was that American capital markets are deep and good.

**Extracted Belief:**

Microsoft can invest hundreds of billions of dollars in projects.

**Context:**

Leopold Aschenbrenner is discussing the feasibility of building a trillion-dollar cluster, and he uses Microsoft's investment capacity as a point of reference.

**Justification:**

The statement is based on the observable capacity of Microsoft to invest large sums of money in various projects.

--------

## Chunk 307

**Chunk:**

Dwarkesh Patel
That’s right. But isn’t your point that Microsoft can issue corporate bonds?
Leopold Aschenbrenner
Microsoft can do hundreds of billions of dollars. The trillion-dollar cluster is closer to a national effort.
Dwarkesh Patel
I thought your earlier point was that American capital markets are deep and good.

**Extracted Belief:**

A trillion-dollar cluster requires a national effort.

**Context:**

Leopold Aschenbrenner is contrasting the capabilities of private companies like Microsoft with the scale of a trillion-dollar cluster.

**Justification:**

The statement is based on the logical deduction that a project of such magnitude would require resources and coordination beyond the capacity of a single company.

--------

## Chunk 308

**Chunk:**

Dwarkesh Patel
I thought your earlier point was that American capital markets are deep and good.
Leopold Aschenbrenner
They’re pretty good. The trillion-dollar cluster is possible privately, but it’s going to be tough.
Dwarkesh Patel
At this point, we have AGI that’s rapidly accelerating productivity.

**Extracted Belief:**

American capital markets are relatively strong and capable of financing large-scale projects.

**Context:**

Leopold Aschenbrenner responded to Dwarkesh Patel's statement that American capital markets are 'deep and good', acknowledging their strength while noting the difficulty of financing a 'trillion-dollar cluster' privately.

**Justification:**

Leopold Aschenbrenner states, 'They’re pretty good', referring to the American capital markets.

--------

## Chunk 309

**Chunk:**

Dwarkesh Patel
I thought your earlier point was that American capital markets are deep and good.
Leopold Aschenbrenner
They’re pretty good. The trillion-dollar cluster is possible privately, but it’s going to be tough.
Dwarkesh Patel
At this point, we have AGI that’s rapidly accelerating productivity.

**Extracted Belief:**

A 'trillion-dollar cluster', a very large-scale computing infrastructure, is possible to build privately, but it would be challenging.

**Context:**

Leopold Aschenbrenner is discussing the feasibility of building a 'trillion-dollar cluster' with private capital in the context of technological advancements and the potential for artificial general intelligence (AGI).

**Justification:**

He states, 'The trillion-dollar cluster is possible privately, but it’s going to be tough'.

--------

## Chunk 310

**Chunk:**

Dwarkesh Patel
At this point, we have AGI that’s rapidly accelerating productivity.
Leopold Aschenbrenner
The trillion-dollar cluster will be planned before the AGI. You get the AGI on the 10 GW cluster. Maybe have one more year of final unhobbling to fully unlock it. Then you have the intelligence explosion.

Meanwhile, the trillion-dollar cluster is almost finished. You run your superintelligence on it. You also have hundreds of millions of GPUs on inference clusters everywhere.
Dwarkesh Patel
In this world, I think private companies have their capital and can raise capital.

**Extracted Belief:**

The development of a trillion-dollar computing cluster will precede the emergence of Artificial General Intelligence (AGI).

**Context:**

Leopold Aschenbrenner is discussing the timeline for the development of AGI and its relation to the construction of a massive computing cluster, referred to as a 'trillion-dollar cluster.'

**Justification:**

This belief is based on the logical sequencing of events: the cluster is required for the development and deployment of AGI.  He states that 'the trillion-dollar cluster will be planned before the AGI. You get the AGI on the 10 GW cluster.'

--------

## Chunk 311

**Chunk:**

Dwarkesh Patel
At this point, we have AGI that’s rapidly accelerating productivity.
Leopold Aschenbrenner
The trillion-dollar cluster will be planned before the AGI. You get the AGI on the 10 GW cluster. Maybe have one more year of final unhobbling to fully unlock it. Then you have the intelligence explosion.

Meanwhile, the trillion-dollar cluster is almost finished. You run your superintelligence on it. You also have hundreds of millions of GPUs on inference clusters everywhere.
Dwarkesh Patel
In this world, I think private companies have their capital and can raise capital.

**Extracted Belief:**

A 10 Gigawatt (GW) computing cluster will be sufficient to facilitate the emergence of AGI.

**Context:**

Leopold Aschenbrenner is suggesting that a specific level of computational power is necessary for achieving AGI, and that a 10 GW cluster will provide that power.

**Justification:**

This belief is based on the presumed connection between computational power and the emergence of AGI. He states that 'You get the AGI on the 10 GW cluster.'

--------

## Chunk 312

**Chunk:**

Dwarkesh Patel
At this point, we have AGI that’s rapidly accelerating productivity.
Leopold Aschenbrenner
The trillion-dollar cluster will be planned before the AGI. You get the AGI on the 10 GW cluster. Maybe have one more year of final unhobbling to fully unlock it. Then you have the intelligence explosion.

Meanwhile, the trillion-dollar cluster is almost finished. You run your superintelligence on it. You also have hundreds of millions of GPUs on inference clusters everywhere.
Dwarkesh Patel
In this world, I think private companies have their capital and can raise capital.

**Extracted Belief:**

The full potential of AGI will be realized after a period of 'unhobbling' or refinement.

**Context:**

Leopold Aschenbrenner is discussing the need to further develop AGI after its initial emergence.

**Justification:**

This belief is based on the perceived need for optimization and refinement of AGI after its initial development. He states that 'Maybe have one more year of final unhobbling to fully unlock it.'

--------

## Chunk 313

**Chunk:**

Dwarkesh Patel
At this point, we have AGI that’s rapidly accelerating productivity.
Leopold Aschenbrenner
The trillion-dollar cluster will be planned before the AGI. You get the AGI on the 10 GW cluster. Maybe have one more year of final unhobbling to fully unlock it. Then you have the intelligence explosion.

Meanwhile, the trillion-dollar cluster is almost finished. You run your superintelligence on it. You also have hundreds of millions of GPUs on inference clusters everywhere.
Dwarkesh Patel
In this world, I think private companies have their capital and can raise capital.

**Extracted Belief:**

The emergence of AGI will trigger an 'intelligence explosion,' a period of rapid advancement in artificial intelligence.

**Context:**

Leopold Aschenbrenner is discussing the expected consequences of achieving AGI.

**Justification:**

This belief is based on the idea that AGI will lead to a dramatic acceleration in technological development and progress. He states that 'Then you have the intelligence explosion.'

--------

## Chunk 314

**Chunk:**

Dwarkesh Patel
At this point, we have AGI that’s rapidly accelerating productivity.
Leopold Aschenbrenner
The trillion-dollar cluster will be planned before the AGI. You get the AGI on the 10 GW cluster. Maybe have one more year of final unhobbling to fully unlock it. Then you have the intelligence explosion.

Meanwhile, the trillion-dollar cluster is almost finished. You run your superintelligence on it. You also have hundreds of millions of GPUs on inference clusters everywhere.
Dwarkesh Patel
In this world, I think private companies have their capital and can raise capital.

**Extracted Belief:**

The trillion-dollar computing cluster will be nearly complete by the time AGI emerges.

**Context:**

Leopold Aschenbrenner is discussing the expected timeline for the completion of the computing cluster.

**Justification:**

This belief is based on the presumed timing of the cluster's completion relative to AGI's emergence. He states that 'Meanwhile, the trillion-dollar cluster is almost finished.'

--------

## Chunk 315

**Chunk:**

Dwarkesh Patel
At this point, we have AGI that’s rapidly accelerating productivity.
Leopold Aschenbrenner
The trillion-dollar cluster will be planned before the AGI. You get the AGI on the 10 GW cluster. Maybe have one more year of final unhobbling to fully unlock it. Then you have the intelligence explosion.

Meanwhile, the trillion-dollar cluster is almost finished. You run your superintelligence on it. You also have hundreds of millions of GPUs on inference clusters everywhere.
Dwarkesh Patel
In this world, I think private companies have their capital and can raise capital.

**Extracted Belief:**

The trillion-dollar computing cluster will be used to run superintelligence.

**Context:**

Leopold Aschenbrenner is describing the intended use of the trillion-dollar cluster.

**Justification:**

This belief is based on the expected application of the cluster for running advanced AI systems. He states that 'You run your superintelligence on it.'

--------

## Chunk 316

**Chunk:**

Dwarkesh Patel
At this point, we have AGI that’s rapidly accelerating productivity.
Leopold Aschenbrenner
The trillion-dollar cluster will be planned before the AGI. You get the AGI on the 10 GW cluster. Maybe have one more year of final unhobbling to fully unlock it. Then you have the intelligence explosion.

Meanwhile, the trillion-dollar cluster is almost finished. You run your superintelligence on it. You also have hundreds of millions of GPUs on inference clusters everywhere.
Dwarkesh Patel
In this world, I think private companies have their capital and can raise capital.

**Extracted Belief:**

There will be a significant number of Graphics Processing Units (GPUs) deployed for inference clusters, likely hundreds of millions.

**Context:**

Leopold Aschenbrenner is discussing the infrastructure requirements for deploying AGI.

**Justification:**

This belief is based on the anticipated scale of GPU deployment for inference tasks related to AI. He states that 'You also have hundreds of millions of GPUs on inference clusters everywhere.'

--------

## Chunk 317

**Chunk:**

Dwarkesh Patel
In this world, I think private companies have their capital and can raise capital.
Leopold Aschenbrenner
You will need the government to do it fast.
Dwarkesh Patel
We know private companies are on track to do this. In China, if they’re unhindered by climate change or whatever—

**Extracted Belief:**

Government involvement is necessary for the rapid development of large-scale computing infrastructure.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement that private companies have the capital to develop such infrastructure, suggesting that government involvement is essential for the speed required.

**Justification:**

The conversation implies that the development of this infrastructure requires significant resources and a faster timeline than what private companies can typically manage. This is supported by the broader context where Leopold Aschenbrenner previously mentioned the need for a 'trillion-dollar cluster' and a '10 GW cluster' for AGI development.

--------

## Chunk 318

**Chunk:**

Dwarkesh Patel
We know private companies are on track to do this. In China, if they’re unhindered by climate change or whatever—
Leopold Aschenbrenner
That’s part of what I’m saying.
Dwarkesh Patel
If it really matters that we beat China…

There will be all sorts of practical difficulties. Will the AI researchers actually join the AI effort? If they do, there will be at least three different teams currently doing pre-training in different companies.

Who decides at some  point that you’re going to have to YOLO the hyperparameters. Who decides that? Merging extremely complicated research and development processes across very different organizations is somehow supposed to speed up America against the Chinese?

**Extracted Belief:**

The development of artificial general intelligence (AGI) is significantly influenced by the availability of computing resources.

**Context:**

Leopold Aschenbrenner states that the development of AGI requires substantial computing power, implying that computational resources are crucial for achieving AGI.

**Justification:**

He mentions the need for a "trillion-dollar cluster" for AGI development, implying that the scale of computing infrastructure is a significant factor in achieving AGI.

--------

## Chunk 319

**Chunk:**

Dwarkesh Patel
If it really matters that we beat China…

There will be all sorts of practical difficulties. Will the AI researchers actually join the AI effort? If they do, there will be at least three different teams currently doing pre-training in different companies.

Who decides at some  point that you’re going to have to YOLO the hyperparameters. Who decides that? Merging extremely complicated research and development processes across very different organizations is somehow supposed to speed up America against the Chinese?
Leopold Aschenbrenner
Brain and DeepMind merged. It was a little messy, but it was fine.
Dwarkesh Patel
It was pretty messy. It was also the same company and much earlier on in the process.

**Extracted Belief:**

Merging different AI research and development teams from different organizations can be a messy process, but it can still be successful.

**Context:**

Leopold Aschenbrenner uses the example of the Brain and DeepMind merger to illustrate this belief. He is referring to his experience in the AI industry as justification for this belief. 

**Justification:**

The Brain and DeepMind merger was a real event in the AI industry, and Leopold Aschenbrenner's statement implies that he has firsthand knowledge of this process and its outcome. He states that the merger was 'messy' but 'fine', implying that there were difficulties but the outcome was ultimately successful.

--------

## Chunk 320

**Chunk:**

Dwarkesh Patel
It was pretty messy. It was also the same company and much earlier on in the process.
Leopold Aschenbrenner
Pretty similar, right? Different codebases, lots of different infrastructure and teams. It wasn’t the smoothest process, but DeepMind is doing very well.
Dwarkesh Patel
You give the example of COVID. In the COVID example, we woke up to it, maybe it was late, but then we deployed all this money. The COVID response from the government was a clusterfuck. I agree that Warp Speed was enabled by the government, but it was literally just giving permission that you can actually—

**Extracted Belief:**

The merger of Brain and DeepMind was not the smoothest process, but DeepMind is currently doing very well.

**Context:**

Leopold Aschenbrenner is responding to a statement by Dwarkesh Patel about the messy nature of the Brain and DeepMind merger. He acknowledges the difficulties but expresses confidence in DeepMind's success.

**Justification:**

He directly states that the merger was not smooth, implying his knowledge of the process and its complexities. However, he affirms DeepMind's current success as evidence of its effectiveness.

--------

## Chunk 321

**Chunk:**

Dwarkesh Patel
You give the example of COVID. In the COVID example, we woke up to it, maybe it was late, but then we deployed all this money. The COVID response from the government was a clusterfuck. I agree that Warp Speed was enabled by the government, but it was literally just giving permission that you can actually—
Leopold Aschenbrenner
It was also making big advance market commitments.
Dwarkesh Patel
I agree. But fundamentally, it was a private sector-led effort. That was the only part of the COVID response that worked.

**Extracted Belief:**

The government's involvement in Operation Warp Speed went beyond simply granting permission to proceed with vaccine development. It also included making significant advance market commitments.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement that Operation Warp Speed was primarily a government-led initiative. He clarifies that the government also made financial commitments beyond simply giving permission.

**Justification:**

Leopold Aschenbrenner states 'It was also making big advance market commitments.' This suggests that the government's role went beyond simply giving permission, implying they made financial commitments.

--------

## Chunk 322

**Chunk:**

Dwarkesh Patel
I agree. But fundamentally, it was a private sector-led effort. That was the only part of the COVID response that worked.
Leopold Aschenbrenner
The project will look closer to Operation Warp Speed. You’ll have all the companies involved in the government project. I’m not convinced that merging is that difficult. You run pre-training on GPUs with one codebase, then do the secondary step on the other codebase with TPUs. It’s fine.

On whether people will sign up for it, they wouldn’t sign up for it today. It would seem crazy to people.

But this is part of the secrecy thing. People gather at parties and… you know this. I don’t think anyone has really gotten up in front of these people and said, "look, what you’re building is the most important thing for the national security of the United States, for the future of the free world and whether we have another century ahead of it. This is really important for your country and democracy. Don’t talk about the secrets." It’s not just about DeepMind or whatever. It’s about these really important things.

We’re talking about the Manhattan Project. It was really contentious initially, but at some point it became clear that this was coming. There was an exigency on the military national security front. A lot of people will come around.

On whether it’ll be competent, a lot of this stuff is more predictive. This is reasonably likely, and not enough people are thinking about it. A lot of people think about AI lab politics but nobody has a plan for the grand project.
Dwarkesh Patel
Should they be more pessimistic about it? We don’t have a plan for it, and we need to act soon because AGI is upon us. The only competent technical institutions capable of making AI right now are private.

**Extracted Belief:**

Merging different codebases and infrastructure for AI development is not a difficult task.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement that the private sector led the COVID response effort, and he believes that a similar model will work for AI development.

**Justification:**

He cites the example of running pre-training on GPUs with one codebase and the secondary step on another codebase with TPUs as evidence for his claim. This suggests an understanding of the technical aspects of AI development.

--------

## Chunk 323

**Chunk:**

Dwarkesh Patel
I agree. But fundamentally, it was a private sector-led effort. That was the only part of the COVID response that worked.
Leopold Aschenbrenner
The project will look closer to Operation Warp Speed. You’ll have all the companies involved in the government project. I’m not convinced that merging is that difficult. You run pre-training on GPUs with one codebase, then do the secondary step on the other codebase with TPUs. It’s fine.

On whether people will sign up for it, they wouldn’t sign up for it today. It would seem crazy to people.

But this is part of the secrecy thing. People gather at parties and… you know this. I don’t think anyone has really gotten up in front of these people and said, "look, what you’re building is the most important thing for the national security of the United States, for the future of the free world and whether we have another century ahead of it. This is really important for your country and democracy. Don’t talk about the secrets." It’s not just about DeepMind or whatever. It’s about these really important things.

We’re talking about the Manhattan Project. It was really contentious initially, but at some point it became clear that this was coming. There was an exigency on the military national security front. A lot of people will come around.

On whether it’ll be competent, a lot of this stuff is more predictive. This is reasonably likely, and not enough people are thinking about it. A lot of people think about AI lab politics but nobody has a plan for the grand project.
Dwarkesh Patel
Should they be more pessimistic about it? We don’t have a plan for it, and we need to act soon because AGI is upon us. The only competent technical institutions capable of making AI right now are private.

**Extracted Belief:**

People are not aware of the importance of AI development for national security and the future of democracy.

**Context:**

Leopold Aschenbrenner is discussing the secrecy surrounding AI development and the lack of understanding about its importance.

**Justification:**

He states that nobody has told AI developers that their work is crucial for national security and democracy. This is based on his personal observation and experience within the AI community.

--------

## Chunk 324

**Chunk:**

Dwarkesh Patel
I agree. But fundamentally, it was a private sector-led effort. That was the only part of the COVID response that worked.
Leopold Aschenbrenner
The project will look closer to Operation Warp Speed. You’ll have all the companies involved in the government project. I’m not convinced that merging is that difficult. You run pre-training on GPUs with one codebase, then do the secondary step on the other codebase with TPUs. It’s fine.

On whether people will sign up for it, they wouldn’t sign up for it today. It would seem crazy to people.

But this is part of the secrecy thing. People gather at parties and… you know this. I don’t think anyone has really gotten up in front of these people and said, "look, what you’re building is the most important thing for the national security of the United States, for the future of the free world and whether we have another century ahead of it. This is really important for your country and democracy. Don’t talk about the secrets." It’s not just about DeepMind or whatever. It’s about these really important things.

We’re talking about the Manhattan Project. It was really contentious initially, but at some point it became clear that this was coming. There was an exigency on the military national security front. A lot of people will come around.

On whether it’ll be competent, a lot of this stuff is more predictive. This is reasonably likely, and not enough people are thinking about it. A lot of people think about AI lab politics but nobody has a plan for the grand project.
Dwarkesh Patel
Should they be more pessimistic about it? We don’t have a plan for it, and we need to act soon because AGI is upon us. The only competent technical institutions capable of making AI right now are private.

**Extracted Belief:**

The importance of AI development for national security and the future of democracy will become clear over time.

**Context:**

Leopold Aschenbrenner is drawing a comparison between the Manhattan Project and AI development.

**Justification:**

He states that the Manhattan Project was initially contentious but later became clear as a necessity for national security. He implies that a similar situation will unfold with AI development.

--------

## Chunk 325

**Chunk:**

Dwarkesh Patel
I agree. But fundamentally, it was a private sector-led effort. That was the only part of the COVID response that worked.
Leopold Aschenbrenner
The project will look closer to Operation Warp Speed. You’ll have all the companies involved in the government project. I’m not convinced that merging is that difficult. You run pre-training on GPUs with one codebase, then do the secondary step on the other codebase with TPUs. It’s fine.

On whether people will sign up for it, they wouldn’t sign up for it today. It would seem crazy to people.

But this is part of the secrecy thing. People gather at parties and… you know this. I don’t think anyone has really gotten up in front of these people and said, "look, what you’re building is the most important thing for the national security of the United States, for the future of the free world and whether we have another century ahead of it. This is really important for your country and democracy. Don’t talk about the secrets." It’s not just about DeepMind or whatever. It’s about these really important things.

We’re talking about the Manhattan Project. It was really contentious initially, but at some point it became clear that this was coming. There was an exigency on the military national security front. A lot of people will come around.

On whether it’ll be competent, a lot of this stuff is more predictive. This is reasonably likely, and not enough people are thinking about it. A lot of people think about AI lab politics but nobody has a plan for the grand project.
Dwarkesh Patel
Should they be more pessimistic about it? We don’t have a plan for it, and we need to act soon because AGI is upon us. The only competent technical institutions capable of making AI right now are private.

**Extracted Belief:**

The development of artificial general intelligence (AGI) is imminent.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement that the only competent technical institutions capable of making AI right now are private.

**Justification:**

He implicitly agrees with Dwarkesh Patel's statement about AGI's imminence by not contradicting him and continuing the discussion about how to prepare for it.

--------

## Chunk 326

**Chunk:**

Dwarkesh Patel
Should they be more pessimistic about it? We don’t have a plan for it, and we need to act soon because AGI is upon us. The only competent technical institutions capable of making AI right now are private.
Leopold Aschenbrenner
Companies will play that leading role. It’ll be a partnership.

We talked about World War II and American unpreparedness. The beginning of World War II was complete shambles. America has a very deep bench of incredibly competent managerial talent. There are a lot of dedicated people. An Operation Warp Speed-like public-private partnership is what I imagine it would look like.
Dwarkesh Patel
Recruiting talent is an interesting question. For the Manhattan Project, you initially had to convince people to beat the Nazis and get on board. Many of them regretted how much they accelerated the bomb. This is generally a thing with war.

**Extracted Belief:**

Companies will play a leading role in the development of artificial general intelligence (AGI) and it will be a partnership between the private and public sectors.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement that the only competent technical institutions capable of making AI right now are private, and that action needs to be taken soon because AGI is upon us.

**Justification:**

Aschenbrenner believes that companies, who are the only competent technical institutions capable of making AI right now, will be the leading force in AGI development, but that it will be a partnership between private and public sectors, likely similar to Operation Warp Speed. This is based on his experience with similar initiatives in the past, like the development of COVID-19 vaccines, and the successful deployment of resources during WWII.

--------

## Chunk 327

**Chunk:**

Dwarkesh Patel
Should they be more pessimistic about it? We don’t have a plan for it, and we need to act soon because AGI is upon us. The only competent technical institutions capable of making AI right now are private.
Leopold Aschenbrenner
Companies will play that leading role. It’ll be a partnership.

We talked about World War II and American unpreparedness. The beginning of World War II was complete shambles. America has a very deep bench of incredibly competent managerial talent. There are a lot of dedicated people. An Operation Warp Speed-like public-private partnership is what I imagine it would look like.
Dwarkesh Patel
Recruiting talent is an interesting question. For the Manhattan Project, you initially had to convince people to beat the Nazis and get on board. Many of them regretted how much they accelerated the bomb. This is generally a thing with war.

**Extracted Belief:**

The United States has a deep bench of talented managers and dedicated individuals who can be leveraged to lead and contribute to a large-scale initiative, such as an Operation Warp Speed-like project for AGI development.

**Context:**

Leopold Aschenbrenner is describing his vision for an effective AGI development project, drawing on historical examples and outlining the potential strengths of the US in this area.

**Justification:**

Aschenbrenner draws on the example of the US's successful mobilization in World War II to highlight the availability of talent and resources. He uses this historical example to support his belief that a similar approach could be effective for AGI development.

--------

## Chunk 328

**Chunk:**

Dwarkesh Patel
Recruiting talent is an interesting question. For the Manhattan Project, you initially had to convince people to beat the Nazis and get on board. Many of them regretted how much they accelerated the bomb. This is generally a thing with war.
Leopold Aschenbrenner
I think they were wrong to regret it.
Dwarkesh Patel
Why?

**Extracted Belief:**

The scientists who worked on the Manhattan Project were wrong to regret their contributions.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's observation that many scientists involved in the Manhattan Project later regretted their role in accelerating the development of the atomic bomb.

**Justification:**

Aschenbrenner expresses his belief by stating that they were 'wrong to regret it', implying he believes their regrets are unjustified.

--------

## Chunk 329

**Chunk:**

Dwarkesh Patel
Why?
Leopold Aschenbrenner
What’s the reason for regretting it?
Dwarkesh Patel
They way nuclear weapons were developed after the war was explosive because there was a precedent that you can use nuclear weapons. Then because of the race that was set up, you immediately go to the H-bomb.

**Extracted Belief:**

The development of nuclear weapons after World War II was inevitable, given the context of the war and the Cold War.

**Context:**

Leopold Aschenbrenner argues that the development of nuclear weapons was a logical consequence of World War II and the subsequent Cold War, citing the intense military focus and the inherent drive to pursue technological advancements in a competitive environment.

**Justification:**

The statement is based on logical reasoning that considers the historical context of World War II and the Cold War, highlighting the intense military focus and the drive to pursue technological advancements in a competitive environment.

--------

## Chunk 330

**Chunk:**

Dwarkesh Patel
They way nuclear weapons were developed after the war was explosive because there was a precedent that you can use nuclear weapons. Then because of the race that was set up, you immediately go to the H-bomb.
Leopold Aschenbrenner
This is related to my view on AI and maybe where we disagree. That was inevitable. There was a world war, then a cold war. Of course the military angle would be pursued with ferocious intensity. There’s no world in which we all decide not to build nukes. Also, nukes went really well. That could have gone terribly.

It’s not physically possible to have something like pocket nukes for everybody, where WMDs proliferated and were fully democratized. The US led on nukes and built a new world order, with a few great powers and a non-proliferation regime for nukes. It was a partnership and a deal: no military application of nuclear technology, but help with civilian technology. They enforced safety norms on the rest of the world. That worked and could have gone much worse.

Not to mention, I say this in the piece but the A-bomb in Hiroshima was just like firebombing. What changed the game was the H-bombs and ICBMs. That’s when it went to a whole new level.
Dwarkesh Patel
When you say we will tell people that we need to pursue this project for the free world to survive, it sounds similar to World War II. World War II is a sad story, not only because it happened, but the victory was sad in the sense that Britain went in to protect Poland.

At the end, the USSR, which as your family knows is incredibly brutal, ends up occupying half of Europe. The idea of protecting the free world by rushing AI might end up with an American AI Leviathan. We might look back on this with the same twisted irony as Britain going into World War II to protect Poland.

**Extracted Belief:**

The development of nuclear weapons after World War II was inevitable, given the context of the Cold War and the military interests of nations.

**Context:**

Leopold Aschenbrenner is comparing the development of nuclear weapons to the development of AI, arguing that both are driven by powerful forces and are difficult to avoid.

**Justification:**

He argues that the existence of a world war followed by a cold war made it almost inevitable that military powers would pursue the development of nuclear weapons with great intensity.

--------

## Chunk 331

**Chunk:**

Dwarkesh Patel
They way nuclear weapons were developed after the war was explosive because there was a precedent that you can use nuclear weapons. Then because of the race that was set up, you immediately go to the H-bomb.
Leopold Aschenbrenner
This is related to my view on AI and maybe where we disagree. That was inevitable. There was a world war, then a cold war. Of course the military angle would be pursued with ferocious intensity. There’s no world in which we all decide not to build nukes. Also, nukes went really well. That could have gone terribly.

It’s not physically possible to have something like pocket nukes for everybody, where WMDs proliferated and were fully democratized. The US led on nukes and built a new world order, with a few great powers and a non-proliferation regime for nukes. It was a partnership and a deal: no military application of nuclear technology, but help with civilian technology. They enforced safety norms on the rest of the world. That worked and could have gone much worse.

Not to mention, I say this in the piece but the A-bomb in Hiroshima was just like firebombing. What changed the game was the H-bombs and ICBMs. That’s when it went to a whole new level.
Dwarkesh Patel
When you say we will tell people that we need to pursue this project for the free world to survive, it sounds similar to World War II. World War II is a sad story, not only because it happened, but the victory was sad in the sense that Britain went in to protect Poland.

At the end, the USSR, which as your family knows is incredibly brutal, ends up occupying half of Europe. The idea of protecting the free world by rushing AI might end up with an American AI Leviathan. We might look back on this with the same twisted irony as Britain going into World War II to protect Poland.

**Extracted Belief:**

The development of nuclear weapons, despite the risks, ultimately turned out relatively well and could have been much worse.

**Context:**

He is defending the development of nuclear weapons, arguing that the risks were outweighed by the benefits.

**Justification:**

He claims that the US leadership in nuclear development led to a new world order with a few great powers and a non-proliferation regime, which he believes worked and could have gone much worse.

--------

## Chunk 332

**Chunk:**

Dwarkesh Patel
They way nuclear weapons were developed after the war was explosive because there was a precedent that you can use nuclear weapons. Then because of the race that was set up, you immediately go to the H-bomb.
Leopold Aschenbrenner
This is related to my view on AI and maybe where we disagree. That was inevitable. There was a world war, then a cold war. Of course the military angle would be pursued with ferocious intensity. There’s no world in which we all decide not to build nukes. Also, nukes went really well. That could have gone terribly.

It’s not physically possible to have something like pocket nukes for everybody, where WMDs proliferated and were fully democratized. The US led on nukes and built a new world order, with a few great powers and a non-proliferation regime for nukes. It was a partnership and a deal: no military application of nuclear technology, but help with civilian technology. They enforced safety norms on the rest of the world. That worked and could have gone much worse.

Not to mention, I say this in the piece but the A-bomb in Hiroshima was just like firebombing. What changed the game was the H-bombs and ICBMs. That’s when it went to a whole new level.
Dwarkesh Patel
When you say we will tell people that we need to pursue this project for the free world to survive, it sounds similar to World War II. World War II is a sad story, not only because it happened, but the victory was sad in the sense that Britain went in to protect Poland.

At the end, the USSR, which as your family knows is incredibly brutal, ends up occupying half of Europe. The idea of protecting the free world by rushing AI might end up with an American AI Leviathan. We might look back on this with the same twisted irony as Britain going into World War II to protect Poland.

**Extracted Belief:**

The use of the A-bomb in Hiroshima was comparable to firebombing in terms of its destructive power, and the real game-changer was the development of H-bombs and ICBMs.

**Context:**

He is highlighting the difference in the level of threat posed by different types of nuclear weapons.

**Justification:**

He mentions that he said this in his piece, suggesting it is a widely held belief or a point he has previously discussed.

--------

## Chunk 333

**Chunk:**

Dwarkesh Patel
When you say we will tell people that we need to pursue this project for the free world to survive, it sounds similar to World War II. World War II is a sad story, not only because it happened, but the victory was sad in the sense that Britain went in to protect Poland.

At the end, the USSR, which as your family knows is incredibly brutal, ends up occupying half of Europe. The idea of protecting the free world by rushing AI might end up with an American AI Leviathan. We might look back on this with the same twisted irony as Britain going into World War II to protect Poland.
Leopold Aschenbrenner
There will be a lot of unfortunate things that happen. I'm just hoping we make it through. The pitch won’t only be about the race. The race will be a backdrop. It's important that democracy shapes this technology. We can't leak this stuff to North Korea.

Safety, including alignment and the creation of new WMDs, is also just  important. I'm not convinced there's another path. Say we have a breakneck race internationally, instantly leaking all this stuff, including the weights, with a commercial race with Demis, Dario, and Sam all wanting to be first. It's incredibly rough for safety.

Safety regulation, as people talk about it, is like NIST involving years of bureaucracy and expert consensus.
Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?

**Extracted Belief:**

A lot of unfortunate things will happen during the development and advancement of AI.

**Context:**

Leopold Aschenbrenner is expressing his concern about the potential negative consequences of the rapid development of AI, drawing a parallel to the unfortunate outcomes of World War II.

**Justification:**

Leopold Aschenbrenner is expressing his personal opinion and drawing a connection to the history of World War II, where many unfortunate events occurred. He is using this historical parallel to highlight the potential risks associated with AI development.

--------

## Chunk 334

**Chunk:**

Dwarkesh Patel
When you say we will tell people that we need to pursue this project for the free world to survive, it sounds similar to World War II. World War II is a sad story, not only because it happened, but the victory was sad in the sense that Britain went in to protect Poland.

At the end, the USSR, which as your family knows is incredibly brutal, ends up occupying half of Europe. The idea of protecting the free world by rushing AI might end up with an American AI Leviathan. We might look back on this with the same twisted irony as Britain going into World War II to protect Poland.
Leopold Aschenbrenner
There will be a lot of unfortunate things that happen. I'm just hoping we make it through. The pitch won’t only be about the race. The race will be a backdrop. It's important that democracy shapes this technology. We can't leak this stuff to North Korea.

Safety, including alignment and the creation of new WMDs, is also just  important. I'm not convinced there's another path. Say we have a breakneck race internationally, instantly leaking all this stuff, including the weights, with a commercial race with Demis, Dario, and Sam all wanting to be first. It's incredibly rough for safety.

Safety regulation, as people talk about it, is like NIST involving years of bureaucracy and expert consensus.
Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?

**Extracted Belief:**

Democracy must play a significant role in shaping the development of AI technology.

**Context:**

Leopold Aschenbrenner emphasizes the importance of democratic values in guiding the development of AI, arguing that it should not be solely driven by competition or military interests.

**Justification:**

Leopold Aschenbrenner believes that democracy is crucial in ensuring ethical and responsible development of AI, preventing potential misuse and ensuring that the technology benefits all of society.

--------

## Chunk 335

**Chunk:**

Dwarkesh Patel
When you say we will tell people that we need to pursue this project for the free world to survive, it sounds similar to World War II. World War II is a sad story, not only because it happened, but the victory was sad in the sense that Britain went in to protect Poland.

At the end, the USSR, which as your family knows is incredibly brutal, ends up occupying half of Europe. The idea of protecting the free world by rushing AI might end up with an American AI Leviathan. We might look back on this with the same twisted irony as Britain going into World War II to protect Poland.
Leopold Aschenbrenner
There will be a lot of unfortunate things that happen. I'm just hoping we make it through. The pitch won’t only be about the race. The race will be a backdrop. It's important that democracy shapes this technology. We can't leak this stuff to North Korea.

Safety, including alignment and the creation of new WMDs, is also just  important. I'm not convinced there's another path. Say we have a breakneck race internationally, instantly leaking all this stuff, including the weights, with a commercial race with Demis, Dario, and Sam all wanting to be first. It's incredibly rough for safety.

Safety regulation, as people talk about it, is like NIST involving years of bureaucracy and expert consensus.
Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?

**Extracted Belief:**

It is imperative to prevent the leakage of AI technology and related information to countries like North Korea.

**Context:**

Leopold Aschenbrenner expresses concern over the potential for the misuse of AI technology by hostile actors, specifically mentioning North Korea as an example.

**Justification:**

Leopold Aschenbrenner believes that the development of AI technology should be carefully controlled and regulated to prevent its access by rogue states like North Korea, which could potentially use it for harmful purposes.

--------

## Chunk 336

**Chunk:**

Dwarkesh Patel
When you say we will tell people that we need to pursue this project for the free world to survive, it sounds similar to World War II. World War II is a sad story, not only because it happened, but the victory was sad in the sense that Britain went in to protect Poland.

At the end, the USSR, which as your family knows is incredibly brutal, ends up occupying half of Europe. The idea of protecting the free world by rushing AI might end up with an American AI Leviathan. We might look back on this with the same twisted irony as Britain going into World War II to protect Poland.
Leopold Aschenbrenner
There will be a lot of unfortunate things that happen. I'm just hoping we make it through. The pitch won’t only be about the race. The race will be a backdrop. It's important that democracy shapes this technology. We can't leak this stuff to North Korea.

Safety, including alignment and the creation of new WMDs, is also just  important. I'm not convinced there's another path. Say we have a breakneck race internationally, instantly leaking all this stuff, including the weights, with a commercial race with Demis, Dario, and Sam all wanting to be first. It's incredibly rough for safety.

Safety regulation, as people talk about it, is like NIST involving years of bureaucracy and expert consensus.
Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?

**Extracted Belief:**

Safety is of paramount importance in the development of AI, especially regarding alignment and the prevention of the creation of new weapons of mass destruction (WMDs).

**Context:**

Leopold Aschenbrenner highlights the importance of safety considerations in the development of AI, particularly addressing alignment and the potential for the creation of new WMDs.

**Justification:**

Leopold Aschenbrenner believes that the risks associated with AI are significant, requiring strict safety protocols and careful attention to alignment to prevent the creation of potentially dangerous technologies.

--------

## Chunk 337

**Chunk:**

Dwarkesh Patel
When you say we will tell people that we need to pursue this project for the free world to survive, it sounds similar to World War II. World War II is a sad story, not only because it happened, but the victory was sad in the sense that Britain went in to protect Poland.

At the end, the USSR, which as your family knows is incredibly brutal, ends up occupying half of Europe. The idea of protecting the free world by rushing AI might end up with an American AI Leviathan. We might look back on this with the same twisted irony as Britain going into World War II to protect Poland.
Leopold Aschenbrenner
There will be a lot of unfortunate things that happen. I'm just hoping we make it through. The pitch won’t only be about the race. The race will be a backdrop. It's important that democracy shapes this technology. We can't leak this stuff to North Korea.

Safety, including alignment and the creation of new WMDs, is also just  important. I'm not convinced there's another path. Say we have a breakneck race internationally, instantly leaking all this stuff, including the weights, with a commercial race with Demis, Dario, and Sam all wanting to be first. It's incredibly rough for safety.

Safety regulation, as people talk about it, is like NIST involving years of bureaucracy and expert consensus.
Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?

**Extracted Belief:**

The rapid pace of international competition in AI development poses a serious threat to safety.

**Context:**

Leopold Aschenbrenner expresses concern over the potential for a breakneck race in AI development, which could lead to the uncontrolled proliferation of AI technologies and compromise safety.

**Justification:**

Leopold Aschenbrenner believes that a fast-paced, competitive environment for AI development, where companies like DeepMind, OpenAI, and Google are vying for dominance, could lead to risks and compromises on safety protocols.

--------

## Chunk 338

**Chunk:**

Dwarkesh Patel
When you say we will tell people that we need to pursue this project for the free world to survive, it sounds similar to World War II. World War II is a sad story, not only because it happened, but the victory was sad in the sense that Britain went in to protect Poland.

At the end, the USSR, which as your family knows is incredibly brutal, ends up occupying half of Europe. The idea of protecting the free world by rushing AI might end up with an American AI Leviathan. We might look back on this with the same twisted irony as Britain going into World War II to protect Poland.
Leopold Aschenbrenner
There will be a lot of unfortunate things that happen. I'm just hoping we make it through. The pitch won’t only be about the race. The race will be a backdrop. It's important that democracy shapes this technology. We can't leak this stuff to North Korea.

Safety, including alignment and the creation of new WMDs, is also just  important. I'm not convinced there's another path. Say we have a breakneck race internationally, instantly leaking all this stuff, including the weights, with a commercial race with Demis, Dario, and Sam all wanting to be first. It's incredibly rough for safety.

Safety regulation, as people talk about it, is like NIST involving years of bureaucracy and expert consensus.
Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?

**Extracted Belief:**

Traditional safety regulations, such as those involving years of bureaucracy and expert consensus, are insufficient for addressing the rapidly evolving nature of AI development.

**Context:**

Leopold Aschenbrenner suggests that traditional safety regulations are not effective in the context of AI development, which is rapidly advancing and requires a more agile approach.

**Justification:**

Leopold Aschenbrenner argues that the traditional regulatory framework for safety, characterized by lengthy bureaucratic processes and consensus-building, is inadequate for addressing the rapid pace of AI development and the need for swift decision-making.

--------

## Chunk 339

**Chunk:**

Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?
Leopold Aschenbrenner
Alignment during the intelligence explosion is not a years-long bureaucratic process. It's more like a war, with a fog of war. Is it safe to do the next OOM? We're three OOMs into the intelligence explosion, and we don't fully understand what's happening.

Our generalization-scaling curves don’t look great, some automated AI researchers say it's fine, but we don't quite trust them. AIs might start doing problematic things, but we hammer it out, and then it's fine. Should we go ahead? Should we take another six months?

Meanwhile, China might steal the weights or deploy a robot army. It's a crazy situation, relying more on a sane chain of command than a deliberative regulatory scheme. Although I wish we could do that more deliberative regulatory scheme.

This is the thing with private companies too. Private companies claim they'll do safety, but it’s rough in a commercial race, especially for startups. Startups are startups. They aren't fit to handle WMDs.
Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.

**Extracted Belief:**

Alignment during the intelligence explosion is not a process that can be achieved through years-long bureaucratic procedures.

**Context:**

Leopold Aschenbrenner is expressing his belief about the nature of AI alignment in the context of the intelligence explosion.

**Justification:**

Aschenbrenner argues that the rapid pace of AI development necessitates a more agile approach to alignment, likening it to a 'war' with a 'fog of war'.

--------

## Chunk 340

**Chunk:**

Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?
Leopold Aschenbrenner
Alignment during the intelligence explosion is not a years-long bureaucratic process. It's more like a war, with a fog of war. Is it safe to do the next OOM? We're three OOMs into the intelligence explosion, and we don't fully understand what's happening.

Our generalization-scaling curves don’t look great, some automated AI researchers say it's fine, but we don't quite trust them. AIs might start doing problematic things, but we hammer it out, and then it's fine. Should we go ahead? Should we take another six months?

Meanwhile, China might steal the weights or deploy a robot army. It's a crazy situation, relying more on a sane chain of command than a deliberative regulatory scheme. Although I wish we could do that more deliberative regulatory scheme.

This is the thing with private companies too. Private companies claim they'll do safety, but it’s rough in a commercial race, especially for startups. Startups are startups. They aren't fit to handle WMDs.
Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.

**Extracted Belief:**

Current understanding of AI development and its potential risks is insufficient to confidently determine the safety of further advancements.

**Context:**

Aschenbrenner is raising concerns about the limitations of current knowledge regarding AI safety in the context of the intelligence explosion.

**Justification:**

Aschenbrenner states that they are 'three OOMs into the intelligence explosion, and we don't fully understand what's happening.' This suggests a lack of complete understanding of AI's behavior and potential risks.

--------

## Chunk 341

**Chunk:**

Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?
Leopold Aschenbrenner
Alignment during the intelligence explosion is not a years-long bureaucratic process. It's more like a war, with a fog of war. Is it safe to do the next OOM? We're three OOMs into the intelligence explosion, and we don't fully understand what's happening.

Our generalization-scaling curves don’t look great, some automated AI researchers say it's fine, but we don't quite trust them. AIs might start doing problematic things, but we hammer it out, and then it's fine. Should we go ahead? Should we take another six months?

Meanwhile, China might steal the weights or deploy a robot army. It's a crazy situation, relying more on a sane chain of command than a deliberative regulatory scheme. Although I wish we could do that more deliberative regulatory scheme.

This is the thing with private companies too. Private companies claim they'll do safety, but it’s rough in a commercial race, especially for startups. Startups are startups. They aren't fit to handle WMDs.
Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.

**Extracted Belief:**

Generalization-scaling curves in AI development do not provide adequate reassurance about the safety of further advancements.

**Context:**

Aschenbrenner is expressing concerns about the reliability of existing data and metrics in predicting AI safety.

**Justification:**

Aschenbrenner mentions that 'generalization-scaling curves don’t look great' and expresses distrust in some automated AI researchers' claims that everything is fine.

--------

## Chunk 342

**Chunk:**

Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?
Leopold Aschenbrenner
Alignment during the intelligence explosion is not a years-long bureaucratic process. It's more like a war, with a fog of war. Is it safe to do the next OOM? We're three OOMs into the intelligence explosion, and we don't fully understand what's happening.

Our generalization-scaling curves don’t look great, some automated AI researchers say it's fine, but we don't quite trust them. AIs might start doing problematic things, but we hammer it out, and then it's fine. Should we go ahead? Should we take another six months?

Meanwhile, China might steal the weights or deploy a robot army. It's a crazy situation, relying more on a sane chain of command than a deliberative regulatory scheme. Although I wish we could do that more deliberative regulatory scheme.

This is the thing with private companies too. Private companies claim they'll do safety, but it’s rough in a commercial race, especially for startups. Startups are startups. They aren't fit to handle WMDs.
Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.

**Extracted Belief:**

The risk of malicious actors, such as China, stealing or weaponizing AI technology is a significant threat to global safety.

**Context:**

Aschenbrenner is expressing concerns about the potential for AI technology to be misused by foreign powers.

**Justification:**

Aschenbrenner refers to the possibility of China 'stealing the weights or deploying a robot army,' highlighting the potential for weaponization and misuse of AI technology by foreign actors.

--------

## Chunk 343

**Chunk:**

Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?
Leopold Aschenbrenner
Alignment during the intelligence explosion is not a years-long bureaucratic process. It's more like a war, with a fog of war. Is it safe to do the next OOM? We're three OOMs into the intelligence explosion, and we don't fully understand what's happening.

Our generalization-scaling curves don’t look great, some automated AI researchers say it's fine, but we don't quite trust them. AIs might start doing problematic things, but we hammer it out, and then it's fine. Should we go ahead? Should we take another six months?

Meanwhile, China might steal the weights or deploy a robot army. It's a crazy situation, relying more on a sane chain of command than a deliberative regulatory scheme. Although I wish we could do that more deliberative regulatory scheme.

This is the thing with private companies too. Private companies claim they'll do safety, but it’s rough in a commercial race, especially for startups. Startups are startups. They aren't fit to handle WMDs.
Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.

**Extracted Belief:**

The existing regulatory frameworks for AI safety are not sufficiently responsive to the rapid pace of AI development.

**Context:**

Aschenbrenner is criticizing the current regulatory approach to AI safety.

**Justification:**

Aschenbrenner contrasts the bureaucratic nature of traditional regulatory processes with the rapid pace of AI development, suggesting that the current system is inadequate for addressing the potential risks posed by AI.

--------

## Chunk 344

**Chunk:**

Dwarkesh Patel
Isn’t that what’s going to happen with the project as well?
Leopold Aschenbrenner
Alignment during the intelligence explosion is not a years-long bureaucratic process. It's more like a war, with a fog of war. Is it safe to do the next OOM? We're three OOMs into the intelligence explosion, and we don't fully understand what's happening.

Our generalization-scaling curves don’t look great, some automated AI researchers say it's fine, but we don't quite trust them. AIs might start doing problematic things, but we hammer it out, and then it's fine. Should we go ahead? Should we take another six months?

Meanwhile, China might steal the weights or deploy a robot army. It's a crazy situation, relying more on a sane chain of command than a deliberative regulatory scheme. Although I wish we could do that more deliberative regulatory scheme.

This is the thing with private companies too. Private companies claim they'll do safety, but it’s rough in a commercial race, especially for startups. Startups are startups. They aren't fit to handle WMDs.
Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.

**Extracted Belief:**

Private companies are not adequately prepared or equipped to handle the risks associated with the development of powerful AI systems, particularly in a competitive environment.

**Context:**

Aschenbrenner is expressing concerns about the capacity of private companies to prioritize safety in the development of powerful AI systems.

**Justification:**

Aschenbrenner states that 'private companies claim they'll do safety, but it’s rough in a commercial race, especially for startups' and that 'Startups are startups. They aren't fit to handle WMDs.' This suggests a lack of trust in private companies' ability to manage the safety of advanced AI technologies in a competitive environment.

--------

## Chunk 345

**Chunk:**

Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.
Leopold Aschenbrenner
Here’s what I think about RSP-type stuff and current safety regulations. They’re important for helping us figure out what world we’re in and flashing the warning signs when we’re close.

The story we’ve been telling is what I think the modal version of this decade is. There are many ways it could be wrong. We should talk about the data wall more. There’s a world where this stuff stagnates or we don’t have AGI.

The RSPs preserve optionality. Let’s see how things go, but we need to be prepared if the red lights start flashing. If we get the automated AI researcher, then it’s crunch time.
Dwarkesh Patel
I can be on the same page with that and have a strong prior on pursuing a market-based way. Unless you’re right about what the intelligence explosion looks like, don’t move yet. But in that world where it really does seem like Alec Radford can be automated, and that's the only bottleneck to getting to ASI…

Okay I think we can leave it at that. I’m somewhat on the way there.

**Extracted Belief:**

Responsible scaling policies (RSPs) are important for helping us understand the current state of AI development and providing early warning signs of potential risks.

**Context:**

Leopold Aschenbrenner discusses his perspective on RSPs and safety regulations, arguing for their importance in providing insights into the current state of AI development and acting as early warning systems.

**Justification:**

The belief is grounded in the practical application of RSPs to track and analyze AI progress, potentially identifying critical junctures or emerging risks.

--------

## Chunk 346

**Chunk:**

Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.
Leopold Aschenbrenner
Here’s what I think about RSP-type stuff and current safety regulations. They’re important for helping us figure out what world we’re in and flashing the warning signs when we’re close.

The story we’ve been telling is what I think the modal version of this decade is. There are many ways it could be wrong. We should talk about the data wall more. There’s a world where this stuff stagnates or we don’t have AGI.

The RSPs preserve optionality. Let’s see how things go, but we need to be prepared if the red lights start flashing. If we get the automated AI researcher, then it’s crunch time.
Dwarkesh Patel
I can be on the same page with that and have a strong prior on pursuing a market-based way. Unless you’re right about what the intelligence explosion looks like, don’t move yet. But in that world where it really does seem like Alec Radford can be automated, and that's the only bottleneck to getting to ASI…

Okay I think we can leave it at that. I’m somewhat on the way there.

**Extracted Belief:**

The current understanding of AI development is likely to be the most common interpretation of the next decade, but there are multiple possibilities.

**Context:**

Leopold Aschenbrenner expresses his view on the current narrative surrounding AI development, acknowledging that this is a dominant view but recognizes alternative scenarios.

**Justification:**

Leopold Aschenbrenner states that the 'story we've been telling' is a likely interpretation, implying that there are other perspectives on the future of AI.

--------

## Chunk 347

**Chunk:**

Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.
Leopold Aschenbrenner
Here’s what I think about RSP-type stuff and current safety regulations. They’re important for helping us figure out what world we’re in and flashing the warning signs when we’re close.

The story we’ve been telling is what I think the modal version of this decade is. There are many ways it could be wrong. We should talk about the data wall more. There’s a world where this stuff stagnates or we don’t have AGI.

The RSPs preserve optionality. Let’s see how things go, but we need to be prepared if the red lights start flashing. If we get the automated AI researcher, then it’s crunch time.
Dwarkesh Patel
I can be on the same page with that and have a strong prior on pursuing a market-based way. Unless you’re right about what the intelligence explosion looks like, don’t move yet. But in that world where it really does seem like Alec Radford can be automated, and that's the only bottleneck to getting to ASI…

Okay I think we can leave it at that. I’m somewhat on the way there.

**Extracted Belief:**

There is a potential scenario where AI development stagnates or fails to achieve artificial general intelligence (AGI).

**Context:**

Leopold Aschenbrenner acknowledges the possibility of AI development stalling or failing to achieve AGI, indicating that he considers this an alternative possibility to the prevailing narrative.

**Justification:**

Leopold Aschenbrenner mentions a scenario where 'this stuff stagnates' or 'we don't have AGI,' highlighting the existence of alternative outcomes beyond the current dominant narrative.

--------

## Chunk 348

**Chunk:**

Dwarkesh Patel
I’m coming closer to your position but…

Let’s talk about the responsible scaling policies. I was told by people advancing this idea — because they know I’m a libertarian-type person and the way they approached me was like this — that the way to think about it was that it's fundamentally a way to protect market-based development of AGI. If you didn’t have this, there would be misuse and lead to nationalization. The RSPs are a way to ensure a market-based order with safeguards to prevent things from going off the rails.

It seems like your story is self-consistent. I know this was never your position, so I’m not looping you into this. But it's almost like a motte-and-bailey argument.
Leopold Aschenbrenner
Here’s what I think about RSP-type stuff and current safety regulations. They’re important for helping us figure out what world we’re in and flashing the warning signs when we’re close.

The story we’ve been telling is what I think the modal version of this decade is. There are many ways it could be wrong. We should talk about the data wall more. There’s a world where this stuff stagnates or we don’t have AGI.

The RSPs preserve optionality. Let’s see how things go, but we need to be prepared if the red lights start flashing. If we get the automated AI researcher, then it’s crunch time.
Dwarkesh Patel
I can be on the same page with that and have a strong prior on pursuing a market-based way. Unless you’re right about what the intelligence explosion looks like, don’t move yet. But in that world where it really does seem like Alec Radford can be automated, and that's the only bottleneck to getting to ASI…

Okay I think we can leave it at that. I’m somewhat on the way there.

**Extracted Belief:**

RSPs are essential for preserving options in the face of uncertainty surrounding AI development, allowing for flexibility in responding to potential risks.

**Context:**

Leopold Aschenbrenner emphasizes the value of RSPs in maintaining options as the future of AI remains uncertain, emphasizing the importance of preparedness for potential risks.

**Justification:**

He argues that 'RSPs preserve optionality' and that we 'need to be prepared if the red lights start flashing,' indicating a logical approach to navigating AI development uncertainties.

--------

## Chunk 349

**Chunk:**

Dwarkesh Patel
I can be on the same page with that and have a strong prior on pursuing a market-based way. Unless you’re right about what the intelligence explosion looks like, don’t move yet. But in that world where it really does seem like Alec Radford can be automated, and that's the only bottleneck to getting to ASI…

Okay I think we can leave it at that. I’m somewhat on the way there.
Leopold Aschenbrenner
I hope it goes well. It's going to be very stressful. Right now is the chill time. Enjoy your vacation while it lasts.
Dwarkesh Patel
It’s funny to look out over this. This is San Francisco.

**Extracted Belief:**

The development of artificial general intelligence (AGI) will be stressful.

**Context:**

Leopold Aschenbrenner expresses his belief about the stressfulness of AGI development in response to Dwarkesh Patel's statement about being 'somewhat on the way' to understanding AGI.

**Justification:**

Aschenbrenner's personal experience and perspective on AGI development, particularly his statement 'It's going to be very stressful.'

--------

## Chunk 350

**Chunk:**

Dwarkesh Patel
It’s funny to look out over this. This is San Francisco.
Leopold Aschenbrenner
Yeah OpenAI is right there. Anthropic is there. You guys have this enormous power over how it’s going to go for the next couple of years, and that power is depreciating.
Dwarkesh Patel
Who's "you guys"?

**Extracted Belief:**

The power held by AI research labs like OpenAI and Anthropic over the development of AGI is significant and will likely decrease over time.

**Context:**

Leopold Aschenbrenner is acknowledging the influence of AI labs in San Francisco on the trajectory of AGI development, suggesting that their power is not static.

**Justification:**

Aschenbrenner references the physical location of OpenAI and Anthropic in San Francisco and their influence on the field. He states that their power is "depreciating", implying an observation of changing dynamics.

--------

## Chunk 351

**Chunk:**

Dwarkesh Patel
Who's "you guys"?
Leopold Aschenbrenner
People at labs.

It's a crazy world you're talking about. You mention that maybe they'll nationalize too soon. Almost nobody sees what's happening. This is what I find stressful about all this.

Maybe I'm wrong, but if I'm right, we’re in this crazy situation where only a few hundred guys are paying attention. It’s daunting.
Dwarkesh Patel
I went to Washington a few months ago. I was talking to people doing AI policy stuff there. I asked how likely they think nationalization is. They said it's really hard to nationalize stuff. It’s been a long time since it's been done. There are specific procedural constraints on what kinds of things can be nationalized.

Then I asked about ASI. Because of constraints like the Defense Production Act, that won’t be nationalized? The Supreme Court would overturn that? They were like, “yeah I guess that would be nationalized.”

**Extracted Belief:**

The current state of artificial intelligence research, specifically the development of advanced AI systems like artificial general intelligence (ASI), is not widely understood by the public or policymakers.

**Context:**

Leopold Aschenbrenner expresses concern about the lack of public understanding of AI development, particularly regarding the potential for rapid advancement.

**Justification:**

Aschenbrenner states "Almost nobody sees what's happening." and "only a few hundred guys are paying attention." implying that he believes most people are unaware of the potential implications of AI development.

--------

## Chunk 352

**Chunk:**

Dwarkesh Patel
Who's "you guys"?
Leopold Aschenbrenner
People at labs.

It's a crazy world you're talking about. You mention that maybe they'll nationalize too soon. Almost nobody sees what's happening. This is what I find stressful about all this.

Maybe I'm wrong, but if I'm right, we’re in this crazy situation where only a few hundred guys are paying attention. It’s daunting.
Dwarkesh Patel
I went to Washington a few months ago. I was talking to people doing AI policy stuff there. I asked how likely they think nationalization is. They said it's really hard to nationalize stuff. It’s been a long time since it's been done. There are specific procedural constraints on what kinds of things can be nationalized.

Then I asked about ASI. Because of constraints like the Defense Production Act, that won’t be nationalized? The Supreme Court would overturn that? They were like, “yeah I guess that would be nationalized.”

**Extracted Belief:**

The rapid development of AI poses a significant risk to society if it is not carefully managed, as it could lead to a scenario where a small group of individuals has disproportionate power and influence over its development and deployment.

**Context:**

Aschenbrenner expresses concern about the potential for a small group of individuals, specifically those working at AI labs, to exert significant influence over the future of AI.

**Justification:**

The statement "It’s daunting." suggests a perceived threat of unchecked power and influence concentrated in the hands of a few individuals.

--------

## Chunk 353

**Chunk:**

Dwarkesh Patel
I went to Washington a few months ago. I was talking to people doing AI policy stuff there. I asked how likely they think nationalization is. They said it's really hard to nationalize stuff. It’s been a long time since it's been done. There are specific procedural constraints on what kinds of things can be nationalized.

Then I asked about ASI. Because of constraints like the Defense Production Act, that won’t be nationalized? The Supreme Court would overturn that? They were like, “yeah I guess that would be nationalized.”
Leopold Aschenbrenner
That’s the short summary of my post or my view on the project.

(02:12:23) – Becoming Valedictorian of Columbia at 19
Dwarkesh Patel
Before we go further on the AI stuff, let’s back up.

We began the conversation, and I think people will be confused. You graduated valedictorian of Columbia when you were 19. So, you got to college when you were 15.

You were in Germany then, and you got to college at 15.

**Extracted Belief:**

The Defense Production Act, a law designed to increase production of certain goods during national emergencies, would likely not be sufficient to prevent the nationalization of advanced artificial intelligence (ASI).

**Context:**

Leopold Aschenbrenner is discussing the likelihood of ASI being nationalized, and he cites the opinion of AI policy experts in Washington, D.C. who believe that the Defense Production Act wouldn't prevent this.

**Justification:**

The justification for this belief is based on the testimony of AI policy experts in Washington, D.C. who believe that the Supreme Court would likely overturn any attempts to use the Defense Production Act to prevent the nationalization of ASI.

--------

## Chunk 354

**Chunk:**

Dwarkesh Patel
Before we go further on the AI stuff, let’s back up.

We began the conversation, and I think people will be confused. You graduated valedictorian of Columbia when you were 19. So, you got to college when you were 15.

You were in Germany then, and you got to college at 15.
Leopold Aschenbrenner
Yeah.
Dwarkesh Patel
How the fuck did that happen?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 355

**Chunk:**

Dwarkesh Patel
How the fuck did that happen?
Leopold Aschenbrenner
I really wanted out of Germany. I went to a German public school. It was not a good environment for me.
Dwarkesh Patel
In what sense? No peers?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 356

**Chunk:**

Dwarkesh Patel
In what sense? No peers?
Leopold Aschenbrenner
There’s also a particular German cultural sense. In the US, there are amazing high schools and an appreciation of excellence. In Germany, there’s a tall poppy syndrome. If you're the curious kid in class wanting to learn more, instead of the teacher encouraging you, they resent you and try to crush you.

There are also no elite universities for undergraduates, which is kind of crazy. The meritocracy was crushed in Germany at some point. There's also an incredible sense of complacency across the board. It always puzzles me but even going to a US college was seen as a radical act. It doesn’t seem radical to anyone here because it’s the obvious thing to do. You can go to Columbia and get a better education.

It’s wild to me because this is where stuff is happening and you can get a better education but people in Germany don’t do it. I skipped a few grades, and it seemed normal to me at the time to go to college at 15 and come to America. One of my sisters is turning 15 now, and when I look at her, I understand why my mother was worried.
Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?

**Extracted Belief:**

In the US, there are high schools with a strong emphasis on academic excellence.

**Context:**

Leopold Aschenbrenner is comparing the education systems in the US and Germany, highlighting his perception of a stronger emphasis on academic excellence in American high schools.

**Justification:**

He states, "In the US, there are amazing high schools and an appreciation of excellence."

--------

## Chunk 357

**Chunk:**

Dwarkesh Patel
In what sense? No peers?
Leopold Aschenbrenner
There’s also a particular German cultural sense. In the US, there are amazing high schools and an appreciation of excellence. In Germany, there’s a tall poppy syndrome. If you're the curious kid in class wanting to learn more, instead of the teacher encouraging you, they resent you and try to crush you.

There are also no elite universities for undergraduates, which is kind of crazy. The meritocracy was crushed in Germany at some point. There's also an incredible sense of complacency across the board. It always puzzles me but even going to a US college was seen as a radical act. It doesn’t seem radical to anyone here because it’s the obvious thing to do. You can go to Columbia and get a better education.

It’s wild to me because this is where stuff is happening and you can get a better education but people in Germany don’t do it. I skipped a few grades, and it seemed normal to me at the time to go to college at 15 and come to America. One of my sisters is turning 15 now, and when I look at her, I understand why my mother was worried.
Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?

**Extracted Belief:**

In Germany, there is a "tall poppy syndrome" where students who excel academically are resented and discouraged by teachers.

**Context:**

Leopold Aschenbrenner describes his experience with the educational culture in Germany and expresses his belief that this syndrome discourages academic ambition.

**Justification:**

He states, "In Germany, there’s a tall poppy syndrome. If you're the curious kid in class wanting to learn more, instead of the teacher encouraging you, they resent you and try to crush you."

--------

## Chunk 358

**Chunk:**

Dwarkesh Patel
In what sense? No peers?
Leopold Aschenbrenner
There’s also a particular German cultural sense. In the US, there are amazing high schools and an appreciation of excellence. In Germany, there’s a tall poppy syndrome. If you're the curious kid in class wanting to learn more, instead of the teacher encouraging you, they resent you and try to crush you.

There are also no elite universities for undergraduates, which is kind of crazy. The meritocracy was crushed in Germany at some point. There's also an incredible sense of complacency across the board. It always puzzles me but even going to a US college was seen as a radical act. It doesn’t seem radical to anyone here because it’s the obvious thing to do. You can go to Columbia and get a better education.

It’s wild to me because this is where stuff is happening and you can get a better education but people in Germany don’t do it. I skipped a few grades, and it seemed normal to me at the time to go to college at 15 and come to America. One of my sisters is turning 15 now, and when I look at her, I understand why my mother was worried.
Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?

**Extracted Belief:**

There are no elite universities for undergraduates in Germany.

**Context:**

Leopold Aschenbrenner expresses his opinion on the absence of elite universities in Germany, contrasting it with the presence of such institutions in the US.

**Justification:**

He states, "There are also no elite universities for undergraduates, which is kind of crazy."

--------

## Chunk 359

**Chunk:**

Dwarkesh Patel
In what sense? No peers?
Leopold Aschenbrenner
There’s also a particular German cultural sense. In the US, there are amazing high schools and an appreciation of excellence. In Germany, there’s a tall poppy syndrome. If you're the curious kid in class wanting to learn more, instead of the teacher encouraging you, they resent you and try to crush you.

There are also no elite universities for undergraduates, which is kind of crazy. The meritocracy was crushed in Germany at some point. There's also an incredible sense of complacency across the board. It always puzzles me but even going to a US college was seen as a radical act. It doesn’t seem radical to anyone here because it’s the obvious thing to do. You can go to Columbia and get a better education.

It’s wild to me because this is where stuff is happening and you can get a better education but people in Germany don’t do it. I skipped a few grades, and it seemed normal to me at the time to go to college at 15 and come to America. One of my sisters is turning 15 now, and when I look at her, I understand why my mother was worried.
Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?

**Extracted Belief:**

The meritocratic system in Germany has been weakened or destroyed.

**Context:**

Leopold Aschenbrenner expresses his belief that the German educational system does not adequately reward and support merit, contrasting this with the perceived meritocratic system in the US.

**Justification:**

He states, "The meritocracy was crushed in Germany at some point."

--------

## Chunk 360

**Chunk:**

Dwarkesh Patel
In what sense? No peers?
Leopold Aschenbrenner
There’s also a particular German cultural sense. In the US, there are amazing high schools and an appreciation of excellence. In Germany, there’s a tall poppy syndrome. If you're the curious kid in class wanting to learn more, instead of the teacher encouraging you, they resent you and try to crush you.

There are also no elite universities for undergraduates, which is kind of crazy. The meritocracy was crushed in Germany at some point. There's also an incredible sense of complacency across the board. It always puzzles me but even going to a US college was seen as a radical act. It doesn’t seem radical to anyone here because it’s the obvious thing to do. You can go to Columbia and get a better education.

It’s wild to me because this is where stuff is happening and you can get a better education but people in Germany don’t do it. I skipped a few grades, and it seemed normal to me at the time to go to college at 15 and come to America. One of my sisters is turning 15 now, and when I look at her, I understand why my mother was worried.
Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?

**Extracted Belief:**

There is a pervasive sense of complacency in German society.

**Context:**

Leopold Aschenbrenner observes a prevailing attitude of complacency among Germans, which he contrasts with the perceived drive for excellence in the US.

**Justification:**

He states, "There's also an incredible sense of complacency across the board."

--------

## Chunk 361

**Chunk:**

Dwarkesh Patel
In what sense? No peers?
Leopold Aschenbrenner
There’s also a particular German cultural sense. In the US, there are amazing high schools and an appreciation of excellence. In Germany, there’s a tall poppy syndrome. If you're the curious kid in class wanting to learn more, instead of the teacher encouraging you, they resent you and try to crush you.

There are also no elite universities for undergraduates, which is kind of crazy. The meritocracy was crushed in Germany at some point. There's also an incredible sense of complacency across the board. It always puzzles me but even going to a US college was seen as a radical act. It doesn’t seem radical to anyone here because it’s the obvious thing to do. You can go to Columbia and get a better education.

It’s wild to me because this is where stuff is happening and you can get a better education but people in Germany don’t do it. I skipped a few grades, and it seemed normal to me at the time to go to college at 15 and come to America. One of my sisters is turning 15 now, and when I look at her, I understand why my mother was worried.
Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?

**Extracted Belief:**

Attending a US college is considered a radical act in Germany.

**Context:**

Leopold Aschenbrenner describes the perceived attitude towards US education in Germany, contrasting it with the general acceptance of such pursuits in the US.

**Justification:**

He states, "It always puzzles me but even going to a US college was seen as a radical act. It doesn’t seem radical to anyone here because it’s the obvious thing to do."

--------

## Chunk 362

**Chunk:**

Dwarkesh Patel
In what sense? No peers?
Leopold Aschenbrenner
There’s also a particular German cultural sense. In the US, there are amazing high schools and an appreciation of excellence. In Germany, there’s a tall poppy syndrome. If you're the curious kid in class wanting to learn more, instead of the teacher encouraging you, they resent you and try to crush you.

There are also no elite universities for undergraduates, which is kind of crazy. The meritocracy was crushed in Germany at some point. There's also an incredible sense of complacency across the board. It always puzzles me but even going to a US college was seen as a radical act. It doesn’t seem radical to anyone here because it’s the obvious thing to do. You can go to Columbia and get a better education.

It’s wild to me because this is where stuff is happening and you can get a better education but people in Germany don’t do it. I skipped a few grades, and it seemed normal to me at the time to go to college at 15 and come to America. One of my sisters is turning 15 now, and when I look at her, I understand why my mother was worried.
Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?

**Extracted Belief:**

A US college education is superior to a German education.

**Context:**

Leopold Aschenbrenner contrasts the educational opportunities in the US and Germany, expressing his belief in the superiority of a US college education.

**Justification:**

He states, "You can go to Columbia and get a better education."

--------

## Chunk 363

**Chunk:**

Dwarkesh Patel
In what sense? No peers?
Leopold Aschenbrenner
There’s also a particular German cultural sense. In the US, there are amazing high schools and an appreciation of excellence. In Germany, there’s a tall poppy syndrome. If you're the curious kid in class wanting to learn more, instead of the teacher encouraging you, they resent you and try to crush you.

There are also no elite universities for undergraduates, which is kind of crazy. The meritocracy was crushed in Germany at some point. There's also an incredible sense of complacency across the board. It always puzzles me but even going to a US college was seen as a radical act. It doesn’t seem radical to anyone here because it’s the obvious thing to do. You can go to Columbia and get a better education.

It’s wild to me because this is where stuff is happening and you can get a better education but people in Germany don’t do it. I skipped a few grades, and it seemed normal to me at the time to go to college at 15 and come to America. One of my sisters is turning 15 now, and when I look at her, I understand why my mother was worried.
Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?

**Extracted Belief:**

Going to college at age 15 is normal.

**Context:**

Leopold Aschenbrenner shares his personal experience of attending college at an early age and expresses his belief that it felt normal at the time.

**Justification:**

He states, "I skipped a few grades, and it seemed normal to me at the time to go to college at 15 and come to America."

--------

## Chunk 364

**Chunk:**

Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?
Leopold Aschenbrenner
Again, it felt so normal at the time. Now I understand why my mother was worried. I worked on my parents for a while and eventually persuaded them. It felt very normal at the time.

It was great. I really liked college. It came at the right time for me. I really appreciated the liberal arts education, the core curriculum, and reading core works of political philosophy and literature.
Dwarkesh Patel
You did what? Econ?

**Extracted Belief:**

A liberal arts education is valuable.

**Context:**

Leopold Aschenbrenner is reflecting on his experience as a 15-year-old college student and expressing his appreciation for the liberal arts education he received.

**Justification:**

Leopold Aschenbrenner states, "I really appreciated the liberal arts education, the core curriculum, and reading core works of political philosophy and literature."

--------

## Chunk 365

**Chunk:**

Dwarkesh Patel
So you were presumably the only 15-year-old. Was it normal for you to be a 15-year-old in college? What were the initial years like?
Leopold Aschenbrenner
Again, it felt so normal at the time. Now I understand why my mother was worried. I worked on my parents for a while and eventually persuaded them. It felt very normal at the time.

It was great. I really liked college. It came at the right time for me. I really appreciated the liberal arts education, the core curriculum, and reading core works of political philosophy and literature.
Dwarkesh Patel
You did what? Econ?

**Extracted Belief:**

Reading core works of political philosophy and literature is beneficial.

**Context:**

Leopold Aschenbrenner is reflecting on his experience as a 15-year-old college student and expressing his appreciation for the liberal arts education he received.

**Justification:**

Leopold Aschenbrenner states, "I really appreciated the liberal arts education, the core curriculum, and reading core works of political philosophy and literature."

--------

## Chunk 366

**Chunk:**

Dwarkesh Patel
You did what? Econ?
Leopold Aschenbrenner
My majors were math, statistics, and economics, but Columbia has a pretty heavy core curriculum and liberal arts education. Honestly, I shouldn’t have done all the majors. The best courses were those with amazing professors in some history classes. That’s what I would recommend people spend their time on in college.
Dwarkesh Patel
Was there one professor or class that stood out?

**Extracted Belief:**

Columbia University has a rigorous core curriculum and liberal arts education.

**Context:**

Leopold Aschenbrenner is describing his experience as a student at Columbia University, highlighting the emphasis on a broad liberal arts education.

**Justification:**

Leopold Aschenbrenner states that Columbia has a 'pretty heavy core curriculum and liberal arts education.' This statement implies that Columbia is known for its rigorous academic program, particularly in the liberal arts.

--------

## Chunk 367

**Chunk:**

Dwarkesh Patel
You did what? Econ?
Leopold Aschenbrenner
My majors were math, statistics, and economics, but Columbia has a pretty heavy core curriculum and liberal arts education. Honestly, I shouldn’t have done all the majors. The best courses were those with amazing professors in some history classes. That’s what I would recommend people spend their time on in college.
Dwarkesh Patel
Was there one professor or class that stood out?

**Extracted Belief:**

The most valuable courses in college are those taught by exceptional professors.

**Context:**

Leopold Aschenbrenner is reflecting on his academic choices and recommending that students prioritize courses with outstanding professors.

**Justification:**

Aschenbrenner explicitly states that the 'best courses were those with amazing professors in some history classes' and recommends others spend their time on such courses. This suggests his belief in the importance of quality teaching.

--------

## Chunk 368

**Chunk:**

Dwarkesh Patel
Was there one professor or class that stood out?
Leopold Aschenbrenner
A few. Richard Betts' class on war, peace, and strategy. Adam Tooze was fantastic and has written very riveting books. You should have him on the podcast, by the way.
Dwarkesh Patel
I’ve tried. I think you tried for me.

**Extracted Belief:**

Richard Betts' class on war, peace, and strategy was outstanding.

**Context:**

Dwarkesh Patel asks Leopold Aschenbrenner about a standout professor or class, and Aschenbrenner mentions Richard Betts' class.

**Justification:**

Aschenbrenner states that Betts' class was 'outstanding' implying his positive experience.

--------

## Chunk 369

**Chunk:**

Dwarkesh Patel
Was there one professor or class that stood out?
Leopold Aschenbrenner
A few. Richard Betts' class on war, peace, and strategy. Adam Tooze was fantastic and has written very riveting books. You should have him on the podcast, by the way.
Dwarkesh Patel
I’ve tried. I think you tried for me.

**Extracted Belief:**

Adam Tooze is a fantastic professor.

**Context:**

Aschenbrenner lists 'a few' outstanding classes and mentions Adam Tooze's teaching as another example.

**Justification:**

Aschenbrenner describes Tooze as 'fantastic' based on his experience in Tooze's class.

--------

## Chunk 370

**Chunk:**

Dwarkesh Patel
Was there one professor or class that stood out?
Leopold Aschenbrenner
A few. Richard Betts' class on war, peace, and strategy. Adam Tooze was fantastic and has written very riveting books. You should have him on the podcast, by the way.
Dwarkesh Patel
I’ve tried. I think you tried for me.

**Extracted Belief:**

Adam Tooze has written compelling books.

**Context:**

Aschenbrenner describes Adam Tooze's work as a reason why he should be on the podcast, implying Tooze is a good writer.

**Justification:**

Aschenbrenner states that Tooze has written 'very riveting books', suggesting his positive opinion on the quality of Tooze's writing.

--------

## Chunk 371

**Chunk:**

Dwarkesh Patel
I’ve tried. I think you tried for me.
Leopold Aschenbrenner
You’ve got to get him on the pod. It’d be so good.
Dwarkesh Patel
Recently, we were talking to Tyler Cowen. He said when he first encountered you, it was through your paper on economic growth and existential risk. He said, “when I read it, I couldn’t believe that a 17-year-old had written it. If this were an MIT dissertation, I’d be impressed.” You’re a junior and you’re writing novel economic papers? Why did you get interested in this, and what was the process to get into that?

**Extracted Belief:**

Moments of peak productivity are more important than average productivity, at least for certain types of work.

**Context:**

Leopold Aschenbrenner is explaining his approach to work and how it relates to his writing process.

**Justification:**

Aschenbrenner mentions that 'moments of peak productivity' are more important than average productivity for certain jobs like writing and that he has periods of intense focus followed by periods of less activity.

--------

## Chunk 372

**Chunk:**

Dwarkesh Patel
I’ve tried. I think you tried for me.
Leopold Aschenbrenner
You’ve got to get him on the pod. It’d be so good.
Dwarkesh Patel
Recently, we were talking to Tyler Cowen. He said when he first encountered you, it was through your paper on economic growth and existential risk. He said, “when I read it, I couldn’t believe that a 17-year-old had written it. If this were an MIT dissertation, I’d be impressed.” You’re a junior and you’re writing novel economic papers? Why did you get interested in this, and what was the process to get into that?

**Extracted Belief:**

Peak productivity can be crucial even for CEOs, as it might drive a greater impact.

**Context:**

Aschenbrenner is reflecting on the importance of peak productivity, even in a context that is not specifically about writing.

**Justification:**

Aschenbrenner builds on the previous point about peak productivity by expanding it to the realm of CEOs, implying that even for such roles, peak productivity holds significance.

--------

## Chunk 373

**Chunk:**

Dwarkesh Patel
I’ve tried. I think you tried for me.
Leopold Aschenbrenner
You’ve got to get him on the pod. It’d be so good.
Dwarkesh Patel
Recently, we were talking to Tyler Cowen. He said when he first encountered you, it was through your paper on economic growth and existential risk. He said, “when I read it, I couldn’t believe that a 17-year-old had written it. If this were an MIT dissertation, I’d be impressed.” You’re a junior and you’re writing novel economic papers? Why did you get interested in this, and what was the process to get into that?

**Extracted Belief:**

The best parts of economics can be explained simply and intuitively, without needing complex mathematics.

**Context:**

Aschenbrenner is expressing his disappointment with the current state of academic economics and contrasting it with what he considers the best examples of the field.

**Justification:**

Aschenbrenner states that the core takeaway of his 100-page paper can be explained in 30 seconds, demonstrating his belief that the core insights of economics can be conveyed clearly and concisely.

--------

## Chunk 374

**Chunk:**

Dwarkesh Patel
Recently, we were talking to Tyler Cowen. He said when he first encountered you, it was through your paper on economic growth and existential risk. He said, “when I read it, I couldn’t believe that a 17-year-old had written it. If this were an MIT dissertation, I’d be impressed.” You’re a junior and you’re writing novel economic papers? Why did you get interested in this, and what was the process to get into that?
Leopold Aschenbrenner
I just get interested in things. It feels natural to me. I get excited about something, read about it, and immerse myself. I can learn and understand information quickly.

Regarding the paper, moments of peak productivity matter more than average productivity, at least for the way at work.  Some jobs, like CEO, require consistent productivity. I have periods of a couple months where there’s effervescence and other times, I'm computing stuff in the background. Writing the series was similar. You write it and it’s really flowing. That’s what ends up mattering.
Dwarkesh Patel
Even for CEOs, peak productivity might be very important. One of our friends in a group chat, following Chatham House rules, pointed out how many famous CEOs and founders have been bipolar or manic. The call option on your productivity is the most important thing, and you get it by increasing volatility through being bipolar. That’s interesting.

You got interested in economics first. Why economics? You could read about anything. You kind of got a slow start on ML. You wasted all these years on econ. There's an alternative world where you’re on the superalignment team at 17 instead of 21 or whatever it was.

**Extracted Belief:**

Moments of peak productivity are more important than average productivity, at least for certain types of work.

**Context:**

Leopold Aschenbrenner is discussing his experience writing a paper on economic growth and existential risk, suggesting that periods of high productivity are crucial for achieving impactful results.

**Justification:**

Aschenbrenner's statement is based on his personal experience and observation of the work process, especially for tasks requiring intense focus and creative bursts. He contrasts this with jobs requiring consistent productivity, like CEOs.

--------

## Chunk 375

**Chunk:**

Dwarkesh Patel
Recently, we were talking to Tyler Cowen. He said when he first encountered you, it was through your paper on economic growth and existential risk. He said, “when I read it, I couldn’t believe that a 17-year-old had written it. If this were an MIT dissertation, I’d be impressed.” You’re a junior and you’re writing novel economic papers? Why did you get interested in this, and what was the process to get into that?
Leopold Aschenbrenner
I just get interested in things. It feels natural to me. I get excited about something, read about it, and immerse myself. I can learn and understand information quickly.

Regarding the paper, moments of peak productivity matter more than average productivity, at least for the way at work.  Some jobs, like CEO, require consistent productivity. I have periods of a couple months where there’s effervescence and other times, I'm computing stuff in the background. Writing the series was similar. You write it and it’s really flowing. That’s what ends up mattering.
Dwarkesh Patel
Even for CEOs, peak productivity might be very important. One of our friends in a group chat, following Chatham House rules, pointed out how many famous CEOs and founders have been bipolar or manic. The call option on your productivity is the most important thing, and you get it by increasing volatility through being bipolar. That’s interesting.

You got interested in economics first. Why economics? You could read about anything. You kind of got a slow start on ML. You wasted all these years on econ. There's an alternative world where you’re on the superalignment team at 17 instead of 21 or whatever it was.

**Extracted Belief:**

Periods of high productivity, or 'effervescence,' are followed by periods of lower productivity, where individuals are 'computing stuff in the background.'

**Context:**

Aschenbrenner describes his personal work patterns, suggesting that phases of intense productivity are interspersed with periods of less output but still processing information.

**Justification:**

He references his own experience, stating that his writing process has similar patterns, with periods of high output and periods of processing and reflection.

--------

## Chunk 376

**Chunk:**

Dwarkesh Patel
Even for CEOs, peak productivity might be very important. One of our friends in a group chat, following Chatham House rules, pointed out how many famous CEOs and founders have been bipolar or manic. The call option on your productivity is the most important thing, and you get it by increasing volatility through being bipolar. That’s interesting.

You got interested in economics first. Why economics? You could read about anything. You kind of got a slow start on ML. You wasted all these years on econ. There's an alternative world where you’re on the superalignment team at 17 instead of 21 or whatever it was.
Leopold Aschenbrenner
In some sense, I’m still doing economics. I’m looking at straight lines on a graph, log-log plots, figuring out trends, and thinking about feedback loops, equilibrium, and arms control dynamics. It’s a way of thinking that I find very useful.

Dario and Ilya seeing scaling early is, in some sense, a very economic way of thinking. It’s also related to empirical physics. Many of them are physicists. Economists often can’t code well enough, which is their issue, but it's that way of thinking.

I also thought a lot of core ideas in economics were beautiful. In some sense, I feel a little duped because econ academia is kind of decadent now. The paper I wrote is long, 100 pages of math, but the core takeaway can be explained in 30 seconds and it makes sense and you don’t really need the math. The best pieces of economics are like that.

You do the work to uncover insights that weren’t obvious to you before. Once you’ve done the work, some sort of mechanism falls out of it that makes a lot of  crisp, intuitive sense and explains facts about the world. You can then use it in arguments. Econ 101 is great like this. A lot of econ in the fifties and sixties was like this. Chad Jones' papers are often like this. I really like his papers for this.

Why didn’t I ultimately pursue econ academia? There were several reasons, one of them being Tyler Cowen. He took me aside and said, "I think you’re one of the top young economists I’ve ever met, but you should probably not go to grad school."
Dwarkesh Patel
Oh, interesting. Really? I didn’t realize that.

**Extracted Belief:**

The fundamental principles of economics are beautiful and elegant, with the ability to explain complex phenomena in a concise and intuitive manner.

**Context:**

Leopold Aschenbrenner expresses his appreciation for core economic ideas, even while acknowledging a negative perception of contemporary economic academia.

**Justification:**

The belief is articulated through the statement 'I also thought a lot of core ideas in economics were beautiful' and reinforced by the description of good economics as producing 'crisp, intuitive sense and explains facts about the world.'

--------

## Chunk 377

**Chunk:**

Dwarkesh Patel
Even for CEOs, peak productivity might be very important. One of our friends in a group chat, following Chatham House rules, pointed out how many famous CEOs and founders have been bipolar or manic. The call option on your productivity is the most important thing, and you get it by increasing volatility through being bipolar. That’s interesting.

You got interested in economics first. Why economics? You could read about anything. You kind of got a slow start on ML. You wasted all these years on econ. There's an alternative world where you’re on the superalignment team at 17 instead of 21 or whatever it was.
Leopold Aschenbrenner
In some sense, I’m still doing economics. I’m looking at straight lines on a graph, log-log plots, figuring out trends, and thinking about feedback loops, equilibrium, and arms control dynamics. It’s a way of thinking that I find very useful.

Dario and Ilya seeing scaling early is, in some sense, a very economic way of thinking. It’s also related to empirical physics. Many of them are physicists. Economists often can’t code well enough, which is their issue, but it's that way of thinking.

I also thought a lot of core ideas in economics were beautiful. In some sense, I feel a little duped because econ academia is kind of decadent now. The paper I wrote is long, 100 pages of math, but the core takeaway can be explained in 30 seconds and it makes sense and you don’t really need the math. The best pieces of economics are like that.

You do the work to uncover insights that weren’t obvious to you before. Once you’ve done the work, some sort of mechanism falls out of it that makes a lot of  crisp, intuitive sense and explains facts about the world. You can then use it in arguments. Econ 101 is great like this. A lot of econ in the fifties and sixties was like this. Chad Jones' papers are often like this. I really like his papers for this.

Why didn’t I ultimately pursue econ academia? There were several reasons, one of them being Tyler Cowen. He took me aside and said, "I think you’re one of the top young economists I’ve ever met, but you should probably not go to grad school."
Dwarkesh Patel
Oh, interesting. Really? I didn’t realize that.

**Extracted Belief:**

The primary value of economics lies in its ability to uncover non-obvious insights and explain real-world phenomena through logical mechanisms.

**Context:**

Leopold Aschenbrenner contrasts his positive view of core economic principles with his negative view of contemporary academic economics, arguing for the importance of uncovering hidden insights and explaining them simply.

**Justification:**

The belief is expressed through the statement, 'You do the work to uncover insights that weren’t obvious to you before. Once you’ve done the work, some sort of mechanism falls out of it that makes a lot of crisp, intuitive sense and explains facts about the world.'

--------

## Chunk 378

**Chunk:**

Dwarkesh Patel
Even for CEOs, peak productivity might be very important. One of our friends in a group chat, following Chatham House rules, pointed out how many famous CEOs and founders have been bipolar or manic. The call option on your productivity is the most important thing, and you get it by increasing volatility through being bipolar. That’s interesting.

You got interested in economics first. Why economics? You could read about anything. You kind of got a slow start on ML. You wasted all these years on econ. There's an alternative world where you’re on the superalignment team at 17 instead of 21 or whatever it was.
Leopold Aschenbrenner
In some sense, I’m still doing economics. I’m looking at straight lines on a graph, log-log plots, figuring out trends, and thinking about feedback loops, equilibrium, and arms control dynamics. It’s a way of thinking that I find very useful.

Dario and Ilya seeing scaling early is, in some sense, a very economic way of thinking. It’s also related to empirical physics. Many of them are physicists. Economists often can’t code well enough, which is their issue, but it's that way of thinking.

I also thought a lot of core ideas in economics were beautiful. In some sense, I feel a little duped because econ academia is kind of decadent now. The paper I wrote is long, 100 pages of math, but the core takeaway can be explained in 30 seconds and it makes sense and you don’t really need the math. The best pieces of economics are like that.

You do the work to uncover insights that weren’t obvious to you before. Once you’ve done the work, some sort of mechanism falls out of it that makes a lot of  crisp, intuitive sense and explains facts about the world. You can then use it in arguments. Econ 101 is great like this. A lot of econ in the fifties and sixties was like this. Chad Jones' papers are often like this. I really like his papers for this.

Why didn’t I ultimately pursue econ academia? There were several reasons, one of them being Tyler Cowen. He took me aside and said, "I think you’re one of the top young economists I’ve ever met, but you should probably not go to grad school."
Dwarkesh Patel
Oh, interesting. Really? I didn’t realize that.

**Extracted Belief:**

Economic thinking involves analyzing trends, feedback loops, equilibrium, and arms control dynamics by looking at graphs and data.

**Context:**

Leopold Aschenbrenner describes his continued use of economic thinking in his current work by describing the specific methods he employs.

**Justification:**

The belief is expressed through the statement, 'In some sense, I’m still doing economics. I’m looking at straight lines on a graph, log-log plots, figuring out trends, and thinking about feedback loops, equilibrium, and arms control dynamics.

--------

## Chunk 379

**Chunk:**

Dwarkesh Patel
Even for CEOs, peak productivity might be very important. One of our friends in a group chat, following Chatham House rules, pointed out how many famous CEOs and founders have been bipolar or manic. The call option on your productivity is the most important thing, and you get it by increasing volatility through being bipolar. That’s interesting.

You got interested in economics first. Why economics? You could read about anything. You kind of got a slow start on ML. You wasted all these years on econ. There's an alternative world where you’re on the superalignment team at 17 instead of 21 or whatever it was.
Leopold Aschenbrenner
In some sense, I’m still doing economics. I’m looking at straight lines on a graph, log-log plots, figuring out trends, and thinking about feedback loops, equilibrium, and arms control dynamics. It’s a way of thinking that I find very useful.

Dario and Ilya seeing scaling early is, in some sense, a very economic way of thinking. It’s also related to empirical physics. Many of them are physicists. Economists often can’t code well enough, which is their issue, but it's that way of thinking.

I also thought a lot of core ideas in economics were beautiful. In some sense, I feel a little duped because econ academia is kind of decadent now. The paper I wrote is long, 100 pages of math, but the core takeaway can be explained in 30 seconds and it makes sense and you don’t really need the math. The best pieces of economics are like that.

You do the work to uncover insights that weren’t obvious to you before. Once you’ve done the work, some sort of mechanism falls out of it that makes a lot of  crisp, intuitive sense and explains facts about the world. You can then use it in arguments. Econ 101 is great like this. A lot of econ in the fifties and sixties was like this. Chad Jones' papers are often like this. I really like his papers for this.

Why didn’t I ultimately pursue econ academia? There were several reasons, one of them being Tyler Cowen. He took me aside and said, "I think you’re one of the top young economists I’ve ever met, but you should probably not go to grad school."
Dwarkesh Patel
Oh, interesting. Really? I didn’t realize that.

**Extracted Belief:**

Economic thinking is a valuable tool for understanding the dynamics of scaling, even in fields outside of traditional economics.

**Context:**

Leopold Aschenbrenner connects economic thinking to the concept of scaling, as observed in the work of Dario Amodei and Ilya Sutskever.

**Justification:**

The belief is expressed through the statement, 'Dario and Ilya seeing scaling early is, in some sense, a very economic way of thinking.', indicating the applicability of economic principles beyond its traditional domain.

--------

## Chunk 380

**Chunk:**

Dwarkesh Patel
Even for CEOs, peak productivity might be very important. One of our friends in a group chat, following Chatham House rules, pointed out how many famous CEOs and founders have been bipolar or manic. The call option on your productivity is the most important thing, and you get it by increasing volatility through being bipolar. That’s interesting.

You got interested in economics first. Why economics? You could read about anything. You kind of got a slow start on ML. You wasted all these years on econ. There's an alternative world where you’re on the superalignment team at 17 instead of 21 or whatever it was.
Leopold Aschenbrenner
In some sense, I’m still doing economics. I’m looking at straight lines on a graph, log-log plots, figuring out trends, and thinking about feedback loops, equilibrium, and arms control dynamics. It’s a way of thinking that I find very useful.

Dario and Ilya seeing scaling early is, in some sense, a very economic way of thinking. It’s also related to empirical physics. Many of them are physicists. Economists often can’t code well enough, which is their issue, but it's that way of thinking.

I also thought a lot of core ideas in economics were beautiful. In some sense, I feel a little duped because econ academia is kind of decadent now. The paper I wrote is long, 100 pages of math, but the core takeaway can be explained in 30 seconds and it makes sense and you don’t really need the math. The best pieces of economics are like that.

You do the work to uncover insights that weren’t obvious to you before. Once you’ve done the work, some sort of mechanism falls out of it that makes a lot of  crisp, intuitive sense and explains facts about the world. You can then use it in arguments. Econ 101 is great like this. A lot of econ in the fifties and sixties was like this. Chad Jones' papers are often like this. I really like his papers for this.

Why didn’t I ultimately pursue econ academia? There were several reasons, one of them being Tyler Cowen. He took me aside and said, "I think you’re one of the top young economists I’ve ever met, but you should probably not go to grad school."
Dwarkesh Patel
Oh, interesting. Really? I didn’t realize that.

**Extracted Belief:**

Contemporary academic economics is decadent and not necessarily aligned with the core value of discovering and explaining complex phenomena.

**Context:**

Leopold Aschenbrenner contrasts the beauty and elegance of core economic principles with the perceived state of contemporary academic economics.

**Justification:**

The belief is expressed through the statement, 'In some sense, I feel a little duped because econ academia is kind of decadent now.'

--------

## Chunk 381

**Chunk:**

Dwarkesh Patel
Oh, interesting. Really? I didn’t realize that.
Leopold Aschenbrenner
Yeah, it was good because he kind of introduced me to the Twitter weirdos. I think the takeaway from that was that I have to move out west one more time.
Dwarkesh Patel
Wait Tyler introduced you to the Twitter weirdos?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 382

**Chunk:**

Dwarkesh Patel
Wait Tyler introduced you to the Twitter weirdos?
Leopold Aschenbrenner
A little bit. Or just kind of the broader culture?
Dwarkesh Patel
A 60-year-old economist introduced you to Twitter?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 383

**Chunk:**

Dwarkesh Patel
A 60-year-old economist introduced you to Twitter?
Leopold Aschenbrenner
Well, I had been in Germany, completely on the periphery, and then moved to a US elite institution. I got a sense of meritocratic elite US society. Basically, there was a directory. To find the true American spirit I had to come out here.

The other reason I didn’t become an economist, or at least pursue econ academia, is that econ academia has become a bit decadent. Maybe it's just that ideas are getting harder to find, or that all the beautiful, simple things have been discovered.

But what are econ papers these days? They’re often 200 pages of empirical analyses on things like how buying 100,000 more textbooks in Wisconsin affects educational outcomes. I'm happy that work happens. It’s important work but it doesn't uncover fundamental insights and mechanisms in society.

Even the theory work often involves really complicated models and the model spits out something like, “Fed does X, then Y happens” and you have no idea why that happened. There’s a gazillion parameters and they’re all calibrated in some way and it’s a computer simulation and you have no idea about the validity. The most important insights are the ones where you have to do a lot of work to get them but then there’s this crisp intuition.
Dwarkesh Patel
The P versus NP of…

**Extracted Belief:**

Econ academia has become decadent.

**Context:**

Leopold Aschenbrenner is discussing his decision not to pursue econ academia, citing a perceived decline in the quality of research and ideas.

**Justification:**

He believes that econ academia has become decadent based on his personal observation and experience.

--------

## Chunk 384

**Chunk:**

Dwarkesh Patel
A 60-year-old economist introduced you to Twitter?
Leopold Aschenbrenner
Well, I had been in Germany, completely on the periphery, and then moved to a US elite institution. I got a sense of meritocratic elite US society. Basically, there was a directory. To find the true American spirit I had to come out here.

The other reason I didn’t become an economist, or at least pursue econ academia, is that econ academia has become a bit decadent. Maybe it's just that ideas are getting harder to find, or that all the beautiful, simple things have been discovered.

But what are econ papers these days? They’re often 200 pages of empirical analyses on things like how buying 100,000 more textbooks in Wisconsin affects educational outcomes. I'm happy that work happens. It’s important work but it doesn't uncover fundamental insights and mechanisms in society.

Even the theory work often involves really complicated models and the model spits out something like, “Fed does X, then Y happens” and you have no idea why that happened. There’s a gazillion parameters and they’re all calibrated in some way and it’s a computer simulation and you have no idea about the validity. The most important insights are the ones where you have to do a lot of work to get them but then there’s this crisp intuition.
Dwarkesh Patel
The P versus NP of…

**Extracted Belief:**

Ideas are getting harder to find in econ academia.

**Context:**

Leopold Aschenbrenner is explaining his perception of the decline in econ academia, suggesting that the field is running out of new ideas.

**Justification:**

This belief is based on his personal observation and experience as an economist.

--------

## Chunk 385

**Chunk:**

Dwarkesh Patel
A 60-year-old economist introduced you to Twitter?
Leopold Aschenbrenner
Well, I had been in Germany, completely on the periphery, and then moved to a US elite institution. I got a sense of meritocratic elite US society. Basically, there was a directory. To find the true American spirit I had to come out here.

The other reason I didn’t become an economist, or at least pursue econ academia, is that econ academia has become a bit decadent. Maybe it's just that ideas are getting harder to find, or that all the beautiful, simple things have been discovered.

But what are econ papers these days? They’re often 200 pages of empirical analyses on things like how buying 100,000 more textbooks in Wisconsin affects educational outcomes. I'm happy that work happens. It’s important work but it doesn't uncover fundamental insights and mechanisms in society.

Even the theory work often involves really complicated models and the model spits out something like, “Fed does X, then Y happens” and you have no idea why that happened. There’s a gazillion parameters and they’re all calibrated in some way and it’s a computer simulation and you have no idea about the validity. The most important insights are the ones where you have to do a lot of work to get them but then there’s this crisp intuition.
Dwarkesh Patel
The P versus NP of…

**Extracted Belief:**

All the beautiful, simple things in economics have been discovered.

**Context:**

Leopold Aschenbrenner is continuing his explanation of the perceived decline in econ academia, suggesting that the field has exhausted its most fundamental insights.

**Justification:**

This belief is based on his personal observation and experience as an economist.

--------

## Chunk 386

**Chunk:**

Dwarkesh Patel
A 60-year-old economist introduced you to Twitter?
Leopold Aschenbrenner
Well, I had been in Germany, completely on the periphery, and then moved to a US elite institution. I got a sense of meritocratic elite US society. Basically, there was a directory. To find the true American spirit I had to come out here.

The other reason I didn’t become an economist, or at least pursue econ academia, is that econ academia has become a bit decadent. Maybe it's just that ideas are getting harder to find, or that all the beautiful, simple things have been discovered.

But what are econ papers these days? They’re often 200 pages of empirical analyses on things like how buying 100,000 more textbooks in Wisconsin affects educational outcomes. I'm happy that work happens. It’s important work but it doesn't uncover fundamental insights and mechanisms in society.

Even the theory work often involves really complicated models and the model spits out something like, “Fed does X, then Y happens” and you have no idea why that happened. There’s a gazillion parameters and they’re all calibrated in some way and it’s a computer simulation and you have no idea about the validity. The most important insights are the ones where you have to do a lot of work to get them but then there’s this crisp intuition.
Dwarkesh Patel
The P versus NP of…

**Extracted Belief:**

Empirical analyses in economics often focus on narrow, specific issues.

**Context:**

Leopold Aschenbrenner is criticizing the trend of econ papers focusing on specific empirical studies, suggesting that they lack broader significance.

**Justification:**

He cites the example of a 200-page paper analyzing the impact of textbook purchases in Wisconsin, which he believes is too narrowly focused.

--------

## Chunk 387

**Chunk:**

Dwarkesh Patel
A 60-year-old economist introduced you to Twitter?
Leopold Aschenbrenner
Well, I had been in Germany, completely on the periphery, and then moved to a US elite institution. I got a sense of meritocratic elite US society. Basically, there was a directory. To find the true American spirit I had to come out here.

The other reason I didn’t become an economist, or at least pursue econ academia, is that econ academia has become a bit decadent. Maybe it's just that ideas are getting harder to find, or that all the beautiful, simple things have been discovered.

But what are econ papers these days? They’re often 200 pages of empirical analyses on things like how buying 100,000 more textbooks in Wisconsin affects educational outcomes. I'm happy that work happens. It’s important work but it doesn't uncover fundamental insights and mechanisms in society.

Even the theory work often involves really complicated models and the model spits out something like, “Fed does X, then Y happens” and you have no idea why that happened. There’s a gazillion parameters and they’re all calibrated in some way and it’s a computer simulation and you have no idea about the validity. The most important insights are the ones where you have to do a lot of work to get them but then there’s this crisp intuition.
Dwarkesh Patel
The P versus NP of…

**Extracted Belief:**

Empirical analyses in economics may not uncover fundamental insights and mechanisms in society.

**Context:**

Leopold Aschenbrenner is expressing his concern about the limitations of empirical analyses in economics, arguing that they may not reveal deeper societal truths.

**Justification:**

He believes that such studies often lack broader significance, failing to provide insights into fundamental societal mechanisms.

--------

## Chunk 388

**Chunk:**

Dwarkesh Patel
A 60-year-old economist introduced you to Twitter?
Leopold Aschenbrenner
Well, I had been in Germany, completely on the periphery, and then moved to a US elite institution. I got a sense of meritocratic elite US society. Basically, there was a directory. To find the true American spirit I had to come out here.

The other reason I didn’t become an economist, or at least pursue econ academia, is that econ academia has become a bit decadent. Maybe it's just that ideas are getting harder to find, or that all the beautiful, simple things have been discovered.

But what are econ papers these days? They’re often 200 pages of empirical analyses on things like how buying 100,000 more textbooks in Wisconsin affects educational outcomes. I'm happy that work happens. It’s important work but it doesn't uncover fundamental insights and mechanisms in society.

Even the theory work often involves really complicated models and the model spits out something like, “Fed does X, then Y happens” and you have no idea why that happened. There’s a gazillion parameters and they’re all calibrated in some way and it’s a computer simulation and you have no idea about the validity. The most important insights are the ones where you have to do a lot of work to get them but then there’s this crisp intuition.
Dwarkesh Patel
The P versus NP of…

**Extracted Belief:**

Theoretical models in economics can be overly complex and lack intuitive understanding.

**Context:**

Leopold Aschenbrenner is criticizing the use of complex models in economics, suggesting that they often obscure the underlying mechanisms.

**Justification:**

He argues that these models are often difficult to interpret and fail to provide clear insights into cause-and-effect relationships.

--------

## Chunk 389

**Chunk:**

Dwarkesh Patel
A 60-year-old economist introduced you to Twitter?
Leopold Aschenbrenner
Well, I had been in Germany, completely on the periphery, and then moved to a US elite institution. I got a sense of meritocratic elite US society. Basically, there was a directory. To find the true American spirit I had to come out here.

The other reason I didn’t become an economist, or at least pursue econ academia, is that econ academia has become a bit decadent. Maybe it's just that ideas are getting harder to find, or that all the beautiful, simple things have been discovered.

But what are econ papers these days? They’re often 200 pages of empirical analyses on things like how buying 100,000 more textbooks in Wisconsin affects educational outcomes. I'm happy that work happens. It’s important work but it doesn't uncover fundamental insights and mechanisms in society.

Even the theory work often involves really complicated models and the model spits out something like, “Fed does X, then Y happens” and you have no idea why that happened. There’s a gazillion parameters and they’re all calibrated in some way and it’s a computer simulation and you have no idea about the validity. The most important insights are the ones where you have to do a lot of work to get them but then there’s this crisp intuition.
Dwarkesh Patel
The P versus NP of…

**Extracted Belief:**

The most important insights in economics are those that are intuitively clear and require significant effort to obtain.

**Context:**

Leopold Aschenbrenner is contrasting complex models with simpler insights, emphasizing the value of clear and intuitive understanding.

**Justification:**

He argues that the most valuable insights are those that are easy to grasp and require substantial effort to discover.

--------

## Chunk 390

**Chunk:**

Dwarkesh Patel
The P versus NP of…
Leopold Aschenbrenner
Sure, yeah.
Dwarkesh Patel
That’s really interesting. Going back to your time in college, you say that peak productivity explains this paper and things. But being valedictorian, getting straight A’s, is very much an average productivity phenomenon.

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 391

**Chunk:**

Dwarkesh Patel
That’s really interesting. Going back to your time in college, you say that peak productivity explains this paper and things. But being valedictorian, getting straight A’s, is very much an average productivity phenomenon.
Leopold Aschenbrenner
There’s one award for the highest GPA, which I won, but the valedictorian is selected by the faculty from among those with the highest GPA.
Dwarkesh Patel
So it's not just peak productivity.

**Extracted Belief:**

The valedictorian is selected by the faculty from among those with the highest GPA.

**Context:**

Leopold Aschenbrenner explains the difference between the award for the highest GPA and the selection of the valedictorian, clarifying the selection process for the latter.

**Justification:**

Leopold Aschenbrenner states that the faculty makes the decision based on GPA.

--------

## Chunk 392

**Chunk:**

Dwarkesh Patel
So it's not just peak productivity.
Leopold Aschenbrenner
I generally just love this stuff. I was curious, found it really interesting, and enjoyed learning about it. It made sense to me, and it felt very natural.

One of my faults is that I’m not that good at eating glass. Some people are very good at it. The moments of peak productivity come when I’m excited and engaged and love it. If you take the right courses, that’s what you get in college.
Dwarkesh Patel
It’s like Bruce Banner’s quote in The Avengers: "I’m always angry." I’m always excited and curious. That’s why I’m always at peak productivity.

By the way, when you were in college, I was also in college. Despite being a year younger than me, you were ahead of me by at least two years or something. We met around this time through the Tyler Cowen universe. It’s very insane how small the world is. Did I reach out to you? I must have.

**Extracted Belief:**

The moments of peak productivity occur when a person is excited, engaged, and enjoys the task at hand.

**Context:**

Leopold Aschenbrenner is discussing his own experiences with peak productivity and attributing it to his own enthusiasm and engagement.

**Justification:**

Leopold Aschenbrenner mentions that "The moments of peak productivity come when I’m excited and engaged and love it." This suggests that his belief is based on his personal experience and observation of his own productivity patterns.

--------

## Chunk 393

**Chunk:**

Dwarkesh Patel
So it's not just peak productivity.
Leopold Aschenbrenner
I generally just love this stuff. I was curious, found it really interesting, and enjoyed learning about it. It made sense to me, and it felt very natural.

One of my faults is that I’m not that good at eating glass. Some people are very good at it. The moments of peak productivity come when I’m excited and engaged and love it. If you take the right courses, that’s what you get in college.
Dwarkesh Patel
It’s like Bruce Banner’s quote in The Avengers: "I’m always angry." I’m always excited and curious. That’s why I’m always at peak productivity.

By the way, when you were in college, I was also in college. Despite being a year younger than me, you were ahead of me by at least two years or something. We met around this time through the Tyler Cowen universe. It’s very insane how small the world is. Did I reach out to you? I must have.

**Extracted Belief:**

Taking the right courses in college can contribute to achieving peak productivity.

**Context:**

Leopold Aschenbrenner is suggesting that the right educational environment can influence a person's productivity levels.

**Justification:**

Leopold Aschenbrenner states that "If you take the right courses, that’s what you get in college."

--------

## Chunk 394

**Chunk:**

Dwarkesh Patel
It’s like Bruce Banner’s quote in The Avengers: "I’m always angry." I’m always excited and curious. That’s why I’m always at peak productivity.

By the way, when you were in college, I was also in college. Despite being a year younger than me, you were ahead of me by at least two years or something. We met around this time through the Tyler Cowen universe. It’s very insane how small the world is. Did I reach out to you? I must have.
Leopold Aschenbrenner
I’m not sure.
Dwarkesh Patel
When I had a couple of videos with a few hundred views.

**Extracted Belief:**

The world of AI and technology is relatively small, with a limited number of individuals involved in leading research and development at major organizations.

**Context:**

Leopold Aschenbrenner comments on the AI world being small, where the same few people are involved at companies like DeepMind, OpenAI, and Anthropic.

**Justification:**

He notes that the AI world consists of a limited number of people involved in running models and projects at prominent AI companies.  This is based on his personal observation and experiences in the AI field.

--------

## Chunk 395

**Chunk:**

Dwarkesh Patel
It’s like Bruce Banner’s quote in The Avengers: "I’m always angry." I’m always excited and curious. That’s why I’m always at peak productivity.

By the way, when you were in college, I was also in college. Despite being a year younger than me, you were ahead of me by at least two years or something. We met around this time through the Tyler Cowen universe. It’s very insane how small the world is. Did I reach out to you? I must have.
Leopold Aschenbrenner
I’m not sure.
Dwarkesh Patel
When I had a couple of videos with a few hundred views.

**Extracted Belief:**

Individuals can have a degree of control over their own choices and actions, even within a seemingly limited or predetermined environment.

**Context:**

Aschenbrenner connects his experience in Germany, where he felt pressured to conform to expectations, to his belief in agency.

**Justification:**

He uses his own experience of skipping grades and moving to the US, actions that were not typical, as evidence that individuals can choose to act differently from expectations.

--------

## Chunk 396

**Chunk:**

Dwarkesh Patel
It’s like Bruce Banner’s quote in The Avengers: "I’m always angry." I’m always excited and curious. That’s why I’m always at peak productivity.

By the way, when you were in college, I was also in college. Despite being a year younger than me, you were ahead of me by at least two years or something. We met around this time through the Tyler Cowen universe. It’s very insane how small the world is. Did I reach out to you? I must have.
Leopold Aschenbrenner
I’m not sure.
Dwarkesh Patel
When I had a couple of videos with a few hundred views.

**Extracted Belief:**

It is possible to succeed by pursuing paths that are unconventional or outside the mainstream.

**Context:**

Leopold Aschenbrenner shares his belief in the importance of pursuing personal goals, even if they are unconventional.

**Justification:**

His personal experience of skipping grades and moving to the US serves as a testimonial to this belief. He believes that these unconventional actions led to his success.

--------

## Chunk 397

**Chunk:**

Dwarkesh Patel
When I had a couple of videos with a few hundred views.
Leopold Aschenbrenner
It’s a small world. This is the crazy thing about the AI world. It’s the same few people at the parties running the models at DeepMind, OpenAI, and Anthropic. Some of our friends, now successful in their careers, met many of the people who are now successful in Silicon Valley before their twenties or in their early twenties.

Why is it a small world? There’s some amount of agency. I think in a funny way, this is what I took away from my Germany experience. It was crushing. I didn’t like it. Skipping grades and moving to the US were unusual moves.

Just trying to do it, and then seeing it work, reinforced the idea that you don’t have to conform to the Overton window. You can try to do what seems right to you, even if most people are wrong. That was a valuable and formative early experience.
Dwarkesh Patel
After college, what did you do?

**Extracted Belief:**

The AI world is a small world, with the same few people involved in key organizations like DeepMind, OpenAI, and Anthropic.

**Context:**

Leopold Aschenbrenner is commenting on the interconnectedness of the AI world, observing the repeated presence of the same individuals across major companies.

**Justification:**

He explicitly mentions the 'same few people at the parties running the models' in these companies, suggesting a shared network of individuals within the AI space.

--------

## Chunk 398

**Chunk:**

Dwarkesh Patel
When I had a couple of videos with a few hundred views.
Leopold Aschenbrenner
It’s a small world. This is the crazy thing about the AI world. It’s the same few people at the parties running the models at DeepMind, OpenAI, and Anthropic. Some of our friends, now successful in their careers, met many of the people who are now successful in Silicon Valley before their twenties or in their early twenties.

Why is it a small world? There’s some amount of agency. I think in a funny way, this is what I took away from my Germany experience. It was crushing. I didn’t like it. Skipping grades and moving to the US were unusual moves.

Just trying to do it, and then seeing it work, reinforced the idea that you don’t have to conform to the Overton window. You can try to do what seems right to you, even if most people are wrong. That was a valuable and formative early experience.
Dwarkesh Patel
After college, what did you do?

**Extracted Belief:**

People have some agency in shaping their own paths and experiences, even if it means going against conventional norms.

**Context:**

Aschenbrenner reflects on his own experience of skipping grades and moving to the US, highlighting the potential for individual agency despite societal expectations.

**Justification:**

He argues that 'you don’t have to conform to the Overton window. You can try to do what seems right to you, even if most people are wrong,' suggesting that individuals can actively choose their own direction.

--------

## Chunk 399

**Chunk:**

Dwarkesh Patel
After college, what did you do?
Leopold Aschenbrenner
I did econ research for a bit, at Oxford and other places, and then I worked at Future Fund.
Dwarkesh Patel
Tell me about it.

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 400

**Chunk:**

Dwarkesh Patel
Tell me about it.
Leopold Aschenbrenner
Future Fund was a foundation funded by Sam Bankman-Fried but we were our own thing. We were based in the Bay Area. At the time, in early 2022, it was an incredibly exciting opportunity. It was basically a startup foundation, which doesn’t come along often. We thought we would be able to give away billions of dollars and remake how philanthropy is done from first principles.

We thought we’d have significant impact, focusing on causes like biosecurity, AI, and finding exceptional talent to work on hard problems. A lot of the work we did was exciting. Academics, who would usually take six months, would send us emails saying, "this is great. This is so quick and straightforward." I often find that with a little encouragement and empowerment, by removing excuses and making the process easy, you can get people to do great things.
Dwarkesh Patel
For context, not only were you guys planning on deploying billions of dollars, but it was a team of four people. So you, at 18, were on a team of four people that was in charge of deploying billions of dollars.

**Extracted Belief:**

It is possible to remake philanthropy from first principles by creating a startup foundation.

**Context:**

Leopold Aschenbrenner was describing his experience working at Future Fund, a startup foundation founded by Sam Bankman-Fried, and explaining his belief that such an approach could revolutionize philanthropy.

**Justification:**

He cites the fact that Future Fund was a startup foundation as evidence for this belief, suggesting that such an approach is novel and could be effective.

--------

## Chunk 401

**Chunk:**

Dwarkesh Patel
Tell me about it.
Leopold Aschenbrenner
Future Fund was a foundation funded by Sam Bankman-Fried but we were our own thing. We were based in the Bay Area. At the time, in early 2022, it was an incredibly exciting opportunity. It was basically a startup foundation, which doesn’t come along often. We thought we would be able to give away billions of dollars and remake how philanthropy is done from first principles.

We thought we’d have significant impact, focusing on causes like biosecurity, AI, and finding exceptional talent to work on hard problems. A lot of the work we did was exciting. Academics, who would usually take six months, would send us emails saying, "this is great. This is so quick and straightforward." I often find that with a little encouragement and empowerment, by removing excuses and making the process easy, you can get people to do great things.
Dwarkesh Patel
For context, not only were you guys planning on deploying billions of dollars, but it was a team of four people. So you, at 18, were on a team of four people that was in charge of deploying billions of dollars.

**Extracted Belief:**

Philanthropic efforts can be significantly impactful by focusing on causes like biosecurity and artificial intelligence, and by finding exceptional talent to work on hard problems.

**Context:**

Leopold Aschenbrenner was outlining the goals and focus of Future Fund, highlighting the potential impact of their work in areas like biosecurity, AI, and attracting top talent.

**Justification:**

He cites the specific areas of focus (biosecurity, AI, exceptional talent) as evidence of the potential for significant impact. This belief is based on his experience working at Future Fund.

--------

## Chunk 402

**Chunk:**

Dwarkesh Patel
Tell me about it.
Leopold Aschenbrenner
Future Fund was a foundation funded by Sam Bankman-Fried but we were our own thing. We were based in the Bay Area. At the time, in early 2022, it was an incredibly exciting opportunity. It was basically a startup foundation, which doesn’t come along often. We thought we would be able to give away billions of dollars and remake how philanthropy is done from first principles.

We thought we’d have significant impact, focusing on causes like biosecurity, AI, and finding exceptional talent to work on hard problems. A lot of the work we did was exciting. Academics, who would usually take six months, would send us emails saying, "this is great. This is so quick and straightforward." I often find that with a little encouragement and empowerment, by removing excuses and making the process easy, you can get people to do great things.
Dwarkesh Patel
For context, not only were you guys planning on deploying billions of dollars, but it was a team of four people. So you, at 18, were on a team of four people that was in charge of deploying billions of dollars.

**Extracted Belief:**

Removing excuses and making processes easier can empower people to achieve great things.

**Context:**

Leopold Aschenbrenner was explaining the positive impact he observed from Future Fund's approach, noting how it encouraged academics to work more efficiently.

**Justification:**

He uses the example of academics responding more quickly to requests when the process was streamlined as evidence for this belief. This belief is grounded in his direct experience working at Future Fund.

--------

## Chunk 403

**Chunk:**

Dwarkesh Patel
For context, not only were you guys planning on deploying billions of dollars, but it was a team of four people. So you, at 18, were on a team of four people that was in charge of deploying billions of dollars.
Leopold Aschenbrenner
That was sort of the heyday. Then in November 2022, it was revealed that Sam was a giant fraud, and from one day to the next, the whole thing collapsed. It was really tough. It was devastating for the people who had their money in FTX. Closer to home, we wanted to help all the grantees do amazing projects but they ended up suddenly saddled with a giant problem.

Personally, it was difficult because it was a startup. I had worked 70-hour weeks every week for almost a year to build it up. We were a tiny team, and then from one day to the next, it was all gone and associated with a giant fraud. That was incredibly tough.
Dwarkesh Patel
Were there any early signs about SBF?

**Extracted Belief:**

Sam Bankman-Fried was a fraud.

**Context:**

Leopold Aschenbrenner is describing the collapse of Future Fund, a foundation funded by Sam Bankman-Fried, and states that it was revealed that Sam was a fraud.

**Justification:**

This statement is based on the revelation of Sam Bankman-Fried's fraudulent activities, which is mentioned in the context of the conversation.

--------

## Chunk 404

**Chunk:**

Dwarkesh Patel
For context, not only were you guys planning on deploying billions of dollars, but it was a team of four people. So you, at 18, were on a team of four people that was in charge of deploying billions of dollars.
Leopold Aschenbrenner
That was sort of the heyday. Then in November 2022, it was revealed that Sam was a giant fraud, and from one day to the next, the whole thing collapsed. It was really tough. It was devastating for the people who had their money in FTX. Closer to home, we wanted to help all the grantees do amazing projects but they ended up suddenly saddled with a giant problem.

Personally, it was difficult because it was a startup. I had worked 70-hour weeks every week for almost a year to build it up. We were a tiny team, and then from one day to the next, it was all gone and associated with a giant fraud. That was incredibly tough.
Dwarkesh Patel
Were there any early signs about SBF?

**Extracted Belief:**

Sam Bankman-Fried's fraudulent activities devastated people who had money in FTX.

**Context:**

Leopold Aschenbrenner is describing the impact of Sam Bankman-Fried's fraud on those who had invested in FTX.

**Justification:**

This belief is based on the statement that the collapse of FTX was devastating for people who had their money in FTX.

--------

## Chunk 405

**Chunk:**

Dwarkesh Patel
For context, not only were you guys planning on deploying billions of dollars, but it was a team of four people. So you, at 18, were on a team of four people that was in charge of deploying billions of dollars.
Leopold Aschenbrenner
That was sort of the heyday. Then in November 2022, it was revealed that Sam was a giant fraud, and from one day to the next, the whole thing collapsed. It was really tough. It was devastating for the people who had their money in FTX. Closer to home, we wanted to help all the grantees do amazing projects but they ended up suddenly saddled with a giant problem.

Personally, it was difficult because it was a startup. I had worked 70-hour weeks every week for almost a year to build it up. We were a tiny team, and then from one day to the next, it was all gone and associated with a giant fraud. That was incredibly tough.
Dwarkesh Patel
Were there any early signs about SBF?

**Extracted Belief:**

The collapse of Future Fund was a difficult experience for Leopold Aschenbrenner.

**Context:**

Leopold Aschenbrenner describes the emotional impact of the collapse of Future Fund, highlighting the personal difficulty of the situation.

**Justification:**

This belief is based on Leopold Aschenbrenner's personal statement that it was incredibly tough, emphasizing the difficulty he experienced.

--------

## Chunk 406

**Chunk:**

Dwarkesh Patel
Were there any early signs about SBF?
Leopold Aschenbrenner
Obviously, I didn’t know he was a fraud. If I had, I would have never worked there. We were a separate entity and didn’t work with the business. I do think there are some takeaways for me.

I, and people in general, had this tendency to give successful CEOs a pass on their behavior because they’re successful. You think that’s just a successful CEO thing. I didn’t know Sam Bankman-Fried was a fraud.

I knew he was extremely risk-taking, narcissistic, and didn’t tolerate disagreement well. By the end, he and I didn’t get along because I pointed out that some biosecurity grants weren’t cost effective but he liked them because they were cool and flashy. He was unhappy about that.

So I knew his character. I realized that it’s really worth paying attention to people’s characters, including people you work for and successful CEOs. That can save you a lot of pain down the line.
Dwarkesh Patel
After FTX imploded and you were out, you went to OpenAI. The superalignment team had just started. You were part of the initial team.

What was the original idea? What compelled you to join?

**Extracted Belief:**

Successful CEOs are often given a pass on their behavior due to their success.

**Context:**

Leopold Aschenbrenner is reflecting on his experience working with Sam Bankman-Fried and acknowledges a tendency to overlook negative traits in successful individuals.

**Justification:**

He states that he and people in general have a tendency to give successful CEOs a pass on their behavior because they’re successful.

--------

## Chunk 407

**Chunk:**

Dwarkesh Patel
Were there any early signs about SBF?
Leopold Aschenbrenner
Obviously, I didn’t know he was a fraud. If I had, I would have never worked there. We were a separate entity and didn’t work with the business. I do think there are some takeaways for me.

I, and people in general, had this tendency to give successful CEOs a pass on their behavior because they’re successful. You think that’s just a successful CEO thing. I didn’t know Sam Bankman-Fried was a fraud.

I knew he was extremely risk-taking, narcissistic, and didn’t tolerate disagreement well. By the end, he and I didn’t get along because I pointed out that some biosecurity grants weren’t cost effective but he liked them because they were cool and flashy. He was unhappy about that.

So I knew his character. I realized that it’s really worth paying attention to people’s characters, including people you work for and successful CEOs. That can save you a lot of pain down the line.
Dwarkesh Patel
After FTX imploded and you were out, you went to OpenAI. The superalignment team had just started. You were part of the initial team.

What was the original idea? What compelled you to join?

**Extracted Belief:**

It is important to pay attention to the character of individuals, including those in positions of power and success.

**Context:**

Leopold Aschenbrenner reflects on his experience with Sam Bankman-Fried and emphasizes the importance of character assessment.

**Justification:**

He states that it’s really worth paying attention to people’s characters, including people you work for and successful CEOs.

--------

## Chunk 408

**Chunk:**

Dwarkesh Patel
Were there any early signs about SBF?
Leopold Aschenbrenner
Obviously, I didn’t know he was a fraud. If I had, I would have never worked there. We were a separate entity and didn’t work with the business. I do think there are some takeaways for me.

I, and people in general, had this tendency to give successful CEOs a pass on their behavior because they’re successful. You think that’s just a successful CEO thing. I didn’t know Sam Bankman-Fried was a fraud.

I knew he was extremely risk-taking, narcissistic, and didn’t tolerate disagreement well. By the end, he and I didn’t get along because I pointed out that some biosecurity grants weren’t cost effective but he liked them because they were cool and flashy. He was unhappy about that.

So I knew his character. I realized that it’s really worth paying attention to people’s characters, including people you work for and successful CEOs. That can save you a lot of pain down the line.
Dwarkesh Patel
After FTX imploded and you were out, you went to OpenAI. The superalignment team had just started. You were part of the initial team.

What was the original idea? What compelled you to join?

**Extracted Belief:**

Paying attention to character can prevent future pain or negative consequences.

**Context:**

Leopold Aschenbrenner emphasizes the importance of character assessment as a way to avoid potential negative outcomes.

**Justification:**

He suggests that paying attention to character can save you a lot of pain down the line.

--------

## Chunk 409

**Chunk:**

Dwarkesh Patel
After FTX imploded and you were out, you went to OpenAI. The superalignment team had just started. You were part of the initial team.

What was the original idea? What compelled you to join?
Leopold Aschenbrenner
The alignment teams at OpenAI and other labs had done basic research and developed RLHF. reinforcement learning from human feedback. That ended up being a really successful technique for controlling current AI models.

Our task was to find the successor to RLHF. The reason we need that is that RLHF probably won’t scale to superhuman systems. RLHF relies on human raters giving feedback, but superintelligent models will produce complex outputs beyond human comprehension. It’ll be like a million lines of complex code and you won’t know at all what’s going on anymore.

How do you steer and control these systems? How do you add side constraints? I joined because I thought this was an important and solvable problem. I still do and even more so. I think there’s a lot of promising ML research on aligning superhuman systems, which we can discuss more later.
Dwarkesh Patel
It was so solvable, you solved it in a year. It’s all over now.

**Extracted Belief:**

RLHF (reinforcement learning from human feedback) is a successful technique for controlling current AI models.

**Context:**

Leopold Aschenbrenner explains the limitations of RLHF and the need for its successor in the context of superintelligent AI systems.

**Justification:**

Leopold Aschenbrenner states that RLHF "ended up being a really successful technique for controlling current AI models."

--------

## Chunk 410

**Chunk:**

Dwarkesh Patel
After FTX imploded and you were out, you went to OpenAI. The superalignment team had just started. You were part of the initial team.

What was the original idea? What compelled you to join?
Leopold Aschenbrenner
The alignment teams at OpenAI and other labs had done basic research and developed RLHF. reinforcement learning from human feedback. That ended up being a really successful technique for controlling current AI models.

Our task was to find the successor to RLHF. The reason we need that is that RLHF probably won’t scale to superhuman systems. RLHF relies on human raters giving feedback, but superintelligent models will produce complex outputs beyond human comprehension. It’ll be like a million lines of complex code and you won’t know at all what’s going on anymore.

How do you steer and control these systems? How do you add side constraints? I joined because I thought this was an important and solvable problem. I still do and even more so. I think there’s a lot of promising ML research on aligning superhuman systems, which we can discuss more later.
Dwarkesh Patel
It was so solvable, you solved it in a year. It’s all over now.

**Extracted Belief:**

RLHF will not scale to superhuman systems.

**Context:**

Leopold Aschenbrenner discusses the need for a successor to RLHF due to its limitations in the context of superintelligent AI systems.

**Justification:**

Leopold Aschenbrenner explains that RLHF "probably won’t scale to superhuman systems" because it relies on human raters, who will be unable to comprehend the complex outputs of superintelligent AI systems.

--------

## Chunk 411

**Chunk:**

Dwarkesh Patel
After FTX imploded and you were out, you went to OpenAI. The superalignment team had just started. You were part of the initial team.

What was the original idea? What compelled you to join?
Leopold Aschenbrenner
The alignment teams at OpenAI and other labs had done basic research and developed RLHF. reinforcement learning from human feedback. That ended up being a really successful technique for controlling current AI models.

Our task was to find the successor to RLHF. The reason we need that is that RLHF probably won’t scale to superhuman systems. RLHF relies on human raters giving feedback, but superintelligent models will produce complex outputs beyond human comprehension. It’ll be like a million lines of complex code and you won’t know at all what’s going on anymore.

How do you steer and control these systems? How do you add side constraints? I joined because I thought this was an important and solvable problem. I still do and even more so. I think there’s a lot of promising ML research on aligning superhuman systems, which we can discuss more later.
Dwarkesh Patel
It was so solvable, you solved it in a year. It’s all over now.

**Extracted Belief:**

Aligning superhuman AI systems is an important and solvable problem.

**Context:**

Leopold Aschenbrenner discusses the challenges of aligning superhuman AI systems and the importance of finding solutions.

**Justification:**

Leopold Aschenbrenner states that he joined the superalignment team because he thought this was an important and solvable problem. He also states that he believes there is a lot of promising ML research on aligning superhuman systems.

--------

## Chunk 412

**Chunk:**

Dwarkesh Patel
It was so solvable, you solved it in a year. It’s all over now.
Leopold Aschenbrenner
OpenAI wanted to do a really ambitious effort on alignment. Ilya was backing it. I liked a lot of the people there. I was really excited. There are always people making hay about alignment. I appreciate people highlighting the importance of the problem and I was just really into trying to solve it. I wanted to do the ambitious effort, like an Operation Warp Speed for solving alignment. It seemed like an amazing opportunity to do it.

(02:30:35) – What happened at OpenAI
Dwarkesh Patel
Now the team basically doesn't exist. The heads of it, Jan and Ilya, have left. That’s been the news of last week. What happened? Why did the team break down?

**Extracted Belief:**

OpenAI was committed to a large-scale, ambitious effort to solve AI alignment.

**Context:**

Leopold Aschenbrenner is describing his motivation for joining the superalignment team at OpenAI.

**Justification:**

He states that OpenAI was committed to a large-scale, ambitious effort on alignment, and that Ilya was backing it.

--------

## Chunk 413

**Chunk:**

Dwarkesh Patel
It was so solvable, you solved it in a year. It’s all over now.
Leopold Aschenbrenner
OpenAI wanted to do a really ambitious effort on alignment. Ilya was backing it. I liked a lot of the people there. I was really excited. There are always people making hay about alignment. I appreciate people highlighting the importance of the problem and I was just really into trying to solve it. I wanted to do the ambitious effort, like an Operation Warp Speed for solving alignment. It seemed like an amazing opportunity to do it.

(02:30:35) – What happened at OpenAI
Dwarkesh Patel
Now the team basically doesn't exist. The heads of it, Jan and Ilya, have left. That’s been the news of last week. What happened? Why did the team break down?

**Extracted Belief:**

AI alignment is an important problem.

**Context:**

Leopold Aschenbrenner is expressing his belief that AI alignment is a worthwhile problem to solve.

**Justification:**

He states that he appreciated people highlighting the importance of the problem and that he was really into trying to solve it.

--------

## Chunk 414

**Chunk:**

Dwarkesh Patel
It was so solvable, you solved it in a year. It’s all over now.
Leopold Aschenbrenner
OpenAI wanted to do a really ambitious effort on alignment. Ilya was backing it. I liked a lot of the people there. I was really excited. There are always people making hay about alignment. I appreciate people highlighting the importance of the problem and I was just really into trying to solve it. I wanted to do the ambitious effort, like an Operation Warp Speed for solving alignment. It seemed like an amazing opportunity to do it.

(02:30:35) – What happened at OpenAI
Dwarkesh Patel
Now the team basically doesn't exist. The heads of it, Jan and Ilya, have left. That’s been the news of last week. What happened? Why did the team break down?

**Extracted Belief:**

The superalignment team's work was a promising opportunity to solve AI alignment.

**Context:**

Leopold Aschenbrenner is expressing his belief in the potential of the superalignment team's work.

**Justification:**

He describes it as an amazing opportunity to solve alignment.

--------

## Chunk 415

**Chunk:**

Dwarkesh Patel
Now the team basically doesn't exist. The heads of it, Jan and Ilya, have left. That’s been the news of last week. What happened? Why did the team break down?
Leopold Aschenbrenner
OpenAI decided to take things in a different direction.
Dwarkesh Patel
Meaning what? That superalignment isn’t the best way to frame it?

**Extracted Belief:**

OpenAI has decided to shift its focus and pursue a different direction in its research and development efforts.

**Context:**

Leopold Aschenbrenner, when asked about the dissolution of the superalignment team at OpenAI, stated that OpenAI had decided to take things in a different direction.

**Justification:**

This belief is based on Leopold Aschenbrenner's testimony as an insider at OpenAI. He attributes the team's dissolution to a change in direction by OpenAI's leadership.

--------

## Chunk 416

**Chunk:**

Dwarkesh Patel
Meaning what? That superalignment isn’t the best way to frame it?
Leopold Aschenbrenner
No, obviously after the November board events there were personnel changes. Ilya leaving was incredibly tragic for OpenAI. There was some reprioritization. There’s been reporting on the superalignment compute commitment, the 20% compute commitment, which was how a lot of people were recruited. There was a decision to not keep that commitment and go in a different direction.
Dwarkesh Patel
Now Jan and Ilya have left, and the team itself has dissolved. You were the first person who left or was forced to leave. The Information reported that you were fired for leaking. What happened? Is this accurate?

**Extracted Belief:**

The departure of Ilya from OpenAI was a significant loss for the organization.

**Context:**

Leopold Aschenbrenner expressed his opinion on the departure of Ilya from OpenAI, a key figure in the organization's alignment efforts.

**Justification:**

Leopold Aschenbrenner stated that Ilya's leaving was "incredibly tragic for OpenAI."

--------

## Chunk 417

**Chunk:**

Dwarkesh Patel
Meaning what? That superalignment isn’t the best way to frame it?
Leopold Aschenbrenner
No, obviously after the November board events there were personnel changes. Ilya leaving was incredibly tragic for OpenAI. There was some reprioritization. There’s been reporting on the superalignment compute commitment, the 20% compute commitment, which was how a lot of people were recruited. There was a decision to not keep that commitment and go in a different direction.
Dwarkesh Patel
Now Jan and Ilya have left, and the team itself has dissolved. You were the first person who left or was forced to leave. The Information reported that you were fired for leaking. What happened? Is this accurate?

**Extracted Belief:**

OpenAI's prioritization of research and development shifted following the November board events.

**Context:**

Leopold Aschenbrenner discussed the changes in OpenAI's direction after the board events in November, including a shift in focus away from superalignment.

**Justification:**

Leopold Aschenbrenner mentioned that there was "some reprioritization" at OpenAI after the November board events.

--------

## Chunk 418

**Chunk:**

Dwarkesh Patel
Meaning what? That superalignment isn’t the best way to frame it?
Leopold Aschenbrenner
No, obviously after the November board events there were personnel changes. Ilya leaving was incredibly tragic for OpenAI. There was some reprioritization. There’s been reporting on the superalignment compute commitment, the 20% compute commitment, which was how a lot of people were recruited. There was a decision to not keep that commitment and go in a different direction.
Dwarkesh Patel
Now Jan and Ilya have left, and the team itself has dissolved. You were the first person who left or was forced to leave. The Information reported that you were fired for leaking. What happened? Is this accurate?

**Extracted Belief:**

OpenAI's decision to not maintain the 20% compute commitment for superalignment led to a change in direction for the organization.

**Context:**

Leopold Aschenbrenner explained that OpenAI's decision to reduce the compute commitment for superalignment impacted the organization's trajectory.

**Justification:**

Leopold Aschenbrenner stated that OpenAI decided "to not keep that commitment and go in a different direction."

--------

## Chunk 419

**Chunk:**

Dwarkesh Patel
Now Jan and Ilya have left, and the team itself has dissolved. You were the first person who left or was forced to leave. The Information reported that you were fired for leaking. What happened? Is this accurate?
Leopold Aschenbrenner
Why don’t I tell you what they claim I leaked, and you can tell me what you think. OpenAI claimed to employees that I was fired for leaking. I and others have pushed them to say what the leak was. Here’s their response in full: Sometime last year, I had written a brainstorming document on preparedness, safety, and security measures needed in the future on the path to AGI. I shared that with three external researchers for feedback. That’s the leak.

For context, it was totally normal at OpenAI at the time to share safety ideas with external researchers for feedback. It happened all the time. The doc had my ideas. Before I shared it, I reviewed it for anything sensitive. The internal version had a reference to a future cluster, which I redacted for the external copy. There was a link to some internal slides, but that was a dead link for the external people. The slides weren’t shared with them.

When I pressed them to specify what confidential information was in this document. They came back with a line about planning for AGI by 2027-2028 and not setting timelines for preparedness.

I wrote this doc a couple of months after the superalignment announcement. We had put out a four-year planning horizon. I didn’t think that planning horizon was sensitive. It’s the sort of thing Sam says publicly all the time. I think Jan mentioned it on a podcast a couple of weeks ago. So, that’s it.
Dwarkesh Patel
That’s it? That sounds pretty thin if the cause was leaking. Was there anything else to it?

**Extracted Belief:**

Sharing safety ideas with external researchers for feedback was a common practice at OpenAI.

**Context:**

Leopold Aschenbrenner defended his actions of sharing a brainstorming document with external researchers by stating that it was a common practice at OpenAI to share safety ideas with external researchers for feedback.

**Justification:**

He explicitly stated, "For context, it was totally normal at OpenAI at the time to share safety ideas with external researchers for feedback. It happened all the time."

--------

## Chunk 420

**Chunk:**

Dwarkesh Patel
Now Jan and Ilya have left, and the team itself has dissolved. You were the first person who left or was forced to leave. The Information reported that you were fired for leaking. What happened? Is this accurate?
Leopold Aschenbrenner
Why don’t I tell you what they claim I leaked, and you can tell me what you think. OpenAI claimed to employees that I was fired for leaking. I and others have pushed them to say what the leak was. Here’s their response in full: Sometime last year, I had written a brainstorming document on preparedness, safety, and security measures needed in the future on the path to AGI. I shared that with three external researchers for feedback. That’s the leak.

For context, it was totally normal at OpenAI at the time to share safety ideas with external researchers for feedback. It happened all the time. The doc had my ideas. Before I shared it, I reviewed it for anything sensitive. The internal version had a reference to a future cluster, which I redacted for the external copy. There was a link to some internal slides, but that was a dead link for the external people. The slides weren’t shared with them.

When I pressed them to specify what confidential information was in this document. They came back with a line about planning for AGI by 2027-2028 and not setting timelines for preparedness.

I wrote this doc a couple of months after the superalignment announcement. We had put out a four-year planning horizon. I didn’t think that planning horizon was sensitive. It’s the sort of thing Sam says publicly all the time. I think Jan mentioned it on a podcast a couple of weeks ago. So, that’s it.
Dwarkesh Patel
That’s it? That sounds pretty thin if the cause was leaking. Was there anything else to it?

**Extracted Belief:**

The planning horizon for AGI development, which was set at four years, was not sensitive information.

**Context:**

Leopold Aschenbrenner expressed his belief that the planning horizon for AGI development was not sensitive information, citing Sam Altman's public statements and Jan's recent podcast discussion.

**Justification:**

He stated, "I didn’t think that planning horizon was sensitive. It’s the sort of thing Sam says publicly all the time. I think Jan mentioned it on a podcast a couple of weeks ago."

--------

## Chunk 421

**Chunk:**

Dwarkesh Patel
That’s it? That sounds pretty thin if the cause was leaking. Was there anything else to it?
Leopold Aschenbrenner
That was the leaking claim. Let me explain more about what happened during the firing. Last year, I wrote an internal memo about OpenAI's security, which I thought was egregiously insufficient to protect against the theft of model weights or key algorithmic secrets from foreign actors. I shared this memo with a few colleagues and a couple of members of leadership, who mostly said it was helpful.

A few weeks later, a major security incident occurred1. That prompted me to share the memo with a couple of board members. Days later, it was made very clear to me that leadership was very unhappy I had shared this memo with the board. Apparently, the board hassled leadership about security.

I got an official HR warning for sharing the memo with the board. The HR person told me it was racist to worry about CCP espionage and that it was unconstructive. I probably wasn’t at my most diplomatic and could have been more politically savvy. I thought it was a really important issue. The security incident made me very worried.

The reason I bring this up is that when I was fired, it was very made explicit that the security memo was a major reason for my being fired. They said, "the reason this is a firing and not a warning is because of the security memo."
Dwarkesh Patel
You sharing it with the board?

**Extracted Belief:**

OpenAI's security measures were insufficient to protect against the theft of model weights or key algorithmic secrets from foreign actors.

**Context:**

Leopold Aschenbrenner expressed his concern about OpenAI's security posture, specifically regarding the risk of theft of sensitive information by foreign actors.

**Justification:**

He stated that he wrote an internal memo about OpenAI's security, which he believed was 'egregiously insufficient' to protect against the theft of model weights and algorithmic secrets from foreign actors. He explicitly mentioned concerns about 'CCP espionage'.

--------

## Chunk 422

**Chunk:**

Dwarkesh Patel
That’s it? That sounds pretty thin if the cause was leaking. Was there anything else to it?
Leopold Aschenbrenner
That was the leaking claim. Let me explain more about what happened during the firing. Last year, I wrote an internal memo about OpenAI's security, which I thought was egregiously insufficient to protect against the theft of model weights or key algorithmic secrets from foreign actors. I shared this memo with a few colleagues and a couple of members of leadership, who mostly said it was helpful.

A few weeks later, a major security incident occurred1. That prompted me to share the memo with a couple of board members. Days later, it was made very clear to me that leadership was very unhappy I had shared this memo with the board. Apparently, the board hassled leadership about security.

I got an official HR warning for sharing the memo with the board. The HR person told me it was racist to worry about CCP espionage and that it was unconstructive. I probably wasn’t at my most diplomatic and could have been more politically savvy. I thought it was a really important issue. The security incident made me very worried.

The reason I bring this up is that when I was fired, it was very made explicit that the security memo was a major reason for my being fired. They said, "the reason this is a firing and not a warning is because of the security memo."
Dwarkesh Patel
You sharing it with the board?

**Extracted Belief:**

Sharing a security memo with the board of directors is a serious offense that could lead to termination.

**Context:**

Leopold Aschenbrenner stated that OpenAI leadership explicitly cited the security memo as a major reason for his termination.

**Justification:**

He recounted that OpenAI's leadership communicated to him that the reason for his termination was the sharing of the security memo with the board. This implies a belief that sharing this information with the board was a significant breach of trust or company protocol.

--------

## Chunk 423

**Chunk:**

Dwarkesh Patel
That’s it? That sounds pretty thin if the cause was leaking. Was there anything else to it?
Leopold Aschenbrenner
That was the leaking claim. Let me explain more about what happened during the firing. Last year, I wrote an internal memo about OpenAI's security, which I thought was egregiously insufficient to protect against the theft of model weights or key algorithmic secrets from foreign actors. I shared this memo with a few colleagues and a couple of members of leadership, who mostly said it was helpful.

A few weeks later, a major security incident occurred1. That prompted me to share the memo with a couple of board members. Days later, it was made very clear to me that leadership was very unhappy I had shared this memo with the board. Apparently, the board hassled leadership about security.

I got an official HR warning for sharing the memo with the board. The HR person told me it was racist to worry about CCP espionage and that it was unconstructive. I probably wasn’t at my most diplomatic and could have been more politically savvy. I thought it was a really important issue. The security incident made me very worried.

The reason I bring this up is that when I was fired, it was very made explicit that the security memo was a major reason for my being fired. They said, "the reason this is a firing and not a warning is because of the security memo."
Dwarkesh Patel
You sharing it with the board?

**Extracted Belief:**

Worrying about CCP espionage is not a constructive or appropriate concern.

**Context:**

Leopold Aschenbrenner described the HR representative's response to his security concerns, specifically the concerns about CCP espionage.

**Justification:**

The HR representative, according to Leopold, stated that worrying about CCP espionage was 'racist' and 'unconstructive'. This implies a belief that concerns about CCP espionage are unfounded or unproductive.

--------

## Chunk 424

**Chunk:**

Dwarkesh Patel
You sharing it with the board?
Leopold Aschenbrenner
The warning I’d gotten for the security memo.

What might also be helpful context is the kinds of questions they asked me when they fired me. A bit over a month ago, I was pulled aside for a chat with a lawyer that quickly turned adversarial. The questions were about my views on AI progress, on AGI, the appropriate level of security for AGI, whether the government should be involved in AGI, whether I and the superalignment team were loyal to the company, and what I was up to during the OpenAI board events. They then talked to a couple of my colleagues and came back and told me I was fired. They’d gone through all of my digital artifacts from my time at OpenAI, and that’s when they found the leak.

The main claim they made was this leaking allegation. That’s what they told employees. The security memo was another thing. There were a couple of other allegations they threw in. One thing they said was that I was unforthcoming during the investigation because I didn’t initially remember who I had shared the preparedness brainstorming document with, only that I had talked to some external researchers about these ideas.

The document was over six months old, I’d spent a day on it. It was a Google Doc I shared with my OpenAI email. It wasn’t a screenshot or anything I was trying to hide. It simply didn’t stick because it was such a non-issue. They also claimed I was engaging on policy in a way they didn’t like. They cited there that I had spoken to a couple of external researchers, including someone at a think tank, about my view that AGI would become a government project, as we just discussed.

In fact, I was speaking with lots of people in the field about that view at the time. I thought it was a really important thing to think about. So they found a DM I had written to a friendly colleague, five or six months earlier, and they cited that too. I had thought it was well within OpenAI norms to discuss high-level issues about the future of AGI with external people in the field.

That’s what they allege happened. I’ve spoken to a few dozen former colleagues about this since. The universal reaction has been, "that’s insane." I was surprised as well. I had been promoted just a few months before. Ilya’s comment for the promotion case at the time was something like, "Leopold’s amazing. We’re lucky to have him."

The thing I understand, and in some sense it’s reasonable, is that I ruffled some feathers and was probably annoying at times with the security stuff. I repeatedly raised that, maybe not always in the most diplomatic way. I didn’t sign the employee letter during the board events, despite pressure to do so.
Dwarkesh Patel
You were one of like eight people or something?

**Extracted Belief:**

Sharing a brainstorming document with external researchers for feedback is a common practice at OpenAI.

**Context:**

Leopold Aschenbrenner is defending his actions of sharing a document with external researchers by stating that it was a common practice at OpenAI.

**Justification:**

Leopold Aschenbrenner mentions that sharing safety ideas with external researchers for feedback was "totally normal" at OpenAI at the time.

--------

## Chunk 425

**Chunk:**

Dwarkesh Patel
You sharing it with the board?
Leopold Aschenbrenner
The warning I’d gotten for the security memo.

What might also be helpful context is the kinds of questions they asked me when they fired me. A bit over a month ago, I was pulled aside for a chat with a lawyer that quickly turned adversarial. The questions were about my views on AI progress, on AGI, the appropriate level of security for AGI, whether the government should be involved in AGI, whether I and the superalignment team were loyal to the company, and what I was up to during the OpenAI board events. They then talked to a couple of my colleagues and came back and told me I was fired. They’d gone through all of my digital artifacts from my time at OpenAI, and that’s when they found the leak.

The main claim they made was this leaking allegation. That’s what they told employees. The security memo was another thing. There were a couple of other allegations they threw in. One thing they said was that I was unforthcoming during the investigation because I didn’t initially remember who I had shared the preparedness brainstorming document with, only that I had talked to some external researchers about these ideas.

The document was over six months old, I’d spent a day on it. It was a Google Doc I shared with my OpenAI email. It wasn’t a screenshot or anything I was trying to hide. It simply didn’t stick because it was such a non-issue. They also claimed I was engaging on policy in a way they didn’t like. They cited there that I had spoken to a couple of external researchers, including someone at a think tank, about my view that AGI would become a government project, as we just discussed.

In fact, I was speaking with lots of people in the field about that view at the time. I thought it was a really important thing to think about. So they found a DM I had written to a friendly colleague, five or six months earlier, and they cited that too. I had thought it was well within OpenAI norms to discuss high-level issues about the future of AGI with external people in the field.

That’s what they allege happened. I’ve spoken to a few dozen former colleagues about this since. The universal reaction has been, "that’s insane." I was surprised as well. I had been promoted just a few months before. Ilya’s comment for the promotion case at the time was something like, "Leopold’s amazing. We’re lucky to have him."

The thing I understand, and in some sense it’s reasonable, is that I ruffled some feathers and was probably annoying at times with the security stuff. I repeatedly raised that, maybe not always in the most diplomatic way. I didn’t sign the employee letter during the board events, despite pressure to do so.
Dwarkesh Patel
You were one of like eight people or something?

**Extracted Belief:**

It is appropriate to discuss high-level issues about the future of AGI with external people in the field.

**Context:**

Leopold Aschenbrenner justifies his conversations with external researchers about the future of AGI by stating that it was within OpenAI norms.

**Justification:**

Leopold Aschenbrenner states that he "thought it was well within OpenAI norms to discuss high-level issues about the future of AGI with external people in the field.

--------

## Chunk 426

**Chunk:**

Dwarkesh Patel
You were one of like eight people or something?
Leopold Aschenbrenner
Not that many people. I think the two most senior people who didn’t sign were Andrej and Jan, who have both since left.

On the letter, by Monday morning when it was circulating, I thought it was probably appropriate for the board to resign because they had lost too much credibility and trust with the employees.

But I thought the letter had issues. It didn’t call for an independent board, which is a basic of corporate governance. In other discussions, I pressed leadership for OpenAI to abide by its public commitments. I raised tough questions about whether it was consistent with the OpenAI mission and the national interest to partner with authoritarian dictatorships to build the core infrastructure for AGI.

It’s a free country. That’s what I love about it. We talked about it. They have no obligation to keep me on staff. It would have been reasonable for them to come to me and say, "we’re taking the company in a different direction. We disagree with your point of view. We don’t trust you to toe the company line anymore. Thank you so much for your work at OpenAI, but it’s time to part ways."

That would have made sense. We had started diverging on important issues. I came in very excited and aligned with OpenAI, but that changed over time. That would have been a very amicable way to part ways. It’s a shame how it went down.

All that being said, I really want to emphasize that there are a lot of incredible people at OpenAI, and it was an incredible privilege to work with them. Overall, I’m extremely grateful for my time there.
Dwarkesh Patel
Now there’s been reporting about an NDA that former employees have to sign to access their vested equity. Did you sign such an NDA?

**Extracted Belief:**

It is appropriate for a corporate board to resign if they have lost the credibility and trust of employees.

**Context:**

Leopold Aschenbrenner stated his belief about the appropriateness of a board's resignation based on his observation of the situation at OpenAI.

**Justification:**

Leopold Aschenbrenner was discussing the events surrounding the circulation of a letter signed by OpenAI employees. He states that he thought it was appropriate for the board to resign as they had lost the trust and credibility of the employees.

--------

## Chunk 427

**Chunk:**

Dwarkesh Patel
You were one of like eight people or something?
Leopold Aschenbrenner
Not that many people. I think the two most senior people who didn’t sign were Andrej and Jan, who have both since left.

On the letter, by Monday morning when it was circulating, I thought it was probably appropriate for the board to resign because they had lost too much credibility and trust with the employees.

But I thought the letter had issues. It didn’t call for an independent board, which is a basic of corporate governance. In other discussions, I pressed leadership for OpenAI to abide by its public commitments. I raised tough questions about whether it was consistent with the OpenAI mission and the national interest to partner with authoritarian dictatorships to build the core infrastructure for AGI.

It’s a free country. That’s what I love about it. We talked about it. They have no obligation to keep me on staff. It would have been reasonable for them to come to me and say, "we’re taking the company in a different direction. We disagree with your point of view. We don’t trust you to toe the company line anymore. Thank you so much for your work at OpenAI, but it’s time to part ways."

That would have made sense. We had started diverging on important issues. I came in very excited and aligned with OpenAI, but that changed over time. That would have been a very amicable way to part ways. It’s a shame how it went down.

All that being said, I really want to emphasize that there are a lot of incredible people at OpenAI, and it was an incredible privilege to work with them. Overall, I’m extremely grateful for my time there.
Dwarkesh Patel
Now there’s been reporting about an NDA that former employees have to sign to access their vested equity. Did you sign such an NDA?

**Extracted Belief:**

An independent board is a basic principle of corporate governance.

**Context:**

Leopold Aschenbrenner is discussing his views on corporate governance and the events surrounding the circulation of the OpenAI employee letter.

**Justification:**

Leopold Aschenbrenner states that the letter circulating among employees did not call for an independent board, which he believes is a basic principle of corporate governance.

--------

## Chunk 428

**Chunk:**

Dwarkesh Patel
You were one of like eight people or something?
Leopold Aschenbrenner
Not that many people. I think the two most senior people who didn’t sign were Andrej and Jan, who have both since left.

On the letter, by Monday morning when it was circulating, I thought it was probably appropriate for the board to resign because they had lost too much credibility and trust with the employees.

But I thought the letter had issues. It didn’t call for an independent board, which is a basic of corporate governance. In other discussions, I pressed leadership for OpenAI to abide by its public commitments. I raised tough questions about whether it was consistent with the OpenAI mission and the national interest to partner with authoritarian dictatorships to build the core infrastructure for AGI.

It’s a free country. That’s what I love about it. We talked about it. They have no obligation to keep me on staff. It would have been reasonable for them to come to me and say, "we’re taking the company in a different direction. We disagree with your point of view. We don’t trust you to toe the company line anymore. Thank you so much for your work at OpenAI, but it’s time to part ways."

That would have made sense. We had started diverging on important issues. I came in very excited and aligned with OpenAI, but that changed over time. That would have been a very amicable way to part ways. It’s a shame how it went down.

All that being said, I really want to emphasize that there are a lot of incredible people at OpenAI, and it was an incredible privilege to work with them. Overall, I’m extremely grateful for my time there.
Dwarkesh Patel
Now there’s been reporting about an NDA that former employees have to sign to access their vested equity. Did you sign such an NDA?

**Extracted Belief:**

It is consistent with the national interest to partner with authoritarian dictatorships to build core infrastructure for artificial general intelligence (AGI).

**Context:**

Leopold Aschenbrenner is expressing his belief about the potential dangers of partnering with authoritarian regimes in the development of AGI.

**Justification:**

Leopold Aschenbrenner, during a discussion about OpenAI's mission, expresses concern about potential partnerships with authoritarian regimes. He suggests that such partnerships might not be consistent with OpenAI's mission and the national interest.

--------

## Chunk 429

**Chunk:**

Dwarkesh Patel
You were one of like eight people or something?
Leopold Aschenbrenner
Not that many people. I think the two most senior people who didn’t sign were Andrej and Jan, who have both since left.

On the letter, by Monday morning when it was circulating, I thought it was probably appropriate for the board to resign because they had lost too much credibility and trust with the employees.

But I thought the letter had issues. It didn’t call for an independent board, which is a basic of corporate governance. In other discussions, I pressed leadership for OpenAI to abide by its public commitments. I raised tough questions about whether it was consistent with the OpenAI mission and the national interest to partner with authoritarian dictatorships to build the core infrastructure for AGI.

It’s a free country. That’s what I love about it. We talked about it. They have no obligation to keep me on staff. It would have been reasonable for them to come to me and say, "we’re taking the company in a different direction. We disagree with your point of view. We don’t trust you to toe the company line anymore. Thank you so much for your work at OpenAI, but it’s time to part ways."

That would have made sense. We had started diverging on important issues. I came in very excited and aligned with OpenAI, but that changed over time. That would have been a very amicable way to part ways. It’s a shame how it went down.

All that being said, I really want to emphasize that there are a lot of incredible people at OpenAI, and it was an incredible privilege to work with them. Overall, I’m extremely grateful for my time there.
Dwarkesh Patel
Now there’s been reporting about an NDA that former employees have to sign to access their vested equity. Did you sign such an NDA?

**Extracted Belief:**

The United States is a free country where people have the right to express their opinions.

**Context:**

Leopold Aschenbrenner is justifying his decision to express his beliefs and disagree with OpenAI's direction.

**Justification:**

Leopold Aschenbrenner states that, due to the freedom he enjoys in the US, OpenAI has no obligation to keep him employed. He is expressing his right to express his opinion and disagree with the company's direction.

--------

## Chunk 430

**Chunk:**

Dwarkesh Patel
You were one of like eight people or something?
Leopold Aschenbrenner
Not that many people. I think the two most senior people who didn’t sign were Andrej and Jan, who have both since left.

On the letter, by Monday morning when it was circulating, I thought it was probably appropriate for the board to resign because they had lost too much credibility and trust with the employees.

But I thought the letter had issues. It didn’t call for an independent board, which is a basic of corporate governance. In other discussions, I pressed leadership for OpenAI to abide by its public commitments. I raised tough questions about whether it was consistent with the OpenAI mission and the national interest to partner with authoritarian dictatorships to build the core infrastructure for AGI.

It’s a free country. That’s what I love about it. We talked about it. They have no obligation to keep me on staff. It would have been reasonable for them to come to me and say, "we’re taking the company in a different direction. We disagree with your point of view. We don’t trust you to toe the company line anymore. Thank you so much for your work at OpenAI, but it’s time to part ways."

That would have made sense. We had started diverging on important issues. I came in very excited and aligned with OpenAI, but that changed over time. That would have been a very amicable way to part ways. It’s a shame how it went down.

All that being said, I really want to emphasize that there are a lot of incredible people at OpenAI, and it was an incredible privilege to work with them. Overall, I’m extremely grateful for my time there.
Dwarkesh Patel
Now there’s been reporting about an NDA that former employees have to sign to access their vested equity. Did you sign such an NDA?

**Extracted Belief:**

It is reasonable for a company to part ways with an employee if their views diverge on important issues, even if the employee is initially aligned with the company.

**Context:**

Leopold Aschenbrenner is reflecting on his experience at OpenAI, acknowledging differences in opinion and the company's right to make staffing decisions.

**Justification:**

Leopold Aschenbrenner states that it would have been reasonable for OpenAI to inform him of their decision to part ways if his views had diverged from the company's direction. This suggests that he believes it is a reasonable practice to align company values and employee perspectives.

--------

## Chunk 431

**Chunk:**

Dwarkesh Patel
Now there’s been reporting about an NDA that former employees have to sign to access their vested equity. Did you sign such an NDA?
Leopold Aschenbrenner
No. My situation was a little different because I was right before my cliff. They still offered me the equity, but I didn’t want to sign. Freedom is priceless
Dwarkesh Patel
How much was the equity?

**Extracted Belief:**

Freedom is more valuable than money.

**Context:**

Leopold Aschenbrenner was offered a large sum of equity in OpenAI, but he declined to sign an NDA to receive it, saying he valued freedom more.

**Justification:**

He stated that "Freedom is priceless." This indicates a belief that freedom has intrinsic value and is more important than material possessions.

--------

## Chunk 432

**Chunk:**

Dwarkesh Patel
How much was the equity?
Leopold Aschenbrenner
Close to a million dollars.
Dwarkesh Patel
So it was definitely something you and others were aware of. OpenAI explicitly offered you a choice. Presumably, the person on OpenAI staff knew they were offering equity but required signing an NDA that prevents making statements about AGI and OpenAI, like the ones you’re making on this podcast.

**Extracted Belief:**

Conditioning vested equity on signing an NDA is harsh or unreasonable.

**Context:**

Leopold Aschenbrenner expresses his opinion on OpenAI's practice of requiring employees to sign an NDA to access vested equity.

**Justification:**

Leopold Aschenbrenner states "I certainly think conditioning vested equity on signing an NDA is pretty rough." He does not provide specific evidence but implies his opinion is based on his personal experience and understanding of ethical corporate practices.

--------

## Chunk 433

**Chunk:**

Dwarkesh Patel
How much was the equity?
Leopold Aschenbrenner
Close to a million dollars.
Dwarkesh Patel
So it was definitely something you and others were aware of. OpenAI explicitly offered you a choice. Presumably, the person on OpenAI staff knew they were offering equity but required signing an NDA that prevents making statements about AGI and OpenAI, like the ones you’re making on this podcast.

**Extracted Belief:**

It might be acceptable to require an NDA in a severance agreement.

**Context:**

Leopold Aschenbrenner contrasts the practice of requiring an NDA for vested equity with a severance agreement, suggesting a possible exception.

**Justification:**

Leopold Aschenbrenner states "It might be different if it’s a severance agreement." He acknowledges a possible scenario where such an NDA might be more justifiable, but doesn't provide specific reasons.

--------

## Chunk 434

**Chunk:**

Dwarkesh Patel
So it was definitely something you and others were aware of. OpenAI explicitly offered you a choice. Presumably, the person on OpenAI staff knew they were offering equity but required signing an NDA that prevents making statements about AGI and OpenAI, like the ones you’re making on this podcast.
Leopold Aschenbrenner
I don’t know the whole situation. I certainly think conditioning vested equity on signing an NDA is pretty rough. It might be different if it’s a severance agreement.
Dwarkesh Patel
Right, but an OpenAI employee who had signed it presumably couldn’t give the podcast you’re giving today.

**Extracted Belief:**

Conditioning vested equity on signing an NDA is 'pretty rough'.

**Context:**

Leopold Aschenbrenner is expressing his opinion on the practice of requiring employees to sign an NDA as a condition for receiving vested equity. He believes that this practice is unfair and potentially harmful.

**Justification:**

Leopold Aschenbrenner states that he thinks conditioning vested equity on signing an NDA is 'pretty rough'. This statement is based on his personal opinion and experience.

--------

## Chunk 435

**Chunk:**

Dwarkesh Patel
So it was definitely something you and others were aware of. OpenAI explicitly offered you a choice. Presumably, the person on OpenAI staff knew they were offering equity but required signing an NDA that prevents making statements about AGI and OpenAI, like the ones you’re making on this podcast.
Leopold Aschenbrenner
I don’t know the whole situation. I certainly think conditioning vested equity on signing an NDA is pretty rough. It might be different if it’s a severance agreement.
Dwarkesh Patel
Right, but an OpenAI employee who had signed it presumably couldn’t give the podcast you’re giving today.

**Extracted Belief:**

It might be different if it’s a severance agreement.

**Context:**

Leopold Aschenbrenner is suggesting that requiring employees to sign an NDA in a severance agreement is potentially more acceptable than requiring it as a condition for receiving vested equity.

**Justification:**

Leopold Aschenbrenner states that requiring an NDA in a severance agreement might be different. This is based on his personal opinion and understanding of the nature of severance agreements.

--------

## Chunk 436

**Chunk:**

Dwarkesh Patel
Right, but an OpenAI employee who had signed it presumably couldn’t give the podcast you’re giving today.
Leopold Aschenbrenner
Quite possibly not. I don’t know.
Dwarkesh Patel
The board thing is really tough. Analyzing the situation here, if you were trying to defend them, you might say, "well, listen you were just going outside the regular chain of command." There might be a point there.

Although the idea that HR thinks you’re supposed to have an adversarial relationship with the board is odd. You’re giving the board relevant information about whether OpenAI is fulfilling its mission and how it can improve. That seems important since the board is supposed to ensure OpenAI follows its mission. Them treating that as part of the leak, as if the board were an external actor…

**Extracted Belief:**

The board of OpenAI should be considered an internal actor rather than an external one.

**Context:**

Leopold Aschenbrenner expresses his belief that the board of OpenAI should be treated as an internal actor, despite their decision to consider him an external actor for leaking a security memo.

**Justification:**

He justifies his belief by arguing that the board is responsible for ensuring OpenAI fulfills its mission. Therefore, he states that providing information to the board about OpenAI's performance is a part of his responsibility as an employee, not a leak.

--------

## Chunk 437

**Chunk:**

Dwarkesh Patel
The board thing is really tough. Analyzing the situation here, if you were trying to defend them, you might say, "well, listen you were just going outside the regular chain of command." There might be a point there.

Although the idea that HR thinks you’re supposed to have an adversarial relationship with the board is odd. You’re giving the board relevant information about whether OpenAI is fulfilling its mission and how it can improve. That seems important since the board is supposed to ensure OpenAI follows its mission. Them treating that as part of the leak, as if the board were an external actor…
Leopold Aschenbrenner
To be clear, the leak allegation was just about that document I shared for feedback. This is a separate issue they cited. They said I wouldn’t have been fired if not for the security memo.
Dwarkesh Patel
They said you wouldn’t have been fired for it.

**Extracted Belief:**

The security memo was the reason for my firing, not the document I shared for feedback.

**Context:**

Leopold Aschenbrenner is clarifying that his firing was due to the security memo, and not the document he shared for feedback, which was the subject of the 'leak' allegation.

**Justification:**

Leopold Aschenbrenner is referring to a specific incident related to a security memo that led to his firing. This information is based on the testimony of OpenAI leadership.

--------

## Chunk 438

**Chunk:**

Dwarkesh Patel
They said you wouldn’t have been fired for it.
Leopold Aschenbrenner
They said the reason this is a firing and not a warning is because of the warning I had gotten for the security memo.
Dwarkesh Patel
Before you left, the incidents with the board happened. Sam was fired and then rehired as CEO, and now he’s on the board. Ilya and Jan, who were the heads of the superalignment team, have left. Ilya, was a co-founder of OpenAI and the most significant member of OpenAI from a research perspective. There has been a lot of personnel drama over the last few months regarding superalignment and just generally with the OpenAI personnel drama. What’s going on?

**Extracted Belief:**

OpenAI employees have been fired for actions that would normally result in warnings.

**Context:**

Leopold Aschenbrenner explains why he was fired from OpenAI, stating that it was due to a prior warning he received.

**Justification:**

He states that he was fired for an action that would typically result in a warning, implying this is not standard practice at OpenAI.

--------

## Chunk 439

**Chunk:**

Dwarkesh Patel
Before you left, the incidents with the board happened. Sam was fired and then rehired as CEO, and now he’s on the board. Ilya and Jan, who were the heads of the superalignment team, have left. Ilya, was a co-founder of OpenAI and the most significant member of OpenAI from a research perspective. There has been a lot of personnel drama over the last few months regarding superalignment and just generally with the OpenAI personnel drama. What’s going on?
Leopold Aschenbrenner
There’s a lot of drama. Why is there so much drama?

There would be much less drama if all OpenAI claimed to be was building ChatGPT or business software. A lot of the drama comes from OpenAI really believing they’re building AGI. That isn’t just a marketing claim. There’s a report that Sam is raising $7 trillion for chips. That only makes sense if you really believe in AGI.

What gets people is the cognitive dissonance between believing in AGI and not taking some of the other implications seriously. This technology will be incredibly powerful, both for good and bad. That implicates national security issues. Are you protecting the secrets from the CCP? Does America control the core AGI infrastructure or does a Middle Eastern dictator control it?

The thing that really gets people is the tendency to make commitments and say they take these issues seriously, but then frequently not follow. For instance, as mentioned, there was a commitment around superalignment compute, dedicating 20% of compute for long-term safety research.

You and I could have a totally reasonable debate about the appropriate level of compute for superalignment. That’s not really the issue. The issue is that the commitment was made and it was used to recruit people. It was very public.

It was made because there was a recognition that there would always be something more urgent than long-term safety research, like a new product. In the end, they just didn’t keep the commitment. There was always something more urgent than long-term safety research.

Another example is when I raised security issues. They would tell me security is our number one priority. Invariably, when it came time to invest serious resources or make trade-offs to take basic measures, security was not prioritized. The cognitive dissonance and unreliability cause a lot of the drama.

(02:45:11) – Accelerating AI research progress
Dwarkesh Patel
Let’s zoom out and talk about a big part of the story. A big motivation for the way we must proceed with regards to geopolitics is that once you have AGI, you soon proceed to ASI, or superintelligence. You have these AGIs functioning as researchers into further AI progress and within a matter of years, maybe less, you reach superintelligence. From there, according to your story, you do all this research and development into robotics, pocket nukes, and other crazy shit.

I’m skeptical of this story for many reasons. At a high level, it’s not clear to me that this input-output model of research is how things actually happen in research. We can look at the economy as a whole. Patrick Collison and others have pointed out that, compared to 100 years ago, we have 100x more researchers in the world. Yet progress isn’t happening 100 times faster. It's clearly not as simple as pumping in more researchers to get higher research output. I don't see why it would be different for AI researchers.

**Extracted Belief:**

OpenAI is building Artificial General Intelligence (AGI).

**Context:**

Leopold Aschenbrenner expresses his belief that OpenAI's ambition is to build AGI, leading to significant internal drama due to the dissonance between this belief and other actions.

**Justification:**

Leopold Aschenbrenner states that OpenAI is building AGI, citing Sam's $7 trillion chip fundraising as evidence for this belief.

--------

## Chunk 440

**Chunk:**

Dwarkesh Patel
Before you left, the incidents with the board happened. Sam was fired and then rehired as CEO, and now he’s on the board. Ilya and Jan, who were the heads of the superalignment team, have left. Ilya, was a co-founder of OpenAI and the most significant member of OpenAI from a research perspective. There has been a lot of personnel drama over the last few months regarding superalignment and just generally with the OpenAI personnel drama. What’s going on?
Leopold Aschenbrenner
There’s a lot of drama. Why is there so much drama?

There would be much less drama if all OpenAI claimed to be was building ChatGPT or business software. A lot of the drama comes from OpenAI really believing they’re building AGI. That isn’t just a marketing claim. There’s a report that Sam is raising $7 trillion for chips. That only makes sense if you really believe in AGI.

What gets people is the cognitive dissonance between believing in AGI and not taking some of the other implications seriously. This technology will be incredibly powerful, both for good and bad. That implicates national security issues. Are you protecting the secrets from the CCP? Does America control the core AGI infrastructure or does a Middle Eastern dictator control it?

The thing that really gets people is the tendency to make commitments and say they take these issues seriously, but then frequently not follow. For instance, as mentioned, there was a commitment around superalignment compute, dedicating 20% of compute for long-term safety research.

You and I could have a totally reasonable debate about the appropriate level of compute for superalignment. That’s not really the issue. The issue is that the commitment was made and it was used to recruit people. It was very public.

It was made because there was a recognition that there would always be something more urgent than long-term safety research, like a new product. In the end, they just didn’t keep the commitment. There was always something more urgent than long-term safety research.

Another example is when I raised security issues. They would tell me security is our number one priority. Invariably, when it came time to invest serious resources or make trade-offs to take basic measures, security was not prioritized. The cognitive dissonance and unreliability cause a lot of the drama.

(02:45:11) – Accelerating AI research progress
Dwarkesh Patel
Let’s zoom out and talk about a big part of the story. A big motivation for the way we must proceed with regards to geopolitics is that once you have AGI, you soon proceed to ASI, or superintelligence. You have these AGIs functioning as researchers into further AI progress and within a matter of years, maybe less, you reach superintelligence. From there, according to your story, you do all this research and development into robotics, pocket nukes, and other crazy shit.

I’m skeptical of this story for many reasons. At a high level, it’s not clear to me that this input-output model of research is how things actually happen in research. We can look at the economy as a whole. Patrick Collison and others have pointed out that, compared to 100 years ago, we have 100x more researchers in the world. Yet progress isn’t happening 100 times faster. It's clearly not as simple as pumping in more researchers to get higher research output. I don't see why it would be different for AI researchers.

**Extracted Belief:**

AGI will be incredibly powerful, both for good and bad, and has significant national security implications.

**Context:**

While discussing the consequences of building AGI, Leopold Aschenbrenner emphasizes the potential for both positive and negative impacts on national security.

**Justification:**

He highlights concerns such as protecting secrets from the CCP, and the control of AGI infrastructure.

--------

## Chunk 441

**Chunk:**

Dwarkesh Patel
Before you left, the incidents with the board happened. Sam was fired and then rehired as CEO, and now he’s on the board. Ilya and Jan, who were the heads of the superalignment team, have left. Ilya, was a co-founder of OpenAI and the most significant member of OpenAI from a research perspective. There has been a lot of personnel drama over the last few months regarding superalignment and just generally with the OpenAI personnel drama. What’s going on?
Leopold Aschenbrenner
There’s a lot of drama. Why is there so much drama?

There would be much less drama if all OpenAI claimed to be was building ChatGPT or business software. A lot of the drama comes from OpenAI really believing they’re building AGI. That isn’t just a marketing claim. There’s a report that Sam is raising $7 trillion for chips. That only makes sense if you really believe in AGI.

What gets people is the cognitive dissonance between believing in AGI and not taking some of the other implications seriously. This technology will be incredibly powerful, both for good and bad. That implicates national security issues. Are you protecting the secrets from the CCP? Does America control the core AGI infrastructure or does a Middle Eastern dictator control it?

The thing that really gets people is the tendency to make commitments and say they take these issues seriously, but then frequently not follow. For instance, as mentioned, there was a commitment around superalignment compute, dedicating 20% of compute for long-term safety research.

You and I could have a totally reasonable debate about the appropriate level of compute for superalignment. That’s not really the issue. The issue is that the commitment was made and it was used to recruit people. It was very public.

It was made because there was a recognition that there would always be something more urgent than long-term safety research, like a new product. In the end, they just didn’t keep the commitment. There was always something more urgent than long-term safety research.

Another example is when I raised security issues. They would tell me security is our number one priority. Invariably, when it came time to invest serious resources or make trade-offs to take basic measures, security was not prioritized. The cognitive dissonance and unreliability cause a lot of the drama.

(02:45:11) – Accelerating AI research progress
Dwarkesh Patel
Let’s zoom out and talk about a big part of the story. A big motivation for the way we must proceed with regards to geopolitics is that once you have AGI, you soon proceed to ASI, or superintelligence. You have these AGIs functioning as researchers into further AI progress and within a matter of years, maybe less, you reach superintelligence. From there, according to your story, you do all this research and development into robotics, pocket nukes, and other crazy shit.

I’m skeptical of this story for many reasons. At a high level, it’s not clear to me that this input-output model of research is how things actually happen in research. We can look at the economy as a whole. Patrick Collison and others have pointed out that, compared to 100 years ago, we have 100x more researchers in the world. Yet progress isn’t happening 100 times faster. It's clearly not as simple as pumping in more researchers to get higher research output. I don't see why it would be different for AI researchers.

**Extracted Belief:**

OpenAI's actions are inconsistent with their stated commitments, leading to a lack of trust and increased drama.

**Context:**

Leopold Aschenbrenner critiques OpenAI's commitment to safety and security, noting a recurring pattern of prioritizing other objectives over these commitments.

**Justification:**

He provides examples of broken commitments, such as the superalignment compute allocation and prioritizing security only verbally, not in practice.

--------

## Chunk 442

**Chunk:**

Dwarkesh Patel
Let’s zoom out and talk about a big part of the story. A big motivation for the way we must proceed with regards to geopolitics is that once you have AGI, you soon proceed to ASI, or superintelligence. You have these AGIs functioning as researchers into further AI progress and within a matter of years, maybe less, you reach superintelligence. From there, according to your story, you do all this research and development into robotics, pocket nukes, and other crazy shit.

I’m skeptical of this story for many reasons. At a high level, it’s not clear to me that this input-output model of research is how things actually happen in research. We can look at the economy as a whole. Patrick Collison and others have pointed out that, compared to 100 years ago, we have 100x more researchers in the world. Yet progress isn’t happening 100 times faster. It's clearly not as simple as pumping in more researchers to get higher research output. I don't see why it would be different for AI researchers.
Leopold Aschenbrenner
This is getting into good stuff. This is the classic disagreement I have with Patrick and others. Obviously, inputs matter. The United States produces a lot more scientific and technological progress than Liechtenstein or Switzerland.

Say you made Patrick Collison dictator of Liechtenstein or Switzerland and he implemented his utopia of ideal institutions. Keep the talent pool fixed. He’s not able to do some crazy high-skilled immigration thing or genetic breeding scheme. You keep the talent pool fixed with amazing institutions. Even then, even if Patrick Collison were the dictator, Switzerland still wouldn’t be able to outcompete the United States in scientific and technological progress. Magnitudes matter.
Dwarkesh Patel
I'm not sure I agree with this. There are many examples in history where small groups of people, Bell Labs or Skunk Works, have made significant progress. OpenAI has a couple hundred researchers.

**Extracted Belief:**

The magnitude of resources available significantly influences scientific and technological progress.

**Context:**

Leopold Aschenbrenner argues that even with ideal institutions, a smaller nation like Switzerland would not be able to outcompete the United States in scientific and technological progress due to the United States' larger resource base.

**Justification:**

He uses the example of the United States' greater scientific and technological progress compared to Liechtenstein or Switzerland, suggesting that resource availability is a key factor.

--------

## Chunk 443

**Chunk:**

Dwarkesh Patel
I'm not sure I agree with this. There are many examples in history where small groups of people, Bell Labs or Skunk Works, have made significant progress. OpenAI has a couple hundred researchers.
Leopold Aschenbrenner
Highly selected though.
Dwarkesh Patel
That’s why Patrick Collison as a dictator would do a good job of this.

**Extracted Belief:**

The OpenAI research team consists of highly selected individuals.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's argument that small groups of highly skilled individuals can achieve significant progress, referencing the example of OpenAI.

**Justification:**

Aschenbrenner states "Highly selected though." in response to Patel's observation that OpenAI has a couple hundred researchers. This implies that OpenAI's research team is not simply a large group of individuals, but rather a group of highly selected individuals.

--------

## Chunk 444

**Chunk:**

Dwarkesh Patel
That’s why Patrick Collison as a dictator would do a good job of this.
Leopold Aschenbrenner
Well, yes, if he can highly select all the best AI researchers in the world, he might only need a few hundred. But that’s the talent pool. You have the 300 best AI researchers in the world.
Dwarkesh Patel
But from 100 years ago to now, the population has increased massively. You would expect the density of talent to have increased, considering that things like malnutrition and poverty which affected past talent are no longer as debilitating to the same level.

**Extracted Belief:**

The number of top-tier AI researchers in the world is limited, potentially around 300 individuals.

**Context:**

In response to Dwarkesh Patel suggesting that Patrick Collison, as a dictator, could assemble a large team of AI researchers, Leopold Aschenbrenner argues that the talent pool is limited.

**Justification:**

The belief is based on Aschenbrenner's personal assessment, suggesting that only a small number of individuals possess the skills and expertise to be considered top-tier AI researchers.

--------

## Chunk 445

**Chunk:**

Dwarkesh Patel
But from 100 years ago to now, the population has increased massively. You would expect the density of talent to have increased, considering that things like malnutrition and poverty which affected past talent are no longer as debilitating to the same level.
Leopold Aschenbrenner
I don’t know if it’s 100x. It’s probably at least 10x. Some people think ideas haven’t gotten much harder to find, so why would we need this 10x increase in research effort? To me, this is a very natural story. Why is it natural? It’s a straight line on a log-log plot. It’s a deep learning researcher’s dream.

What is this log-log plot? On the x-axis you have log cumulative research effort. On the y-axis you have log GDP, OOMs of algorithmic progress, log transistors per square inch, log price for a gigawatt of solar energy. It’s extremely natural for that to be a straight line. It’s classic. Initially, things are easy, but you need logarithmic increments of cumulative research effort to find the next big thing. This is a natural story.

One objection people make is, “isn’t it suspicious that we increased research effort 10x and ideas also got 10x harder to find, perfectly equilibrating?” I say it’s just equilibrium—it’s in a endogenous equilibrium. Isn’t it a coincidence that supply equals demand and the market clears? It’s the same here. The difficulty of finding new ideas depends on how much progress has been made.

The overall growth rate is a function of how much ideas have gotten harder to find in ratio to how much research effort has increased. This story is fairly natural, and you see it not just economy-wide but also in the experience curve for various technologies.

It’s plausible that institutions have worsened by some factor. Obviously, there’s some sort of exponent of diminishing returns on adding more people. Serial time is better than just parallelizing. Still, clearly inputs clearly matter.
Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?

**Extracted Belief:**

The density of talent has increased at least 10 times in the last 100 years, despite the massive population increase.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's assertion that the density of talent has increased dramatically over the last century, considering population growth and improvements in factors like malnutrition and poverty.

**Justification:**

Leopold Aschenbrenner provides a specific numerical estimate, stating 'It's probably at least 10x.' This suggests he is drawing from observable data, although he doesn't explicitly reference specific sources.

--------

## Chunk 446

**Chunk:**

Dwarkesh Patel
But from 100 years ago to now, the population has increased massively. You would expect the density of talent to have increased, considering that things like malnutrition and poverty which affected past talent are no longer as debilitating to the same level.
Leopold Aschenbrenner
I don’t know if it’s 100x. It’s probably at least 10x. Some people think ideas haven’t gotten much harder to find, so why would we need this 10x increase in research effort? To me, this is a very natural story. Why is it natural? It’s a straight line on a log-log plot. It’s a deep learning researcher’s dream.

What is this log-log plot? On the x-axis you have log cumulative research effort. On the y-axis you have log GDP, OOMs of algorithmic progress, log transistors per square inch, log price for a gigawatt of solar energy. It’s extremely natural for that to be a straight line. It’s classic. Initially, things are easy, but you need logarithmic increments of cumulative research effort to find the next big thing. This is a natural story.

One objection people make is, “isn’t it suspicious that we increased research effort 10x and ideas also got 10x harder to find, perfectly equilibrating?” I say it’s just equilibrium—it’s in a endogenous equilibrium. Isn’t it a coincidence that supply equals demand and the market clears? It’s the same here. The difficulty of finding new ideas depends on how much progress has been made.

The overall growth rate is a function of how much ideas have gotten harder to find in ratio to how much research effort has increased. This story is fairly natural, and you see it not just economy-wide but also in the experience curve for various technologies.

It’s plausible that institutions have worsened by some factor. Obviously, there’s some sort of exponent of diminishing returns on adding more people. Serial time is better than just parallelizing. Still, clearly inputs clearly matter.
Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?

**Extracted Belief:**

The difficulty of finding new ideas increases in proportion to the amount of progress made.

**Context:**

Leopold Aschenbrenner is explaining his belief that the difficulty of finding new ideas has increased proportionally to the research effort, using the log-log plot as a visual representation.

**Justification:**

The log-log plot is a visual representation of his belief, suggesting that he is drawing from observable data points related to various fields, including GDP, algorithmic progress, and technological advancements. He specifically mentions that the plot is a "deep learning researcher's dream," implying that this pattern is observed in research data.

--------

## Chunk 447

**Chunk:**

Dwarkesh Patel
But from 100 years ago to now, the population has increased massively. You would expect the density of talent to have increased, considering that things like malnutrition and poverty which affected past talent are no longer as debilitating to the same level.
Leopold Aschenbrenner
I don’t know if it’s 100x. It’s probably at least 10x. Some people think ideas haven’t gotten much harder to find, so why would we need this 10x increase in research effort? To me, this is a very natural story. Why is it natural? It’s a straight line on a log-log plot. It’s a deep learning researcher’s dream.

What is this log-log plot? On the x-axis you have log cumulative research effort. On the y-axis you have log GDP, OOMs of algorithmic progress, log transistors per square inch, log price for a gigawatt of solar energy. It’s extremely natural for that to be a straight line. It’s classic. Initially, things are easy, but you need logarithmic increments of cumulative research effort to find the next big thing. This is a natural story.

One objection people make is, “isn’t it suspicious that we increased research effort 10x and ideas also got 10x harder to find, perfectly equilibrating?” I say it’s just equilibrium—it’s in a endogenous equilibrium. Isn’t it a coincidence that supply equals demand and the market clears? It’s the same here. The difficulty of finding new ideas depends on how much progress has been made.

The overall growth rate is a function of how much ideas have gotten harder to find in ratio to how much research effort has increased. This story is fairly natural, and you see it not just economy-wide but also in the experience curve for various technologies.

It’s plausible that institutions have worsened by some factor. Obviously, there’s some sort of exponent of diminishing returns on adding more people. Serial time is better than just parallelizing. Still, clearly inputs clearly matter.
Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?

**Extracted Belief:**

The difficulty of finding new ideas increases logarithmically as research effort increases, resulting in a straight line on a log-log plot.

**Context:**

Leopold Aschenbrenner is explaining his belief about the difficulty of finding new ideas in relation to research effort, using the log-log plot as a visual representation.

**Justification:**

He describes the log-log plot and highlights that "it's extremely natural for that to be a straight line." This suggests he is referring to empirical data plotted on a log-log scale, showing a straight line pattern.

--------

## Chunk 448

**Chunk:**

Dwarkesh Patel
But from 100 years ago to now, the population has increased massively. You would expect the density of talent to have increased, considering that things like malnutrition and poverty which affected past talent are no longer as debilitating to the same level.
Leopold Aschenbrenner
I don’t know if it’s 100x. It’s probably at least 10x. Some people think ideas haven’t gotten much harder to find, so why would we need this 10x increase in research effort? To me, this is a very natural story. Why is it natural? It’s a straight line on a log-log plot. It’s a deep learning researcher’s dream.

What is this log-log plot? On the x-axis you have log cumulative research effort. On the y-axis you have log GDP, OOMs of algorithmic progress, log transistors per square inch, log price for a gigawatt of solar energy. It’s extremely natural for that to be a straight line. It’s classic. Initially, things are easy, but you need logarithmic increments of cumulative research effort to find the next big thing. This is a natural story.

One objection people make is, “isn’t it suspicious that we increased research effort 10x and ideas also got 10x harder to find, perfectly equilibrating?” I say it’s just equilibrium—it’s in a endogenous equilibrium. Isn’t it a coincidence that supply equals demand and the market clears? It’s the same here. The difficulty of finding new ideas depends on how much progress has been made.

The overall growth rate is a function of how much ideas have gotten harder to find in ratio to how much research effort has increased. This story is fairly natural, and you see it not just economy-wide but also in the experience curve for various technologies.

It’s plausible that institutions have worsened by some factor. Obviously, there’s some sort of exponent of diminishing returns on adding more people. Serial time is better than just parallelizing. Still, clearly inputs clearly matter.
Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?

**Extracted Belief:**

The relationship between increased research effort and the difficulty of finding new ideas is an endogenous equilibrium.

**Context:**

Leopold Aschenbrenner is responding to the objection that it's suspicious for increased research effort to correlate with increased difficulty of finding ideas.

**Justification:**

He argues that it's not a coincidence but rather an "endogenous equilibrium." He compares it to the market clearing process where supply and demand balance each other out. This implies a logical relationship between research effort and idea discovery.

--------

## Chunk 449

**Chunk:**

Dwarkesh Patel
But from 100 years ago to now, the population has increased massively. You would expect the density of talent to have increased, considering that things like malnutrition and poverty which affected past talent are no longer as debilitating to the same level.
Leopold Aschenbrenner
I don’t know if it’s 100x. It’s probably at least 10x. Some people think ideas haven’t gotten much harder to find, so why would we need this 10x increase in research effort? To me, this is a very natural story. Why is it natural? It’s a straight line on a log-log plot. It’s a deep learning researcher’s dream.

What is this log-log plot? On the x-axis you have log cumulative research effort. On the y-axis you have log GDP, OOMs of algorithmic progress, log transistors per square inch, log price for a gigawatt of solar energy. It’s extremely natural for that to be a straight line. It’s classic. Initially, things are easy, but you need logarithmic increments of cumulative research effort to find the next big thing. This is a natural story.

One objection people make is, “isn’t it suspicious that we increased research effort 10x and ideas also got 10x harder to find, perfectly equilibrating?” I say it’s just equilibrium—it’s in a endogenous equilibrium. Isn’t it a coincidence that supply equals demand and the market clears? It’s the same here. The difficulty of finding new ideas depends on how much progress has been made.

The overall growth rate is a function of how much ideas have gotten harder to find in ratio to how much research effort has increased. This story is fairly natural, and you see it not just economy-wide but also in the experience curve for various technologies.

It’s plausible that institutions have worsened by some factor. Obviously, there’s some sort of exponent of diminishing returns on adding more people. Serial time is better than just parallelizing. Still, clearly inputs clearly matter.
Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?

**Extracted Belief:**

The overall growth rate is a function of the ratio between the difficulty of finding new ideas and the amount of research effort.

**Context:**

Leopold Aschenbrenner is explaining the relationship between research effort, the difficulty of finding new ideas, and the overall growth rate.

**Justification:**

He states that "The overall growth rate is a function of how much ideas have gotten harder to find in ratio to how much research effort has increased." This statement implies a logical connection between these factors and the growth rate.

--------

## Chunk 450

**Chunk:**

Dwarkesh Patel
But from 100 years ago to now, the population has increased massively. You would expect the density of talent to have increased, considering that things like malnutrition and poverty which affected past talent are no longer as debilitating to the same level.
Leopold Aschenbrenner
I don’t know if it’s 100x. It’s probably at least 10x. Some people think ideas haven’t gotten much harder to find, so why would we need this 10x increase in research effort? To me, this is a very natural story. Why is it natural? It’s a straight line on a log-log plot. It’s a deep learning researcher’s dream.

What is this log-log plot? On the x-axis you have log cumulative research effort. On the y-axis you have log GDP, OOMs of algorithmic progress, log transistors per square inch, log price for a gigawatt of solar energy. It’s extremely natural for that to be a straight line. It’s classic. Initially, things are easy, but you need logarithmic increments of cumulative research effort to find the next big thing. This is a natural story.

One objection people make is, “isn’t it suspicious that we increased research effort 10x and ideas also got 10x harder to find, perfectly equilibrating?” I say it’s just equilibrium—it’s in a endogenous equilibrium. Isn’t it a coincidence that supply equals demand and the market clears? It’s the same here. The difficulty of finding new ideas depends on how much progress has been made.

The overall growth rate is a function of how much ideas have gotten harder to find in ratio to how much research effort has increased. This story is fairly natural, and you see it not just economy-wide but also in the experience curve for various technologies.

It’s plausible that institutions have worsened by some factor. Obviously, there’s some sort of exponent of diminishing returns on adding more people. Serial time is better than just parallelizing. Still, clearly inputs clearly matter.
Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?

**Extracted Belief:**

Institutions have a negative impact on growth, but the effect is limited by diminishing returns.

**Context:**

Leopold Aschenbrenner is discussing the impact of institutions on growth, acknowledging that they can be detrimental.

**Justification:**

He acknowledges that "institutions have worsened by some factor" and mentions that "there's some sort of exponent of diminishing returns on adding more people." This suggests he is referencing empirical observations or data related to institutional impact on growth.

--------

## Chunk 451

**Chunk:**

Dwarkesh Patel
But from 100 years ago to now, the population has increased massively. You would expect the density of talent to have increased, considering that things like malnutrition and poverty which affected past talent are no longer as debilitating to the same level.
Leopold Aschenbrenner
I don’t know if it’s 100x. It’s probably at least 10x. Some people think ideas haven’t gotten much harder to find, so why would we need this 10x increase in research effort? To me, this is a very natural story. Why is it natural? It’s a straight line on a log-log plot. It’s a deep learning researcher’s dream.

What is this log-log plot? On the x-axis you have log cumulative research effort. On the y-axis you have log GDP, OOMs of algorithmic progress, log transistors per square inch, log price for a gigawatt of solar energy. It’s extremely natural for that to be a straight line. It’s classic. Initially, things are easy, but you need logarithmic increments of cumulative research effort to find the next big thing. This is a natural story.

One objection people make is, “isn’t it suspicious that we increased research effort 10x and ideas also got 10x harder to find, perfectly equilibrating?” I say it’s just equilibrium—it’s in a endogenous equilibrium. Isn’t it a coincidence that supply equals demand and the market clears? It’s the same here. The difficulty of finding new ideas depends on how much progress has been made.

The overall growth rate is a function of how much ideas have gotten harder to find in ratio to how much research effort has increased. This story is fairly natural, and you see it not just economy-wide but also in the experience curve for various technologies.

It’s plausible that institutions have worsened by some factor. Obviously, there’s some sort of exponent of diminishing returns on adding more people. Serial time is better than just parallelizing. Still, clearly inputs clearly matter.
Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?

**Extracted Belief:**

Serial time is more effective than parallelizing research effort.

**Context:**

Leopold Aschenbrenner is discussing the effectiveness of different research approaches.

**Justification:**

He states that "Serial time is better than just parallelizing." This statement suggests he has observed evidence or data indicating that sequential research efforts are more fruitful than parallel efforts.

--------

## Chunk 452

**Chunk:**

Dwarkesh Patel
But from 100 years ago to now, the population has increased massively. You would expect the density of talent to have increased, considering that things like malnutrition and poverty which affected past talent are no longer as debilitating to the same level.
Leopold Aschenbrenner
I don’t know if it’s 100x. It’s probably at least 10x. Some people think ideas haven’t gotten much harder to find, so why would we need this 10x increase in research effort? To me, this is a very natural story. Why is it natural? It’s a straight line on a log-log plot. It’s a deep learning researcher’s dream.

What is this log-log plot? On the x-axis you have log cumulative research effort. On the y-axis you have log GDP, OOMs of algorithmic progress, log transistors per square inch, log price for a gigawatt of solar energy. It’s extremely natural for that to be a straight line. It’s classic. Initially, things are easy, but you need logarithmic increments of cumulative research effort to find the next big thing. This is a natural story.

One objection people make is, “isn’t it suspicious that we increased research effort 10x and ideas also got 10x harder to find, perfectly equilibrating?” I say it’s just equilibrium—it’s in a endogenous equilibrium. Isn’t it a coincidence that supply equals demand and the market clears? It’s the same here. The difficulty of finding new ideas depends on how much progress has been made.

The overall growth rate is a function of how much ideas have gotten harder to find in ratio to how much research effort has increased. This story is fairly natural, and you see it not just economy-wide but also in the experience curve for various technologies.

It’s plausible that institutions have worsened by some factor. Obviously, there’s some sort of exponent of diminishing returns on adding more people. Serial time is better than just parallelizing. Still, clearly inputs clearly matter.
Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?

**Extracted Belief:**

Inputs clearly matter for growth, despite the existence of diminishing returns.

**Context:**

Leopold Aschenbrenner is acknowledging the importance of inputs for growth.

**Justification:**

He concludes that "clearly inputs clearly matter" even after acknowledging the existence of diminishing returns. This implies he is drawing on empirical evidence that supports the importance of inputs in driving growth.

--------

## Chunk 453

**Chunk:**

Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?
Leopold Aschenbrenner
Let’s talk about the OpenAI example and the automated AI researchers. Look at the inflation of AI researcher salaries over the last yea. It’s gone up by 4x or 5x. They’re clearly trying to recruit the best AI researchers in the world and they do find them. My response would be that almost all of these 150 IQ people wouldn’t just be good AI researchers if you hired them tomorrow. They wouldn’t be Alec Radford.
Dwarkesh Patel
They’re willing to make investments that take years to pan out. The data centers they’re buying now will come online in 2026. Some of them won’t work out, some won’t have traits we like. But why wouldn’t they make the investment to turn these 150 IQ people into amazing AI researchers by 2026?

**Extracted Belief:**

The inflation of AI researcher salaries in recent years, increasing by 4x or 5x, indicates that organizations like OpenAI are actively seeking to recruit the best AI researchers.

**Context:**

Leopold Aschenbrenner discusses OpenAI's recruitment practices in the context of the question why they don't hire every person with a 150 IQ.

**Justification:**

The justification is provided through the observation of AI researcher salary inflation, which Leopold Aschenbrenner presents as evidence of OpenAI's recruitment efforts.

--------

## Chunk 454

**Chunk:**

Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?
Leopold Aschenbrenner
Let’s talk about the OpenAI example and the automated AI researchers. Look at the inflation of AI researcher salaries over the last yea. It’s gone up by 4x or 5x. They’re clearly trying to recruit the best AI researchers in the world and they do find them. My response would be that almost all of these 150 IQ people wouldn’t just be good AI researchers if you hired them tomorrow. They wouldn’t be Alec Radford.
Dwarkesh Patel
They’re willing to make investments that take years to pan out. The data centers they’re buying now will come online in 2026. Some of them won’t work out, some won’t have traits we like. But why wouldn’t they make the investment to turn these 150 IQ people into amazing AI researchers by 2026?

**Extracted Belief:**

Most individuals with a 150 IQ would not automatically become successful AI researchers even if hired.

**Context:**

Leopold Aschenbrenner responds to the suggestion of hiring all 150 IQ individuals by stating that most wouldn't be successful AI researchers.

**Justification:**

The belief is based on Leopold Aschenbrenner's personal experience and knowledge of the AI research field. He emphasizes that becoming a successful AI researcher requires more than just high IQ.

--------

## Chunk 455

**Chunk:**

Dwarkesh Patel
I agree, but if the coefficient, of how fast they diminish as you grow the input, is high enough, then in the abstract the fact that inputs matter isn’t that relevant.

We’re talking at a very high level, but let’s take it down to the concrete. OpenAI has a staff of at most a few hundred directly involved in algorithmic progress for future models. Let’s say you could really arbitrarily scale this number for faster algorithmic progress and better AI. It’s not clear why OpenAI doesn’t just go hire every person with a 150 IQ, of which there are hundreds of thousands in the world.

My story is that there are transaction costs to managing all these people. They don’t just go away if you have a bunch of AIs. These tasks aren’t easy to parallelize. I’m not sure how you would explain the fact that OpenAI doesn’t go on a recruiting binge of every genius in the world?
Leopold Aschenbrenner
Let’s talk about the OpenAI example and the automated AI researchers. Look at the inflation of AI researcher salaries over the last yea. It’s gone up by 4x or 5x. They’re clearly trying to recruit the best AI researchers in the world and they do find them. My response would be that almost all of these 150 IQ people wouldn’t just be good AI researchers if you hired them tomorrow. They wouldn’t be Alec Radford.
Dwarkesh Patel
They’re willing to make investments that take years to pan out. The data centers they’re buying now will come online in 2026. Some of them won’t work out, some won’t have traits we like. But why wouldn’t they make the investment to turn these 150 IQ people into amazing AI researchers by 2026?

**Extracted Belief:**

A successful AI researcher like Alec Radford possesses a unique combination of skills and experiences that cannot be replicated by simply hiring individuals with high IQ.

**Context:**

Leopold Aschenbrenner uses Alec Radford as an example to illustrate that high IQ alone is insufficient for success in AI research.

**Justification:**

The belief is based on Leopold Aschenbrenner's personal perspective and experience, comparing the qualities of a top AI researcher to those with high IQ.

--------

## Chunk 456

**Chunk:**

Dwarkesh Patel
They’re willing to make investments that take years to pan out. The data centers they’re buying now will come online in 2026. Some of them won’t work out, some won’t have traits we like. But why wouldn’t they make the investment to turn these 150 IQ people into amazing AI researchers by 2026?
Leopold Aschenbrenner
Sometimes this does happen. Smart physicists have been really good at AI research, like all the Anthropic co-founders.
Dwarkesh Patel
But for example, Dario said on the podcast that they have a careful policy of being extremely selective and not hiring arbitrarily.

**Extracted Belief:**

Smart physicists can be successful AI researchers.

**Context:**

Leopold Aschenbrenner used the example of Anthropic co-founders to support his claim that smart physicists can transition into AI research.

**Justification:**

Leopold Aschenbrenner cites the example of Anthropic co-founders, who are smart physicists and have been successful in AI research.

--------

## Chunk 457

**Chunk:**

Dwarkesh Patel
But for example, Dario said on the podcast that they have a careful policy of being extremely selective and not hiring arbitrarily.
Leopold Aschenbrenner
Training is not as easily scalable. Training is really hard. If you hired 100,000 people, it would be really hard to train them all. You wouldn’t be doing any AI research. There are huge costs to bringing on new people and training them.

This is very different with AIs. It’s important to talk about the advantages AIs will have. What does it take to be an Alec Radford? You need to be a really good engineer. AIs will be amazing engineers and coders. You can train them to do that. They also need to have good research intuitions and a really good understanding of deep learning.

Alec Radford, or people like him, has acquired this over years of being deeply immersed in deep learning and having tried lots of things himself and failed. AIs will be able to read every research paper ever written, learn from every experiment ever run at the lab, and gain intuition from all of this. They’ll be able to learn in parallel from each other's experiments and experiences.

There’s also a cultural acclimation aspect. If you hire someone new, there’s politicking, and maybe they don’t fit in well. With AIs, you just make replicas. There's a motivation aspect as well. If I could duplicate Alec Radford, and before I run every experiment, have him spend a decade’s worth of human time double-checking code and thinking really carefully about it, he wouldn’t care and he wouldn't be motivated. With AIs, you can have 100 million of them focused on making sure the code is correct with no bugs.

The idea of 100 million human-equivalent AI researchers is just a way to visualize it. You might not have literally 100 million copies. There’s tradeoffs you can make between serial speed and parallel. You might run them at 10x or 100x serial speed, resulting in fewer tokens overall because of inherent trade-offs. You might have 100,000 AIs running at 100x human speed. They can coordinate by sharing latent space and attending to each other's context. There’s a huge range of possibilities for what you can do.

Another illustration is that by 2027 or 2028, with automated AI researchers, you'll be able to generate an entire Internet’s worth of tokens every day. It’s clearly a huge amount of intellectual work that you can do.
Dwarkesh Patel
Here’s an analogy. Today, we generate more patents in a year than during the actual physics revolution in the early 20th century. Are we making more physics progress in a year today than we did in half a century back then? Generating all these tokens doesn't necessarily equate to generating as much codified knowledge in the initial creation of the Internet.

**Extracted Belief:**

Training large numbers of human AI researchers is difficult and costly due to the challenges of onboarding, training, and acclimating new individuals to research environments.

**Context:**

Leopold Aschenbrenner is refuting the idea that hiring vast numbers of people with high IQs would be a simple solution to accelerate AI research, arguing that training and onboarding pose significant challenges.

**Justification:**

Aschenbrenner states that training 100,000 people would be difficult and would not result in productive AI research. He also mentions the significant cost associated with bringing in and training new employees.

--------

## Chunk 458

**Chunk:**

Dwarkesh Patel
But for example, Dario said on the podcast that they have a careful policy of being extremely selective and not hiring arbitrarily.
Leopold Aschenbrenner
Training is not as easily scalable. Training is really hard. If you hired 100,000 people, it would be really hard to train them all. You wouldn’t be doing any AI research. There are huge costs to bringing on new people and training them.

This is very different with AIs. It’s important to talk about the advantages AIs will have. What does it take to be an Alec Radford? You need to be a really good engineer. AIs will be amazing engineers and coders. You can train them to do that. They also need to have good research intuitions and a really good understanding of deep learning.

Alec Radford, or people like him, has acquired this over years of being deeply immersed in deep learning and having tried lots of things himself and failed. AIs will be able to read every research paper ever written, learn from every experiment ever run at the lab, and gain intuition from all of this. They’ll be able to learn in parallel from each other's experiments and experiences.

There’s also a cultural acclimation aspect. If you hire someone new, there’s politicking, and maybe they don’t fit in well. With AIs, you just make replicas. There's a motivation aspect as well. If I could duplicate Alec Radford, and before I run every experiment, have him spend a decade’s worth of human time double-checking code and thinking really carefully about it, he wouldn’t care and he wouldn't be motivated. With AIs, you can have 100 million of them focused on making sure the code is correct with no bugs.

The idea of 100 million human-equivalent AI researchers is just a way to visualize it. You might not have literally 100 million copies. There’s tradeoffs you can make between serial speed and parallel. You might run them at 10x or 100x serial speed, resulting in fewer tokens overall because of inherent trade-offs. You might have 100,000 AIs running at 100x human speed. They can coordinate by sharing latent space and attending to each other's context. There’s a huge range of possibilities for what you can do.

Another illustration is that by 2027 or 2028, with automated AI researchers, you'll be able to generate an entire Internet’s worth of tokens every day. It’s clearly a huge amount of intellectual work that you can do.
Dwarkesh Patel
Here’s an analogy. Today, we generate more patents in a year than during the actual physics revolution in the early 20th century. Are we making more physics progress in a year today than we did in half a century back then? Generating all these tokens doesn't necessarily equate to generating as much codified knowledge in the initial creation of the Internet.

**Extracted Belief:**

AI researchers require both strong engineering skills and a deep understanding of deep learning and research intuition, which are acquired through years of immersion and experience.

**Context:**

Aschenbrenner is contrasting the capabilities of humans and AI in the context of AI research, highlighting the key skills and knowledge necessary to be a successful AI researcher.

**Justification:**

He cites Alec Radford as an example of an exceptional AI researcher and points out that such individuals possess both strong engineering skills and a deep understanding of deep learning, acquired through years of dedicated work and experimentation.

--------

## Chunk 459

**Chunk:**

Dwarkesh Patel
But for example, Dario said on the podcast that they have a careful policy of being extremely selective and not hiring arbitrarily.
Leopold Aschenbrenner
Training is not as easily scalable. Training is really hard. If you hired 100,000 people, it would be really hard to train them all. You wouldn’t be doing any AI research. There are huge costs to bringing on new people and training them.

This is very different with AIs. It’s important to talk about the advantages AIs will have. What does it take to be an Alec Radford? You need to be a really good engineer. AIs will be amazing engineers and coders. You can train them to do that. They also need to have good research intuitions and a really good understanding of deep learning.

Alec Radford, or people like him, has acquired this over years of being deeply immersed in deep learning and having tried lots of things himself and failed. AIs will be able to read every research paper ever written, learn from every experiment ever run at the lab, and gain intuition from all of this. They’ll be able to learn in parallel from each other's experiments and experiences.

There’s also a cultural acclimation aspect. If you hire someone new, there’s politicking, and maybe they don’t fit in well. With AIs, you just make replicas. There's a motivation aspect as well. If I could duplicate Alec Radford, and before I run every experiment, have him spend a decade’s worth of human time double-checking code and thinking really carefully about it, he wouldn’t care and he wouldn't be motivated. With AIs, you can have 100 million of them focused on making sure the code is correct with no bugs.

The idea of 100 million human-equivalent AI researchers is just a way to visualize it. You might not have literally 100 million copies. There’s tradeoffs you can make between serial speed and parallel. You might run them at 10x or 100x serial speed, resulting in fewer tokens overall because of inherent trade-offs. You might have 100,000 AIs running at 100x human speed. They can coordinate by sharing latent space and attending to each other's context. There’s a huge range of possibilities for what you can do.

Another illustration is that by 2027 or 2028, with automated AI researchers, you'll be able to generate an entire Internet’s worth of tokens every day. It’s clearly a huge amount of intellectual work that you can do.
Dwarkesh Patel
Here’s an analogy. Today, we generate more patents in a year than during the actual physics revolution in the early 20th century. Are we making more physics progress in a year today than we did in half a century back then? Generating all these tokens doesn't necessarily equate to generating as much codified knowledge in the initial creation of the Internet.

**Extracted Belief:**

AI systems will surpass human capabilities in engineering, coding, and research intuition due to their ability to process vast amounts of data, learn from past experiences, and operate in parallel.

**Context:**

Aschenbrenner is outlining his vision for the future of AI research, arguing that AI will eventually surpass human capabilities in key areas.

**Justification:**

He explains that AIs will be able to access and learn from all research papers and experiments, gain intuition from this data, and even learn from each other's work in parallel, leading to accelerated progress and ultimately surpassing human capabilities.

--------

## Chunk 460

**Chunk:**

Dwarkesh Patel
But for example, Dario said on the podcast that they have a careful policy of being extremely selective and not hiring arbitrarily.
Leopold Aschenbrenner
Training is not as easily scalable. Training is really hard. If you hired 100,000 people, it would be really hard to train them all. You wouldn’t be doing any AI research. There are huge costs to bringing on new people and training them.

This is very different with AIs. It’s important to talk about the advantages AIs will have. What does it take to be an Alec Radford? You need to be a really good engineer. AIs will be amazing engineers and coders. You can train them to do that. They also need to have good research intuitions and a really good understanding of deep learning.

Alec Radford, or people like him, has acquired this over years of being deeply immersed in deep learning and having tried lots of things himself and failed. AIs will be able to read every research paper ever written, learn from every experiment ever run at the lab, and gain intuition from all of this. They’ll be able to learn in parallel from each other's experiments and experiences.

There’s also a cultural acclimation aspect. If you hire someone new, there’s politicking, and maybe they don’t fit in well. With AIs, you just make replicas. There's a motivation aspect as well. If I could duplicate Alec Radford, and before I run every experiment, have him spend a decade’s worth of human time double-checking code and thinking really carefully about it, he wouldn’t care and he wouldn't be motivated. With AIs, you can have 100 million of them focused on making sure the code is correct with no bugs.

The idea of 100 million human-equivalent AI researchers is just a way to visualize it. You might not have literally 100 million copies. There’s tradeoffs you can make between serial speed and parallel. You might run them at 10x or 100x serial speed, resulting in fewer tokens overall because of inherent trade-offs. You might have 100,000 AIs running at 100x human speed. They can coordinate by sharing latent space and attending to each other's context. There’s a huge range of possibilities for what you can do.

Another illustration is that by 2027 or 2028, with automated AI researchers, you'll be able to generate an entire Internet’s worth of tokens every day. It’s clearly a huge amount of intellectual work that you can do.
Dwarkesh Patel
Here’s an analogy. Today, we generate more patents in a year than during the actual physics revolution in the early 20th century. Are we making more physics progress in a year today than we did in half a century back then? Generating all these tokens doesn't necessarily equate to generating as much codified knowledge in the initial creation of the Internet.

**Extracted Belief:**

AI systems will be able to learn from past experiments and experiences in parallel, leading to accelerated progress and eventually surpassing human capabilities.

**Context:**

Aschenbrenner is arguing that AI systems will be able to learn and improve at a much faster rate than humans due to their ability to process information and experiment in parallel.

**Justification:**

He states that AIs will be able to learn in parallel from each other's experiments and experiences, implying that they can collectively learn and evolve faster than individual humans.

--------

## Chunk 461

**Chunk:**

Dwarkesh Patel
But for example, Dario said on the podcast that they have a careful policy of being extremely selective and not hiring arbitrarily.
Leopold Aschenbrenner
Training is not as easily scalable. Training is really hard. If you hired 100,000 people, it would be really hard to train them all. You wouldn’t be doing any AI research. There are huge costs to bringing on new people and training them.

This is very different with AIs. It’s important to talk about the advantages AIs will have. What does it take to be an Alec Radford? You need to be a really good engineer. AIs will be amazing engineers and coders. You can train them to do that. They also need to have good research intuitions and a really good understanding of deep learning.

Alec Radford, or people like him, has acquired this over years of being deeply immersed in deep learning and having tried lots of things himself and failed. AIs will be able to read every research paper ever written, learn from every experiment ever run at the lab, and gain intuition from all of this. They’ll be able to learn in parallel from each other's experiments and experiences.

There’s also a cultural acclimation aspect. If you hire someone new, there’s politicking, and maybe they don’t fit in well. With AIs, you just make replicas. There's a motivation aspect as well. If I could duplicate Alec Radford, and before I run every experiment, have him spend a decade’s worth of human time double-checking code and thinking really carefully about it, he wouldn’t care and he wouldn't be motivated. With AIs, you can have 100 million of them focused on making sure the code is correct with no bugs.

The idea of 100 million human-equivalent AI researchers is just a way to visualize it. You might not have literally 100 million copies. There’s tradeoffs you can make between serial speed and parallel. You might run them at 10x or 100x serial speed, resulting in fewer tokens overall because of inherent trade-offs. You might have 100,000 AIs running at 100x human speed. They can coordinate by sharing latent space and attending to each other's context. There’s a huge range of possibilities for what you can do.

Another illustration is that by 2027 or 2028, with automated AI researchers, you'll be able to generate an entire Internet’s worth of tokens every day. It’s clearly a huge amount of intellectual work that you can do.
Dwarkesh Patel
Here’s an analogy. Today, we generate more patents in a year than during the actual physics revolution in the early 20th century. Are we making more physics progress in a year today than we did in half a century back then? Generating all these tokens doesn't necessarily equate to generating as much codified knowledge in the initial creation of the Internet.

**Extracted Belief:**

AI systems can be trained to be highly effective engineers and coders, capable of eliminating bugs and ensuring code accuracy.

**Context:**

Aschenbrenner is arguing that AI systems will be capable of surpassing human capabilities in coding and engineering, leading to more efficient and accurate research processes.

**Justification:**

He states that AIs can be trained to be amazing engineers and coders, implying that they will be able to perform these tasks with high accuracy and efficiency, ultimately surpassing human capabilities.

--------

## Chunk 462

**Chunk:**

Dwarkesh Patel
But for example, Dario said on the podcast that they have a careful policy of being extremely selective and not hiring arbitrarily.
Leopold Aschenbrenner
Training is not as easily scalable. Training is really hard. If you hired 100,000 people, it would be really hard to train them all. You wouldn’t be doing any AI research. There are huge costs to bringing on new people and training them.

This is very different with AIs. It’s important to talk about the advantages AIs will have. What does it take to be an Alec Radford? You need to be a really good engineer. AIs will be amazing engineers and coders. You can train them to do that. They also need to have good research intuitions and a really good understanding of deep learning.

Alec Radford, or people like him, has acquired this over years of being deeply immersed in deep learning and having tried lots of things himself and failed. AIs will be able to read every research paper ever written, learn from every experiment ever run at the lab, and gain intuition from all of this. They’ll be able to learn in parallel from each other's experiments and experiences.

There’s also a cultural acclimation aspect. If you hire someone new, there’s politicking, and maybe they don’t fit in well. With AIs, you just make replicas. There's a motivation aspect as well. If I could duplicate Alec Radford, and before I run every experiment, have him spend a decade’s worth of human time double-checking code and thinking really carefully about it, he wouldn’t care and he wouldn't be motivated. With AIs, you can have 100 million of them focused on making sure the code is correct with no bugs.

The idea of 100 million human-equivalent AI researchers is just a way to visualize it. You might not have literally 100 million copies. There’s tradeoffs you can make between serial speed and parallel. You might run them at 10x or 100x serial speed, resulting in fewer tokens overall because of inherent trade-offs. You might have 100,000 AIs running at 100x human speed. They can coordinate by sharing latent space and attending to each other's context. There’s a huge range of possibilities for what you can do.

Another illustration is that by 2027 or 2028, with automated AI researchers, you'll be able to generate an entire Internet’s worth of tokens every day. It’s clearly a huge amount of intellectual work that you can do.
Dwarkesh Patel
Here’s an analogy. Today, we generate more patents in a year than during the actual physics revolution in the early 20th century. Are we making more physics progress in a year today than we did in half a century back then? Generating all these tokens doesn't necessarily equate to generating as much codified knowledge in the initial creation of the Internet.

**Extracted Belief:**

AI systems will not be susceptible to human limitations like motivation and cultural acclimation issues, allowing them to work more efficiently and productively.

**Context:**

Aschenbrenner is arguing that AI systems will be more efficient than human researchers due to their lack of human limitations, such as motivation and cultural acclimation issues.

**Justification:**

He contrasts the potential of AI systems to be replicated and work tirelessly with the challenges of managing human researchers, highlighting the lack of motivation and cultural compatibility issues with AI.

--------

## Chunk 463

**Chunk:**

Dwarkesh Patel
But for example, Dario said on the podcast that they have a careful policy of being extremely selective and not hiring arbitrarily.
Leopold Aschenbrenner
Training is not as easily scalable. Training is really hard. If you hired 100,000 people, it would be really hard to train them all. You wouldn’t be doing any AI research. There are huge costs to bringing on new people and training them.

This is very different with AIs. It’s important to talk about the advantages AIs will have. What does it take to be an Alec Radford? You need to be a really good engineer. AIs will be amazing engineers and coders. You can train them to do that. They also need to have good research intuitions and a really good understanding of deep learning.

Alec Radford, or people like him, has acquired this over years of being deeply immersed in deep learning and having tried lots of things himself and failed. AIs will be able to read every research paper ever written, learn from every experiment ever run at the lab, and gain intuition from all of this. They’ll be able to learn in parallel from each other's experiments and experiences.

There’s also a cultural acclimation aspect. If you hire someone new, there’s politicking, and maybe they don’t fit in well. With AIs, you just make replicas. There's a motivation aspect as well. If I could duplicate Alec Radford, and before I run every experiment, have him spend a decade’s worth of human time double-checking code and thinking really carefully about it, he wouldn’t care and he wouldn't be motivated. With AIs, you can have 100 million of them focused on making sure the code is correct with no bugs.

The idea of 100 million human-equivalent AI researchers is just a way to visualize it. You might not have literally 100 million copies. There’s tradeoffs you can make between serial speed and parallel. You might run them at 10x or 100x serial speed, resulting in fewer tokens overall because of inherent trade-offs. You might have 100,000 AIs running at 100x human speed. They can coordinate by sharing latent space and attending to each other's context. There’s a huge range of possibilities for what you can do.

Another illustration is that by 2027 or 2028, with automated AI researchers, you'll be able to generate an entire Internet’s worth of tokens every day. It’s clearly a huge amount of intellectual work that you can do.
Dwarkesh Patel
Here’s an analogy. Today, we generate more patents in a year than during the actual physics revolution in the early 20th century. Are we making more physics progress in a year today than we did in half a century back then? Generating all these tokens doesn't necessarily equate to generating as much codified knowledge in the initial creation of the Internet.

**Extracted Belief:**

AI systems will be able to generate an entire Internet's worth of tokens daily by 2027 or 2028, representing a massive increase in intellectual output.

**Context:**

Aschenbrenner is predicting the future potential of AI systems to generate a vast amount of information and intellectual output.

**Justification:**

He states that with automated AI researchers, by 2027 or 2028, it will be possible to generate an entire Internet's worth of tokens every day, suggesting a significant increase in intellectual output.

--------

## Chunk 464

**Chunk:**

Dwarkesh Patel
Here’s an analogy. Today, we generate more patents in a year than during the actual physics revolution in the early 20th century. Are we making more physics progress in a year today than we did in half a century back then? Generating all these tokens doesn't necessarily equate to generating as much codified knowledge in the initial creation of the Internet.
Leopold Aschenbrenner
Internet tokens are usually final output. We talked about the unhobbling. I think of a GPN token as one token of my internal monologue. That’s how I do this math on human equivalents. It's like 100 tokens a minute and then humans working for X hours. What is the equivalent there?
Dwarkesh Patel
This goes back to something from earlier. Why haven’t we seen huge revenues from AI yet? People often ask this question. If you took GPT-4 back ten years, people would think it would automate half the jobs. There’s a modus ponens, modus tollens here. Part of the explanation is that we’re on the verge and we just need to do these unhobblings. Part of that is probably true. But there is another lesson to learn there. Just looking at a set of abilities at face value, there are likely more hobblings behind the scenes. The same will be true of AGIs running as AI researchers.

**Extracted Belief:**

A GPN token is equivalent to one token of an individual's internal monologue.

**Context:**

Leopold Aschenbrenner is explaining how he calculates the human equivalent of AI researchers, using the analogy of a GPN token being equivalent to a thought in a person's mind.

**Justification:**

He states that 'I think of a GPN token as one token of my internal monologue.'

--------

## Chunk 465

**Chunk:**

Dwarkesh Patel
Here’s an analogy. Today, we generate more patents in a year than during the actual physics revolution in the early 20th century. Are we making more physics progress in a year today than we did in half a century back then? Generating all these tokens doesn't necessarily equate to generating as much codified knowledge in the initial creation of the Internet.
Leopold Aschenbrenner
Internet tokens are usually final output. We talked about the unhobbling. I think of a GPN token as one token of my internal monologue. That’s how I do this math on human equivalents. It's like 100 tokens a minute and then humans working for X hours. What is the equivalent there?
Dwarkesh Patel
This goes back to something from earlier. Why haven’t we seen huge revenues from AI yet? People often ask this question. If you took GPT-4 back ten years, people would think it would automate half the jobs. There’s a modus ponens, modus tollens here. Part of the explanation is that we’re on the verge and we just need to do these unhobblings. Part of that is probably true. But there is another lesson to learn there. Just looking at a set of abilities at face value, there are likely more hobblings behind the scenes. The same will be true of AGIs running as AI researchers.

**Extracted Belief:**

AI research does not encounter real-world bottlenecks, such as the need for physical resources or labor.

**Context:**

Leopold Aschenbrenner is explaining why he believes AI research will initially experience an explosion of progress, before facing real-world bottlenecks.

**Justification:**

He states that 'AI research doesn’t run into these real-world bottlenecks. It doesn’t require plowing a field or digging up coal. It’s just doing AI research.'

--------

## Chunk 466

**Chunk:**

Dwarkesh Patel
This goes back to something from earlier. Why haven’t we seen huge revenues from AI yet? People often ask this question. If you took GPT-4 back ten years, people would think it would automate half the jobs. There’s a modus ponens, modus tollens here. Part of the explanation is that we’re on the verge and we just need to do these unhobblings. Part of that is probably true. But there is another lesson to learn there. Just looking at a set of abilities at face value, there are likely more hobblings behind the scenes. The same will be true of AGIs running as AI researchers.
Leopold Aschenbrenner
I basically agree with a lot of what you said. My story here is that there’s going to be a long tail. Maybe by 2026 or 202, you’ll have the proto-automated engineer that’s really good at engineering. It doesn’t yet have the research intuition. You don’t quite know how to put them to work.

Even so, the underlying pace of AI progress is already so fast. In just three years, we've gone from AI not being able to do any kind of math at all to now crushing these math competitions. So, you might have the initial automated research engineer by 2026 or 2027, which speeds you up by 2x. You go through a lot more progress in that year. By the end of the year, you’ve figured out the remaining unhobblings and you've got a smarter model.

Maybe it’s two years but then maybe that model can automate 100% of the research. They don’t need to be doing everything. They don’t need to make coffee or deal with tacit knowledge in other fields. AI researchers at AI labs really know the job of an AI researcher. There are lots of clear metrics. It's all virtual. There’s code. There are things you can develop and train for.
Dwarkesh Patel
Another thing is how do you actually manage a million AI researchers? Humans’ comparative ability, and we’ve been especially trained for it, is to work in teams. We’ve been learning for thousands of years about how we work together in groups. Despite this, management is a clusterfuck. Most companies are poorly managed. It's really hard to do this stuff.

For AIs, we talk about AGI, but for it will be some bespoke set of abilities some of which will be higher than human level and some at human level. It will be some bundle and you’ll need to figure out how to put these bundles together with their human overseers and equipment. I’m just very skeptical of the idea that as soon as you get the bundle, you can just shove millions of them together and manage them.

Any other technological revolution in history has been much more piecemeal than you'd expect on paper. What is the industrial revolution? We dug up coal to power steam engines, used steam engines to run railroads, which helped us get more coal. There’s sort of a Factorio story you can tell where in six hours you can be pumping out thousands of times more coal. In real life, it takes centuries.

For example, with electrification, there's a famous study showing how it took decades after electricity to switch from the pulley and water wheel-based system for steam engines to one that works with more spread-out electrical motors. This will be the same kind of thing. It might take decades to actually get millions of AI researchers to work together effectively.

**Extracted Belief:**

AI progress is happening at a rapid pace, and within three years, AI has gone from not being able to perform math to being able to solve complex math problems.

**Context:**

Leopold Aschenbrenner is expressing his belief about the speed of AI progress, citing the recent advancement of AI in solving math problems as an example.

**Justification:**

He uses the example of AI's ability to solve complex math problems within a short period as evidence for the rapid pace of AI progress.

--------

## Chunk 467

**Chunk:**

Dwarkesh Patel
This goes back to something from earlier. Why haven’t we seen huge revenues from AI yet? People often ask this question. If you took GPT-4 back ten years, people would think it would automate half the jobs. There’s a modus ponens, modus tollens here. Part of the explanation is that we’re on the verge and we just need to do these unhobblings. Part of that is probably true. But there is another lesson to learn there. Just looking at a set of abilities at face value, there are likely more hobblings behind the scenes. The same will be true of AGIs running as AI researchers.
Leopold Aschenbrenner
I basically agree with a lot of what you said. My story here is that there’s going to be a long tail. Maybe by 2026 or 202, you’ll have the proto-automated engineer that’s really good at engineering. It doesn’t yet have the research intuition. You don’t quite know how to put them to work.

Even so, the underlying pace of AI progress is already so fast. In just three years, we've gone from AI not being able to do any kind of math at all to now crushing these math competitions. So, you might have the initial automated research engineer by 2026 or 2027, which speeds you up by 2x. You go through a lot more progress in that year. By the end of the year, you’ve figured out the remaining unhobblings and you've got a smarter model.

Maybe it’s two years but then maybe that model can automate 100% of the research. They don’t need to be doing everything. They don’t need to make coffee or deal with tacit knowledge in other fields. AI researchers at AI labs really know the job of an AI researcher. There are lots of clear metrics. It's all virtual. There’s code. There are things you can develop and train for.
Dwarkesh Patel
Another thing is how do you actually manage a million AI researchers? Humans’ comparative ability, and we’ve been especially trained for it, is to work in teams. We’ve been learning for thousands of years about how we work together in groups. Despite this, management is a clusterfuck. Most companies are poorly managed. It's really hard to do this stuff.

For AIs, we talk about AGI, but for it will be some bespoke set of abilities some of which will be higher than human level and some at human level. It will be some bundle and you’ll need to figure out how to put these bundles together with their human overseers and equipment. I’m just very skeptical of the idea that as soon as you get the bundle, you can just shove millions of them together and manage them.

Any other technological revolution in history has been much more piecemeal than you'd expect on paper. What is the industrial revolution? We dug up coal to power steam engines, used steam engines to run railroads, which helped us get more coal. There’s sort of a Factorio story you can tell where in six hours you can be pumping out thousands of times more coal. In real life, it takes centuries.

For example, with electrification, there's a famous study showing how it took decades after electricity to switch from the pulley and water wheel-based system for steam engines to one that works with more spread-out electrical motors. This will be the same kind of thing. It might take decades to actually get millions of AI researchers to work together effectively.

**Extracted Belief:**

By 2026 or 2027, there will be an automated research engineer that is skilled at engineering but lacks research intuition.

**Context:**

Leopold Aschenbrenner is predicting the development of an AI engineer with specific capabilities and limitations.

**Justification:**

He suggests that this type of AI engineer will be able to perform engineering tasks but may lack the intuition needed for independent research.

--------

## Chunk 468

**Chunk:**

Dwarkesh Patel
This goes back to something from earlier. Why haven’t we seen huge revenues from AI yet? People often ask this question. If you took GPT-4 back ten years, people would think it would automate half the jobs. There’s a modus ponens, modus tollens here. Part of the explanation is that we’re on the verge and we just need to do these unhobblings. Part of that is probably true. But there is another lesson to learn there. Just looking at a set of abilities at face value, there are likely more hobblings behind the scenes. The same will be true of AGIs running as AI researchers.
Leopold Aschenbrenner
I basically agree with a lot of what you said. My story here is that there’s going to be a long tail. Maybe by 2026 or 202, you’ll have the proto-automated engineer that’s really good at engineering. It doesn’t yet have the research intuition. You don’t quite know how to put them to work.

Even so, the underlying pace of AI progress is already so fast. In just three years, we've gone from AI not being able to do any kind of math at all to now crushing these math competitions. So, you might have the initial automated research engineer by 2026 or 2027, which speeds you up by 2x. You go through a lot more progress in that year. By the end of the year, you’ve figured out the remaining unhobblings and you've got a smarter model.

Maybe it’s two years but then maybe that model can automate 100% of the research. They don’t need to be doing everything. They don’t need to make coffee or deal with tacit knowledge in other fields. AI researchers at AI labs really know the job of an AI researcher. There are lots of clear metrics. It's all virtual. There’s code. There are things you can develop and train for.
Dwarkesh Patel
Another thing is how do you actually manage a million AI researchers? Humans’ comparative ability, and we’ve been especially trained for it, is to work in teams. We’ve been learning for thousands of years about how we work together in groups. Despite this, management is a clusterfuck. Most companies are poorly managed. It's really hard to do this stuff.

For AIs, we talk about AGI, but for it will be some bespoke set of abilities some of which will be higher than human level and some at human level. It will be some bundle and you’ll need to figure out how to put these bundles together with their human overseers and equipment. I’m just very skeptical of the idea that as soon as you get the bundle, you can just shove millions of them together and manage them.

Any other technological revolution in history has been much more piecemeal than you'd expect on paper. What is the industrial revolution? We dug up coal to power steam engines, used steam engines to run railroads, which helped us get more coal. There’s sort of a Factorio story you can tell where in six hours you can be pumping out thousands of times more coal. In real life, it takes centuries.

For example, with electrification, there's a famous study showing how it took decades after electricity to switch from the pulley and water wheel-based system for steam engines to one that works with more spread-out electrical motors. This will be the same kind of thing. It might take decades to actually get millions of AI researchers to work together effectively.

**Extracted Belief:**

Within a year, the automated research engineer will learn to overcome remaining limitations and become a smarter model capable of automating 100% of the research.

**Context:**

Leopold Aschenbrenner is predicting the rapid improvement of the automated research engineer.

**Justification:**

He suggests that the automated research engineer will rapidly evolve to become smarter and more capable, eventually able to automate all research tasks.

--------

## Chunk 469

**Chunk:**

Dwarkesh Patel
This goes back to something from earlier. Why haven’t we seen huge revenues from AI yet? People often ask this question. If you took GPT-4 back ten years, people would think it would automate half the jobs. There’s a modus ponens, modus tollens here. Part of the explanation is that we’re on the verge and we just need to do these unhobblings. Part of that is probably true. But there is another lesson to learn there. Just looking at a set of abilities at face value, there are likely more hobblings behind the scenes. The same will be true of AGIs running as AI researchers.
Leopold Aschenbrenner
I basically agree with a lot of what you said. My story here is that there’s going to be a long tail. Maybe by 2026 or 202, you’ll have the proto-automated engineer that’s really good at engineering. It doesn’t yet have the research intuition. You don’t quite know how to put them to work.

Even so, the underlying pace of AI progress is already so fast. In just three years, we've gone from AI not being able to do any kind of math at all to now crushing these math competitions. So, you might have the initial automated research engineer by 2026 or 2027, which speeds you up by 2x. You go through a lot more progress in that year. By the end of the year, you’ve figured out the remaining unhobblings and you've got a smarter model.

Maybe it’s two years but then maybe that model can automate 100% of the research. They don’t need to be doing everything. They don’t need to make coffee or deal with tacit knowledge in other fields. AI researchers at AI labs really know the job of an AI researcher. There are lots of clear metrics. It's all virtual. There’s code. There are things you can develop and train for.
Dwarkesh Patel
Another thing is how do you actually manage a million AI researchers? Humans’ comparative ability, and we’ve been especially trained for it, is to work in teams. We’ve been learning for thousands of years about how we work together in groups. Despite this, management is a clusterfuck. Most companies are poorly managed. It's really hard to do this stuff.

For AIs, we talk about AGI, but for it will be some bespoke set of abilities some of which will be higher than human level and some at human level. It will be some bundle and you’ll need to figure out how to put these bundles together with their human overseers and equipment. I’m just very skeptical of the idea that as soon as you get the bundle, you can just shove millions of them together and manage them.

Any other technological revolution in history has been much more piecemeal than you'd expect on paper. What is the industrial revolution? We dug up coal to power steam engines, used steam engines to run railroads, which helped us get more coal. There’s sort of a Factorio story you can tell where in six hours you can be pumping out thousands of times more coal. In real life, it takes centuries.

For example, with electrification, there's a famous study showing how it took decades after electricity to switch from the pulley and water wheel-based system for steam engines to one that works with more spread-out electrical motors. This will be the same kind of thing. It might take decades to actually get millions of AI researchers to work together effectively.

**Extracted Belief:**

AI researchers at AI labs have a clear and well-defined job description with measurable metrics, making it easier to automate their tasks.

**Context:**

Leopold Aschenbrenner is explaining why AI researchers are a suitable target for automation.

**Justification:**

He emphasizes the clarity and measurability of the AI researcher role, suggesting that it is easier to develop and train AI systems for this particular job.

--------

## Chunk 470

**Chunk:**

Dwarkesh Patel
Another thing is how do you actually manage a million AI researchers? Humans’ comparative ability, and we’ve been especially trained for it, is to work in teams. We’ve been learning for thousands of years about how we work together in groups. Despite this, management is a clusterfuck. Most companies are poorly managed. It's really hard to do this stuff.

For AIs, we talk about AGI, but for it will be some bespoke set of abilities some of which will be higher than human level and some at human level. It will be some bundle and you’ll need to figure out how to put these bundles together with their human overseers and equipment. I’m just very skeptical of the idea that as soon as you get the bundle, you can just shove millions of them together and manage them.

Any other technological revolution in history has been much more piecemeal than you'd expect on paper. What is the industrial revolution? We dug up coal to power steam engines, used steam engines to run railroads, which helped us get more coal. There’s sort of a Factorio story you can tell where in six hours you can be pumping out thousands of times more coal. In real life, it takes centuries.

For example, with electrification, there's a famous study showing how it took decades after electricity to switch from the pulley and water wheel-based system for steam engines to one that works with more spread-out electrical motors. This will be the same kind of thing. It might take decades to actually get millions of AI researchers to work together effectively.
Leopold Aschenbrenner
This is great. A few responses to that. I totally agree with the real-world bottlenecks idea. It's easy to underrate these constraints. Basically, we’re automating labor and exploiting technology, but there are still many other bottlenecks in the world.

That’s why the story starts narrowly where there aren’t these bottlenecks and then expands to broader areas over time. This is part of why I think initially it’s an AI research explosion. AI research doesn’t run into these real-world bottlenecks. It doesn’t require plowing a field or digging up coal. It’s just doing AI research.
Dwarkesh Patel
I love how in your model, AI research isn’t complicated. It’s like flipping a burger. It’s just AI research.

**Extracted Belief:**

Real-world constraints, such as the need for physical resources and labor, can significantly limit the pace of technological progress, even when technological advancements are made.

**Context:**

Leopold Aschenbrenner agrees with Dwarkesh Patel's point that technological advancements often face real-world limitations.

**Justification:**

Aschenbrenner acknowledges that while automation and technology are progressing rapidly, there are still significant constraints in the real world. This suggests his belief is based on observing the interaction between technological advancement and real-world limitations.

--------

## Chunk 471

**Chunk:**

Dwarkesh Patel
Another thing is how do you actually manage a million AI researchers? Humans’ comparative ability, and we’ve been especially trained for it, is to work in teams. We’ve been learning for thousands of years about how we work together in groups. Despite this, management is a clusterfuck. Most companies are poorly managed. It's really hard to do this stuff.

For AIs, we talk about AGI, but for it will be some bespoke set of abilities some of which will be higher than human level and some at human level. It will be some bundle and you’ll need to figure out how to put these bundles together with their human overseers and equipment. I’m just very skeptical of the idea that as soon as you get the bundle, you can just shove millions of them together and manage them.

Any other technological revolution in history has been much more piecemeal than you'd expect on paper. What is the industrial revolution? We dug up coal to power steam engines, used steam engines to run railroads, which helped us get more coal. There’s sort of a Factorio story you can tell where in six hours you can be pumping out thousands of times more coal. In real life, it takes centuries.

For example, with electrification, there's a famous study showing how it took decades after electricity to switch from the pulley and water wheel-based system for steam engines to one that works with more spread-out electrical motors. This will be the same kind of thing. It might take decades to actually get millions of AI researchers to work together effectively.
Leopold Aschenbrenner
This is great. A few responses to that. I totally agree with the real-world bottlenecks idea. It's easy to underrate these constraints. Basically, we’re automating labor and exploiting technology, but there are still many other bottlenecks in the world.

That’s why the story starts narrowly where there aren’t these bottlenecks and then expands to broader areas over time. This is part of why I think initially it’s an AI research explosion. AI research doesn’t run into these real-world bottlenecks. It doesn’t require plowing a field or digging up coal. It’s just doing AI research.
Dwarkesh Patel
I love how in your model, AI research isn’t complicated. It’s like flipping a burger. It’s just AI research.

**Extracted Belief:**

AI research is initially less constrained by real-world bottlenecks compared to other fields because it primarily involves computational processes and does not rely on physical resources or labor in the same way.

**Context:**

Aschenbrenner explains why AI research may experience rapid initial progress.

**Justification:**

He highlights that AI research primarily involves computational tasks, unlike other fields that require physical resources and labor. This suggests an understanding of how different fields are affected by real-world constraints.

--------

## Chunk 472

**Chunk:**

Dwarkesh Patel
I love how in your model, AI research isn’t complicated. It’s like flipping a burger. It’s just AI research.
Leopold Aschenbrenner
People make these arguments like, “AGI won’t do anything because it can’t flip a burger.” Yeah it won’t be able to flip a burger, but it’ll be able to do algorithmic progress. Once it achieves that, it can figure out how to create a robot that flips burgers. The quantities we’re talking about are a lower bound. We can definitely run 100 million of these.

One of the first things we’ll figure out is how to translate quantity into quality. Even at the baseline rate of progress, you’re quickly getting smarter and smarter systems. It took four years to go from preschooler to high schooler. Pretty quickly, there are probably some simple algorithmic changes you find if you have a hundred Alec Radfors instead of one. You don’t even need a hundred million. We’ll soon have systems that are even smarter and capable of creative, complicated behavior we don’t understand.

Maybe there’s some way to use all this test time compute in a more unified way than all these parallel copies. They won’t just be quantitatively superhuman. They’ll pretty quickly become qualitatively superhuman. It’s like a high school student trying to understand standard physics versus a super-smart professor who gets quantum physics. You quickly enter that regime just given the underlying pace of AI progress but even more quickly with the accelerated force of automated AI research.
Dwarkesh Patel
I agree that over time you would get there. I'm not denying that ASI is possible. I’m just questioning how this happens in a year.

**Extracted Belief:**

AI systems will be able to achieve algorithmic progress, even if they cannot perform tasks like flipping a burger.

**Context:**

Leopold Aschenbrenner is arguing that the ability to perform complex tasks like flipping a burger is not a necessary condition for AI to achieve significant progress, particularly in the domain of algorithmic advancement.

**Justification:**

Aschenbrenner argues that once AI systems achieve algorithmic progress, they can use this ability to develop solutions for tasks like flipping a burger. He highlights that AI's focus is on algorithmic progress, not on mimicking human physical abilities.

--------

## Chunk 473

**Chunk:**

Dwarkesh Patel
I love how in your model, AI research isn’t complicated. It’s like flipping a burger. It’s just AI research.
Leopold Aschenbrenner
People make these arguments like, “AGI won’t do anything because it can’t flip a burger.” Yeah it won’t be able to flip a burger, but it’ll be able to do algorithmic progress. Once it achieves that, it can figure out how to create a robot that flips burgers. The quantities we’re talking about are a lower bound. We can definitely run 100 million of these.

One of the first things we’ll figure out is how to translate quantity into quality. Even at the baseline rate of progress, you’re quickly getting smarter and smarter systems. It took four years to go from preschooler to high schooler. Pretty quickly, there are probably some simple algorithmic changes you find if you have a hundred Alec Radfors instead of one. You don’t even need a hundred million. We’ll soon have systems that are even smarter and capable of creative, complicated behavior we don’t understand.

Maybe there’s some way to use all this test time compute in a more unified way than all these parallel copies. They won’t just be quantitatively superhuman. They’ll pretty quickly become qualitatively superhuman. It’s like a high school student trying to understand standard physics versus a super-smart professor who gets quantum physics. You quickly enter that regime just given the underlying pace of AI progress but even more quickly with the accelerated force of automated AI research.
Dwarkesh Patel
I agree that over time you would get there. I'm not denying that ASI is possible. I’m just questioning how this happens in a year.

**Extracted Belief:**

AI systems will be able to achieve a high degree of intelligence and capability through the use of large numbers of instances, even if they are not qualitatively different from their individual counterparts.

**Context:**

Aschenbrenner is discussing the potential of AI systems to achieve a high level of intelligence through scaling up their numbers. He is suggesting that by running 100 million instances of a system, the overall intelligence and capability can significantly increase.

**Justification:**

Aschenbrenner uses the analogy of a hundred Alec Radfords being more capable than just one, suggesting that a large number of AI systems can create significant improvement through collective learning and problem-solving.

--------

## Chunk 474

**Chunk:**

Dwarkesh Patel
I love how in your model, AI research isn’t complicated. It’s like flipping a burger. It’s just AI research.
Leopold Aschenbrenner
People make these arguments like, “AGI won’t do anything because it can’t flip a burger.” Yeah it won’t be able to flip a burger, but it’ll be able to do algorithmic progress. Once it achieves that, it can figure out how to create a robot that flips burgers. The quantities we’re talking about are a lower bound. We can definitely run 100 million of these.

One of the first things we’ll figure out is how to translate quantity into quality. Even at the baseline rate of progress, you’re quickly getting smarter and smarter systems. It took four years to go from preschooler to high schooler. Pretty quickly, there are probably some simple algorithmic changes you find if you have a hundred Alec Radfors instead of one. You don’t even need a hundred million. We’ll soon have systems that are even smarter and capable of creative, complicated behavior we don’t understand.

Maybe there’s some way to use all this test time compute in a more unified way than all these parallel copies. They won’t just be quantitatively superhuman. They’ll pretty quickly become qualitatively superhuman. It’s like a high school student trying to understand standard physics versus a super-smart professor who gets quantum physics. You quickly enter that regime just given the underlying pace of AI progress but even more quickly with the accelerated force of automated AI research.
Dwarkesh Patel
I agree that over time you would get there. I'm not denying that ASI is possible. I’m just questioning how this happens in a year.

**Extracted Belief:**

AI systems will eventually surpass human intelligence and capabilities, becoming qualitatively superhuman.

**Context:**

Aschenbrenner is making a prediction about the future of AI, suggesting that it will eventually surpass human intelligence and reach a qualitatively different level of understanding.

**Justification:**

He draws a comparison between a high school student's understanding of standard physics and a super-smart professor's understanding of quantum physics. This comparison suggests that AI systems will not only become more intelligent but also develop a fundamentally different understanding of the world.

--------

## Chunk 475

**Chunk:**

Dwarkesh Patel
I agree that over time you would get there. I'm not denying that ASI is possible. I’m just questioning how this happens in a year.
Leopold Aschenbrenner
The story is a bit more continuous. By 2025 or 2026, you’ll already have models as good as a college graduate. I don’t know where all the unhobbling is going to be but even it’s possible that you have a proto-automated engineer.

There’s a bit of an AGI smear that there are unhobblings missing. There’s ways of connecting them that are missing. There’s some level of intelligence you’re missing. At some point you are going to get this thing that is 100% automated Alec Radford and once you have that, things really take off.
Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?

**Extracted Belief:**

By 2025 or 2026, AI models will be as intelligent as a college graduate.

**Context:**

Leopold Aschenbrenner is making a prediction about the capabilities of AI models in the near future, drawing on his expertise in the field.

**Justification:**

Aschenbrenner does not explicitly state his justification for this belief, but it is likely based on his understanding of the current pace of AI development and the projected capabilities of future AI systems.

--------

## Chunk 476

**Chunk:**

Dwarkesh Patel
I agree that over time you would get there. I'm not denying that ASI is possible. I’m just questioning how this happens in a year.
Leopold Aschenbrenner
The story is a bit more continuous. By 2025 or 2026, you’ll already have models as good as a college graduate. I don’t know where all the unhobbling is going to be but even it’s possible that you have a proto-automated engineer.

There’s a bit of an AGI smear that there are unhobblings missing. There’s ways of connecting them that are missing. There’s some level of intelligence you’re missing. At some point you are going to get this thing that is 100% automated Alec Radford and once you have that, things really take off.
Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?

**Extracted Belief:**

There are currently missing 'unhobbblings' or missing connections in AI development, limiting its progress.

**Context:**

Aschenbrenner describes the limitations of current AI systems as a result of missing components or connections, implying that these limitations hinder the full potential of AI.

**Justification:**

Aschenbrenner doesn't provide specific examples of these 'unhobbblings,' but he implies they represent limitations that need to be overcome for AI to reach its full potential. It's likely based on his understanding of the current state of AI research.

--------

## Chunk 477

**Chunk:**

Dwarkesh Patel
I agree that over time you would get there. I'm not denying that ASI is possible. I’m just questioning how this happens in a year.
Leopold Aschenbrenner
The story is a bit more continuous. By 2025 or 2026, you’ll already have models as good as a college graduate. I don’t know where all the unhobbling is going to be but even it’s possible that you have a proto-automated engineer.

There’s a bit of an AGI smear that there are unhobblings missing. There’s ways of connecting them that are missing. There’s some level of intelligence you’re missing. At some point you are going to get this thing that is 100% automated Alec Radford and once you have that, things really take off.
Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?

**Extracted Belief:**

The development of a fully automated AI system, capable of acting as an engineer, will lead to a significant acceleration of AI progress.

**Context:**

Aschenbrenner suggests that the development of an AI capable of independent engineering work will be a crucial milestone, enabling a significant acceleration in AI development.

**Justification:**

Aschenbrenner does not directly explain why a fully automated engineer would accelerate AI progress, but it likely reflects his view that such a system would be able to autonomously address many of the current challenges in AI research and development.

--------

## Chunk 478

**Chunk:**

Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?
Leopold Aschenbrenner
There are two key things: the unhobbling and the data wall. Let's talk about the data wall for a moment. Even though we’re seeing crazy AI progress, the data wall is actually underrated. There's a real scenario where we stagnate because we've been riding this tailwind of easily bootstrapping unsupervised learning.

It learns these amazing world models. You just buy more compute, make simple efficiency changes, and get big gains. All of the big gains in efficiency have been pretty dumb things. You add a normalization layer. You fix scaling laws. These have already been huge things, let alone obvious ways in which these models aren’t good yet.

The data wall is a big deal. For instance, Common Crawl online is about 30 trillion tokens. Llama-3 was trained on 15 trillion tokens. We're already using all of the data. You can get somewhat further by repeating data, but an academic paper by Boaz Barak that does scaling laws for this. It says that after about 16 repetitions, the returns basically go to zero.

Llama-3 is already at the limit of data. Maybe we can get 10x more by repeating data. At most that’s a 100x model than GPT-4, a 100x effective compute from GPT-4. That’s not that much. If you do half an OOM of compute and half an OOM of algorithmic progress each year, that's like two years from GPT-4. GPT-4 finished pre-training in 2022, so it’s 2024. We won’t quite know by the end of the year but by 2025 and 2026 we’ll get a sense of if we’re cracking the data wall.
Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?

**Extracted Belief:**

The 'data wall' is a significant limitation in current AI progress.

**Context:**

Leopold Aschenbrenner is explaining the challenges to continued rapid AI progress, arguing that the limited amount of available data may be a bottleneck.

**Justification:**

He uses the example of Common Crawl and Llama-3, noting that they are already using all available data. He further cites Boaz Barak's academic paper on scaling laws, which suggests diminishing returns after 16 repetitions of data.

--------

## Chunk 479

**Chunk:**

Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?
Leopold Aschenbrenner
There are two key things: the unhobbling and the data wall. Let's talk about the data wall for a moment. Even though we’re seeing crazy AI progress, the data wall is actually underrated. There's a real scenario where we stagnate because we've been riding this tailwind of easily bootstrapping unsupervised learning.

It learns these amazing world models. You just buy more compute, make simple efficiency changes, and get big gains. All of the big gains in efficiency have been pretty dumb things. You add a normalization layer. You fix scaling laws. These have already been huge things, let alone obvious ways in which these models aren’t good yet.

The data wall is a big deal. For instance, Common Crawl online is about 30 trillion tokens. Llama-3 was trained on 15 trillion tokens. We're already using all of the data. You can get somewhat further by repeating data, but an academic paper by Boaz Barak that does scaling laws for this. It says that after about 16 repetitions, the returns basically go to zero.

Llama-3 is already at the limit of data. Maybe we can get 10x more by repeating data. At most that’s a 100x model than GPT-4, a 100x effective compute from GPT-4. That’s not that much. If you do half an OOM of compute and half an OOM of algorithmic progress each year, that's like two years from GPT-4. GPT-4 finished pre-training in 2022, so it’s 2024. We won’t quite know by the end of the year but by 2025 and 2026 we’ll get a sense of if we’re cracking the data wall.
Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?

**Extracted Belief:**

Current AI progress is driven by easily bootstrapping unsupervised learning, which has led to significant efficiency gains.

**Context:**

Leopold Aschenbrenner is explaining the current state of AI progress and its dependence on readily available data for training.

**Justification:**

He mentions that the current progress relies on unsupervised learning, which is achieved through easily accessible data and simple efficiency changes, like adding normalization layers or fixing scaling laws.

--------

## Chunk 480

**Chunk:**

Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?
Leopold Aschenbrenner
There are two key things: the unhobbling and the data wall. Let's talk about the data wall for a moment. Even though we’re seeing crazy AI progress, the data wall is actually underrated. There's a real scenario where we stagnate because we've been riding this tailwind of easily bootstrapping unsupervised learning.

It learns these amazing world models. You just buy more compute, make simple efficiency changes, and get big gains. All of the big gains in efficiency have been pretty dumb things. You add a normalization layer. You fix scaling laws. These have already been huge things, let alone obvious ways in which these models aren’t good yet.

The data wall is a big deal. For instance, Common Crawl online is about 30 trillion tokens. Llama-3 was trained on 15 trillion tokens. We're already using all of the data. You can get somewhat further by repeating data, but an academic paper by Boaz Barak that does scaling laws for this. It says that after about 16 repetitions, the returns basically go to zero.

Llama-3 is already at the limit of data. Maybe we can get 10x more by repeating data. At most that’s a 100x model than GPT-4, a 100x effective compute from GPT-4. That’s not that much. If you do half an OOM of compute and half an OOM of algorithmic progress each year, that's like two years from GPT-4. GPT-4 finished pre-training in 2022, so it’s 2024. We won’t quite know by the end of the year but by 2025 and 2026 we’ll get a sense of if we’re cracking the data wall.
Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?

**Extracted Belief:**

Simple efficiency changes, like adding normalization layers or fixing scaling laws, have yielded substantial improvements in AI systems.

**Context:**

Leopold Aschenbrenner is highlighting the role of efficiency changes in accelerating AI progress.

**Justification:**

He states that the big gains in efficiency have been achieved through relatively simple improvements such as normalization layers and scaling laws, which have had a significant impact on AI performance.

--------

## Chunk 481

**Chunk:**

Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?
Leopold Aschenbrenner
There are two key things: the unhobbling and the data wall. Let's talk about the data wall for a moment. Even though we’re seeing crazy AI progress, the data wall is actually underrated. There's a real scenario where we stagnate because we've been riding this tailwind of easily bootstrapping unsupervised learning.

It learns these amazing world models. You just buy more compute, make simple efficiency changes, and get big gains. All of the big gains in efficiency have been pretty dumb things. You add a normalization layer. You fix scaling laws. These have already been huge things, let alone obvious ways in which these models aren’t good yet.

The data wall is a big deal. For instance, Common Crawl online is about 30 trillion tokens. Llama-3 was trained on 15 trillion tokens. We're already using all of the data. You can get somewhat further by repeating data, but an academic paper by Boaz Barak that does scaling laws for this. It says that after about 16 repetitions, the returns basically go to zero.

Llama-3 is already at the limit of data. Maybe we can get 10x more by repeating data. At most that’s a 100x model than GPT-4, a 100x effective compute from GPT-4. That’s not that much. If you do half an OOM of compute and half an OOM of algorithmic progress each year, that's like two years from GPT-4. GPT-4 finished pre-training in 2022, so it’s 2024. We won’t quite know by the end of the year but by 2025 and 2026 we’ll get a sense of if we’re cracking the data wall.
Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?

**Extracted Belief:**

Repeating data for training AI models yields diminishing returns after about 16 repetitions.

**Context:**

Leopold Aschenbrenner is explaining the limitations of data repetition for training AI models.

**Justification:**

He refers to an academic paper by Boaz Barak on scaling laws, which shows that after 16 repetitions of data, the returns on AI performance are minimal.

--------

## Chunk 482

**Chunk:**

Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?
Leopold Aschenbrenner
There are two key things: the unhobbling and the data wall. Let's talk about the data wall for a moment. Even though we’re seeing crazy AI progress, the data wall is actually underrated. There's a real scenario where we stagnate because we've been riding this tailwind of easily bootstrapping unsupervised learning.

It learns these amazing world models. You just buy more compute, make simple efficiency changes, and get big gains. All of the big gains in efficiency have been pretty dumb things. You add a normalization layer. You fix scaling laws. These have already been huge things, let alone obvious ways in which these models aren’t good yet.

The data wall is a big deal. For instance, Common Crawl online is about 30 trillion tokens. Llama-3 was trained on 15 trillion tokens. We're already using all of the data. You can get somewhat further by repeating data, but an academic paper by Boaz Barak that does scaling laws for this. It says that after about 16 repetitions, the returns basically go to zero.

Llama-3 is already at the limit of data. Maybe we can get 10x more by repeating data. At most that’s a 100x model than GPT-4, a 100x effective compute from GPT-4. That’s not that much. If you do half an OOM of compute and half an OOM of algorithmic progress each year, that's like two years from GPT-4. GPT-4 finished pre-training in 2022, so it’s 2024. We won’t quite know by the end of the year but by 2025 and 2026 we’ll get a sense of if we’re cracking the data wall.
Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?

**Extracted Belief:**

Current AI models, such as Llama-3, are already at the limit of available data for training.

**Context:**

Leopold Aschenbrenner is highlighting the current limitations in data availability for AI training.

**Justification:**

He states that Llama-3 has been trained on 15 trillion tokens, close to the limit of available data, suggesting that reaching significant performance gains through data alone may be difficult.

--------

## Chunk 483

**Chunk:**

Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?
Leopold Aschenbrenner
There are two key things: the unhobbling and the data wall. Let's talk about the data wall for a moment. Even though we’re seeing crazy AI progress, the data wall is actually underrated. There's a real scenario where we stagnate because we've been riding this tailwind of easily bootstrapping unsupervised learning.

It learns these amazing world models. You just buy more compute, make simple efficiency changes, and get big gains. All of the big gains in efficiency have been pretty dumb things. You add a normalization layer. You fix scaling laws. These have already been huge things, let alone obvious ways in which these models aren’t good yet.

The data wall is a big deal. For instance, Common Crawl online is about 30 trillion tokens. Llama-3 was trained on 15 trillion tokens. We're already using all of the data. You can get somewhat further by repeating data, but an academic paper by Boaz Barak that does scaling laws for this. It says that after about 16 repetitions, the returns basically go to zero.

Llama-3 is already at the limit of data. Maybe we can get 10x more by repeating data. At most that’s a 100x model than GPT-4, a 100x effective compute from GPT-4. That’s not that much. If you do half an OOM of compute and half an OOM of algorithmic progress each year, that's like two years from GPT-4. GPT-4 finished pre-training in 2022, so it’s 2024. We won’t quite know by the end of the year but by 2025 and 2026 we’ll get a sense of if we’re cracking the data wall.
Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?

**Extracted Belief:**

The current amount of data available may be sufficient to train AI models powerful enough to initiate self-play reinforcement learning.

**Context:**

Leopold Aschenbrenner is discussing the possibility of AI models achieving self-learning capabilities with the current amount of data.

**Justification:**

He is considering the scenario of having three OOMs less data than currently available and speculating that it might still be sufficient to train models capable of self-learning.

--------

## Chunk 484

**Chunk:**

Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?
Leopold Aschenbrenner
There are two key things: the unhobbling and the data wall. Let's talk about the data wall for a moment. Even though we’re seeing crazy AI progress, the data wall is actually underrated. There's a real scenario where we stagnate because we've been riding this tailwind of easily bootstrapping unsupervised learning.

It learns these amazing world models. You just buy more compute, make simple efficiency changes, and get big gains. All of the big gains in efficiency have been pretty dumb things. You add a normalization layer. You fix scaling laws. These have already been huge things, let alone obvious ways in which these models aren’t good yet.

The data wall is a big deal. For instance, Common Crawl online is about 30 trillion tokens. Llama-3 was trained on 15 trillion tokens. We're already using all of the data. You can get somewhat further by repeating data, but an academic paper by Boaz Barak that does scaling laws for this. It says that after about 16 repetitions, the returns basically go to zero.

Llama-3 is already at the limit of data. Maybe we can get 10x more by repeating data. At most that’s a 100x model than GPT-4, a 100x effective compute from GPT-4. That’s not that much. If you do half an OOM of compute and half an OOM of algorithmic progress each year, that's like two years from GPT-4. GPT-4 finished pre-training in 2022, so it’s 2024. We won’t quite know by the end of the year but by 2025 and 2026 we’ll get a sense of if we’re cracking the data wall.
Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?

**Extracted Belief:**

A significant decrease in the amount of training data would negatively impact AI progress.

**Context:**

Leopold Aschenbrenner is discussing the impact of data availability on AI progress.

**Justification:**

He suggests that a reduction of three OOMs in data would be detrimental to AI progress, limiting models to a level of performance barely better than GPT-2.

--------

## Chunk 485

**Chunk:**

Dwarkesh Patel
Let’s go back to the unhobbling.

We’re going to get a bunch of models by the end of the year. Suppose we didn’t get some capacity by the end of the year. Is there some such capacity which us lacking would suggest that AI progress will take longer than you are projecting?
Leopold Aschenbrenner
There are two key things: the unhobbling and the data wall. Let's talk about the data wall for a moment. Even though we’re seeing crazy AI progress, the data wall is actually underrated. There's a real scenario where we stagnate because we've been riding this tailwind of easily bootstrapping unsupervised learning.

It learns these amazing world models. You just buy more compute, make simple efficiency changes, and get big gains. All of the big gains in efficiency have been pretty dumb things. You add a normalization layer. You fix scaling laws. These have already been huge things, let alone obvious ways in which these models aren’t good yet.

The data wall is a big deal. For instance, Common Crawl online is about 30 trillion tokens. Llama-3 was trained on 15 trillion tokens. We're already using all of the data. You can get somewhat further by repeating data, but an academic paper by Boaz Barak that does scaling laws for this. It says that after about 16 repetitions, the returns basically go to zero.

Llama-3 is already at the limit of data. Maybe we can get 10x more by repeating data. At most that’s a 100x model than GPT-4, a 100x effective compute from GPT-4. That’s not that much. If you do half an OOM of compute and half an OOM of algorithmic progress each year, that's like two years from GPT-4. GPT-4 finished pre-training in 2022, so it’s 2024. We won’t quite know by the end of the year but by 2025 and 2026 we’ll get a sense of if we’re cracking the data wall.
Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?

**Extracted Belief:**

The availability of more data would accelerate AI progress.

**Context:**

Leopold Aschenbrenner is speculating on the effects of increased data availability on AI progress.

**Justification:**

He states that having one or two OOMs of more data would be beneficial for AI development, although he does not explicitly quantify the impact.

--------

## Chunk 486

**Chunk:**

Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?
Leopold Aschenbrenner
3 OOMs less data is pretty rough. That would mean 6 OOMs less compute model and Chinchilla scaling laws. That’s basically capping out at something barely better than GPT-2. That would be really rough.

You make an interesting point about contingency. If we consider the human trajectory analogy, a preschooler model can't learn from itself. An elementary school model can't learn from itself. Maybe GPT-4 is like a smart high schooler that can start learning from itself. Ideally, you want a somewhat better model that can truly learn by itself. 1 OOM less data would make me more iffy, but it might still be doable. It would feel chiller if we had 1 or two OOMs of more data.
Dwarkesh Patel
It would be an interesting exercise to get probability distributions of AGI contingent across OOMs of data.

**Extracted Belief:**

Having 3 OOMs less data for training AI models would significantly hinder progress, resulting in models with capabilities comparable to GPT-2, which are considered insufficient for self-learning.

**Context:**

Leopold Aschenbrenner responds to a hypothetical scenario where AI training data is significantly reduced, drawing a comparison to existing model limitations.

**Justification:**

He states that 3 OOMs less data would equate to 6 OOMs less compute model based on Chinchilla scaling laws, which would limit the model's capabilities to levels comparable to GPT-2.

--------

## Chunk 487

**Chunk:**

Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?
Leopold Aschenbrenner
3 OOMs less data is pretty rough. That would mean 6 OOMs less compute model and Chinchilla scaling laws. That’s basically capping out at something barely better than GPT-2. That would be really rough.

You make an interesting point about contingency. If we consider the human trajectory analogy, a preschooler model can't learn from itself. An elementary school model can't learn from itself. Maybe GPT-4 is like a smart high schooler that can start learning from itself. Ideally, you want a somewhat better model that can truly learn by itself. 1 OOM less data would make me more iffy, but it might still be doable. It would feel chiller if we had 1 or two OOMs of more data.
Dwarkesh Patel
It would be an interesting exercise to get probability distributions of AGI contingent across OOMs of data.

**Extracted Belief:**

The current amount of data available for training AI models is sufficient to develop models capable of self-learning, similar to the way humans develop the capacity for self-learning as they progress through different stages of education.

**Context:**

Leopold Aschenbrenner uses the analogy of human development to illustrate his belief about the current data availability and its impact on AI development.

**Justification:**

He draws a comparison between the progression of a human's learning ability from preschool to high school and the development of AI models, suggesting that the current data availability allows for AI models to reach a level comparable to a 'smart high schooler' capable of self-learning.

--------

## Chunk 488

**Chunk:**

Dwarkesh Patel
Suppose we had three OOMs less data in Common Crawl on the Internet than we happen to have now. For decades, with the Internet and other things, the stock of data humanity has has been rapidly increasing. Is it your view that, for contingent reasons, we just happen to have enough data to train models just powerful enough, like GPT-4.5, to kick off the self-play RL loop?

Or is it just that if it had been 3 OOMs higher, then progress would have been slightly faster? In that world, we would have been looking back, thinking it would have been hard to kick off the RL explosion with just GPT-4.5. We would have figured it out eventually.

In this world, we would have gotten to GPT-3 and then had to kick off some sort of RL explosion. We would have still figured it out. Did we just luck out on the amount of data we happen to have in the world?
Leopold Aschenbrenner
3 OOMs less data is pretty rough. That would mean 6 OOMs less compute model and Chinchilla scaling laws. That’s basically capping out at something barely better than GPT-2. That would be really rough.

You make an interesting point about contingency. If we consider the human trajectory analogy, a preschooler model can't learn from itself. An elementary school model can't learn from itself. Maybe GPT-4 is like a smart high schooler that can start learning from itself. Ideally, you want a somewhat better model that can truly learn by itself. 1 OOM less data would make me more iffy, but it might still be doable. It would feel chiller if we had 1 or two OOMs of more data.
Dwarkesh Patel
It would be an interesting exercise to get probability distributions of AGI contingent across OOMs of data.

**Extracted Belief:**

Having 1 or 2 OOMs more data for training AI models would be beneficial but is not a critical requirement for achieving self-learning AI.

**Context:**

Leopold Aschenbrenner expresses a preference for more training data but acknowledges that it might not be essential for AI to reach the level of self-learning.

**Justification:**

He states that while 1 or 2 OOMs of more data would be 'chiller', having 1 OOM less data might still be 'doable'. This suggests that he believes self-learning AI is achievable with the current data levels, but more data would be advantageous.

--------

## Chunk 489

**Chunk:**

Dwarkesh Patel
It would be an interesting exercise to get probability distributions of AGI contingent across OOMs of data.
Leopold Aschenbrenner
Yeah, I agree.
Dwarkesh Patel
The thing that makes me skeptical of this story is that it totally makes sense why pre-training works so well. With these other things, there are stories of why they ought to work in principle.. Humans can learn this way and so on. Maybe they're true. 

I worry that a lot of this case is based on first principles evaluation of how learning happens. Maybe fundamentally, we don't understand how humans learn. Maybe there's some key thing we're missing. On sample efficiency, you say the fact that these things are way less sample efficient than humans in learning suggests there's a lot of room for improvement. Another perspective is that we are just on the wrong path altogether. That’s why they’re so sample inefficient when it comes to pre-training.

There are a lot of first principles arguments stacked on top of each other where you get these unhobblings, then you get to AGI. Then because of these reasons why you can stack all these things on top of each other and you get to ASI. I'm worried that there are too many steps of this sort of first principles thinking.

**Extracted Belief:**

Current AI models are significantly less sample efficient than humans in learning.

**Context:**

While discussing the potential limitations of AI development, Leopold Aschenbrenner acknowledges the lower sample efficiency of AI models compared to humans in learning.

**Justification:**

Leopold Aschenbrenner states that the fact that AI models are way less sample efficient than humans in learning suggests there's a lot of room for improvement.

--------

## Chunk 490

**Chunk:**

Dwarkesh Patel
The thing that makes me skeptical of this story is that it totally makes sense why pre-training works so well. With these other things, there are stories of why they ought to work in principle.. Humans can learn this way and so on. Maybe they're true. 

I worry that a lot of this case is based on first principles evaluation of how learning happens. Maybe fundamentally, we don't understand how humans learn. Maybe there's some key thing we're missing. On sample efficiency, you say the fact that these things are way less sample efficient than humans in learning suggests there's a lot of room for improvement. Another perspective is that we are just on the wrong path altogether. That’s why they’re so sample inefficient when it comes to pre-training.

There are a lot of first principles arguments stacked on top of each other where you get these unhobblings, then you get to AGI. Then because of these reasons why you can stack all these things on top of each other and you get to ASI. I'm worried that there are too many steps of this sort of first principles thinking.
Leopold Aschenbrenner
We'll see. On sample efficiency, it’s sort of first principles but there's this clear missing middle. People hadn't been trying. Now people are really trying. Again, often in deep learning something like the obvious thing works and there are a lot of details to get right. It might take some time, but now people are really trying. We will get a lot of signal in the next couple of years on unhobbling.

What is the signal on unhobbling that would be interesting? The question is basically, are you making progress on test time compute? Is this thing able to think longer horizon than just a couple hundred tokens? That was unlocked by chain-of-thought.
Dwarkesh Patel
On that point in particular, many people who have longer timelines have come on the podcast and made the point that the way to train this long horizon RL, it's not…

Earlier we were talking about how they can think for five minutes, but not for longer. It's not because they can't physically output an hour's worth of tokens.

**Extracted Belief:**

Deep learning models are often less sample efficient than humans in learning, indicating potential for improvement.

**Context:**

Responding to Dwarkesh Patel's skepticism about the current path to artificial general intelligence (AGI), Leopold Aschenbrenner acknowledges the lower sample efficiency of deep learning models compared to humans.

**Justification:**

The fact that current deep learning models require significantly more data to achieve comparable learning outcomes to humans is cited as evidence for this belief.

--------

## Chunk 491

**Chunk:**

Dwarkesh Patel
The thing that makes me skeptical of this story is that it totally makes sense why pre-training works so well. With these other things, there are stories of why they ought to work in principle.. Humans can learn this way and so on. Maybe they're true. 

I worry that a lot of this case is based on first principles evaluation of how learning happens. Maybe fundamentally, we don't understand how humans learn. Maybe there's some key thing we're missing. On sample efficiency, you say the fact that these things are way less sample efficient than humans in learning suggests there's a lot of room for improvement. Another perspective is that we are just on the wrong path altogether. That’s why they’re so sample inefficient when it comes to pre-training.

There are a lot of first principles arguments stacked on top of each other where you get these unhobblings, then you get to AGI. Then because of these reasons why you can stack all these things on top of each other and you get to ASI. I'm worried that there are too many steps of this sort of first principles thinking.
Leopold Aschenbrenner
We'll see. On sample efficiency, it’s sort of first principles but there's this clear missing middle. People hadn't been trying. Now people are really trying. Again, often in deep learning something like the obvious thing works and there are a lot of details to get right. It might take some time, but now people are really trying. We will get a lot of signal in the next couple of years on unhobbling.

What is the signal on unhobbling that would be interesting? The question is basically, are you making progress on test time compute? Is this thing able to think longer horizon than just a couple hundred tokens? That was unlocked by chain-of-thought.
Dwarkesh Patel
On that point in particular, many people who have longer timelines have come on the podcast and made the point that the way to train this long horizon RL, it's not…

Earlier we were talking about how they can think for five minutes, but not for longer. It's not because they can't physically output an hour's worth of tokens.

**Extracted Belief:**

The 'obvious' solution in deep learning often works, but requires extensive refinement and detail.

**Context:**

Leopold Aschenbrenner emphasizes the importance of getting details right in deep learning despite the existence of seemingly straightforward solutions.

**Justification:**

His statement suggests that while obvious approaches exist, implementation and optimization are crucial to success, implying an empirical observation of this trend in deep learning.

--------

## Chunk 492

**Chunk:**

Dwarkesh Patel
The thing that makes me skeptical of this story is that it totally makes sense why pre-training works so well. With these other things, there are stories of why they ought to work in principle.. Humans can learn this way and so on. Maybe they're true. 

I worry that a lot of this case is based on first principles evaluation of how learning happens. Maybe fundamentally, we don't understand how humans learn. Maybe there's some key thing we're missing. On sample efficiency, you say the fact that these things are way less sample efficient than humans in learning suggests there's a lot of room for improvement. Another perspective is that we are just on the wrong path altogether. That’s why they’re so sample inefficient when it comes to pre-training.

There are a lot of first principles arguments stacked on top of each other where you get these unhobblings, then you get to AGI. Then because of these reasons why you can stack all these things on top of each other and you get to ASI. I'm worried that there are too many steps of this sort of first principles thinking.
Leopold Aschenbrenner
We'll see. On sample efficiency, it’s sort of first principles but there's this clear missing middle. People hadn't been trying. Now people are really trying. Again, often in deep learning something like the obvious thing works and there are a lot of details to get right. It might take some time, but now people are really trying. We will get a lot of signal in the next couple of years on unhobbling.

What is the signal on unhobbling that would be interesting? The question is basically, are you making progress on test time compute? Is this thing able to think longer horizon than just a couple hundred tokens? That was unlocked by chain-of-thought.
Dwarkesh Patel
On that point in particular, many people who have longer timelines have come on the podcast and made the point that the way to train this long horizon RL, it's not…

Earlier we were talking about how they can think for five minutes, but not for longer. It's not because they can't physically output an hour's worth of tokens.

**Extracted Belief:**

Significant progress on 'unhobbling' deep learning models, particularly in improving their ability to think long-term, is anticipated within the next few years.

**Context:**

Leopold Aschenbrenner expresses optimism about future advancements in deep learning, particularly in the area of 'unhobbling' models to enhance their reasoning capabilities.

**Justification:**

He states that 'we will get a lot of signal in the next couple of years on unhobbling,' suggesting a high degree of confidence in the progress anticipated.

--------

## Chunk 493

**Chunk:**

Dwarkesh Patel
The thing that makes me skeptical of this story is that it totally makes sense why pre-training works so well. With these other things, there are stories of why they ought to work in principle.. Humans can learn this way and so on. Maybe they're true. 

I worry that a lot of this case is based on first principles evaluation of how learning happens. Maybe fundamentally, we don't understand how humans learn. Maybe there's some key thing we're missing. On sample efficiency, you say the fact that these things are way less sample efficient than humans in learning suggests there's a lot of room for improvement. Another perspective is that we are just on the wrong path altogether. That’s why they’re so sample inefficient when it comes to pre-training.

There are a lot of first principles arguments stacked on top of each other where you get these unhobblings, then you get to AGI. Then because of these reasons why you can stack all these things on top of each other and you get to ASI. I'm worried that there are too many steps of this sort of first principles thinking.
Leopold Aschenbrenner
We'll see. On sample efficiency, it’s sort of first principles but there's this clear missing middle. People hadn't been trying. Now people are really trying. Again, often in deep learning something like the obvious thing works and there are a lot of details to get right. It might take some time, but now people are really trying. We will get a lot of signal in the next couple of years on unhobbling.

What is the signal on unhobbling that would be interesting? The question is basically, are you making progress on test time compute? Is this thing able to think longer horizon than just a couple hundred tokens? That was unlocked by chain-of-thought.
Dwarkesh Patel
On that point in particular, many people who have longer timelines have come on the podcast and made the point that the way to train this long horizon RL, it's not…

Earlier we were talking about how they can think for five minutes, but not for longer. It's not because they can't physically output an hour's worth of tokens.

**Extracted Belief:**

The ability to think over longer time horizons is a key aspect of 'unhobbling' deep learning models.

**Context:**

When discussing the signal of progress in 'unhobbling,' Leopold Aschenbrenner focuses on the ability of deep learning models to think beyond a short time horizon.

**Justification:**

This statement implicitly connects the ability to think long-term to the broader concept of 'unhobbling,' suggesting it as a key component in unlocking more sophisticated reasoning capabilities.

--------

## Chunk 494

**Chunk:**

Dwarkesh Patel
The thing that makes me skeptical of this story is that it totally makes sense why pre-training works so well. With these other things, there are stories of why they ought to work in principle.. Humans can learn this way and so on. Maybe they're true. 

I worry that a lot of this case is based on first principles evaluation of how learning happens. Maybe fundamentally, we don't understand how humans learn. Maybe there's some key thing we're missing. On sample efficiency, you say the fact that these things are way less sample efficient than humans in learning suggests there's a lot of room for improvement. Another perspective is that we are just on the wrong path altogether. That’s why they’re so sample inefficient when it comes to pre-training.

There are a lot of first principles arguments stacked on top of each other where you get these unhobblings, then you get to AGI. Then because of these reasons why you can stack all these things on top of each other and you get to ASI. I'm worried that there are too many steps of this sort of first principles thinking.
Leopold Aschenbrenner
We'll see. On sample efficiency, it’s sort of first principles but there's this clear missing middle. People hadn't been trying. Now people are really trying. Again, often in deep learning something like the obvious thing works and there are a lot of details to get right. It might take some time, but now people are really trying. We will get a lot of signal in the next couple of years on unhobbling.

What is the signal on unhobbling that would be interesting? The question is basically, are you making progress on test time compute? Is this thing able to think longer horizon than just a couple hundred tokens? That was unlocked by chain-of-thought.
Dwarkesh Patel
On that point in particular, many people who have longer timelines have come on the podcast and made the point that the way to train this long horizon RL, it's not…

Earlier we were talking about how they can think for five minutes, but not for longer. It's not because they can't physically output an hour's worth of tokens.

**Extracted Belief:**

Chain-of-thought reasoning has been instrumental in enabling deep learning models to think over longer time horizons.

**Context:**

Leopold Aschenbrenner mentions chain-of-thought as a key breakthrough in extending the reasoning horizon of deep learning models.

**Justification:**

This statement suggests that chain-of-thought has been observed to have a tangible impact on improving the long-term thinking abilities of deep learning models.

--------

## Chunk 495

**Chunk:**

Dwarkesh Patel
On that point in particular, many people who have longer timelines have come on the podcast and made the point that the way to train this long horizon RL, it's not…

Earlier we were talking about how they can think for five minutes, but not for longer. It's not because they can't physically output an hour's worth of tokens.
Leopold Aschenbrenner
Even Gemini has a million in context, and the million of context is actually great for consumption. It solves one important hobbling, which is the onboarding problem. A new coworker in your first five minutes, like a new smart high school intern, is not useful at all.

A month in, they’re much more useful because they've looked at the monorepo, understand how the code works, and they've read your internal docs. Being able to put that in context solves this onboarding problem. They're not good at the production of a million tokens yet.
Dwarkesh Patel
On the production of a million tokens, there's no public evidence that there's some easy loss function where you can...

**Extracted Belief:**

Gemini, a large language model, has a context window of one million tokens.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's skepticism about the ability of AI to think for long horizons, specifically referring to Gemini's limitations.

**Justification:**

He states that "Even Gemini has a million in context" in reference to its context window.

--------

## Chunk 496

**Chunk:**

Dwarkesh Patel
On that point in particular, many people who have longer timelines have come on the podcast and made the point that the way to train this long horizon RL, it's not…

Earlier we were talking about how they can think for five minutes, but not for longer. It's not because they can't physically output an hour's worth of tokens.
Leopold Aschenbrenner
Even Gemini has a million in context, and the million of context is actually great for consumption. It solves one important hobbling, which is the onboarding problem. A new coworker in your first five minutes, like a new smart high school intern, is not useful at all.

A month in, they’re much more useful because they've looked at the monorepo, understand how the code works, and they've read your internal docs. Being able to put that in context solves this onboarding problem. They're not good at the production of a million tokens yet.
Dwarkesh Patel
On the production of a million tokens, there's no public evidence that there's some easy loss function where you can...

**Extracted Belief:**

A large context window is beneficial for consumption and can solve the onboarding problem for new AI systems.

**Context:**

Leopold Aschenbrenner is explaining how a large context window can be useful for AI systems.

**Justification:**

He states that "the million of context is actually great for consumption. It solves one important hobbling, which is the onboarding problem." This suggests that having a large context window allows AI to learn and understand new information more effectively, similar to how humans learn from their experiences.

--------

## Chunk 497

**Chunk:**

Dwarkesh Patel
On that point in particular, many people who have longer timelines have come on the podcast and made the point that the way to train this long horizon RL, it's not…

Earlier we were talking about how they can think for five minutes, but not for longer. It's not because they can't physically output an hour's worth of tokens.
Leopold Aschenbrenner
Even Gemini has a million in context, and the million of context is actually great for consumption. It solves one important hobbling, which is the onboarding problem. A new coworker in your first five minutes, like a new smart high school intern, is not useful at all.

A month in, they’re much more useful because they've looked at the monorepo, understand how the code works, and they've read your internal docs. Being able to put that in context solves this onboarding problem. They're not good at the production of a million tokens yet.
Dwarkesh Patel
On the production of a million tokens, there's no public evidence that there's some easy loss function where you can...

**Extracted Belief:**

New AI systems, like a new intern, are initially less effective but become more useful with time as they learn and gain experience.

**Context:**

Leopold Aschenbrenner uses an analogy to explain how a large context window can be beneficial for AI systems.

**Justification:**

He compares a new AI system to a new intern who initially is "not useful at all" but becomes "much more useful" after gaining experience by "looking at the monorepo, understand how the code works, and they've read your internal docs."

--------

## Chunk 498

**Chunk:**

Dwarkesh Patel
On that point in particular, many people who have longer timelines have come on the podcast and made the point that the way to train this long horizon RL, it's not…

Earlier we were talking about how they can think for five minutes, but not for longer. It's not because they can't physically output an hour's worth of tokens.
Leopold Aschenbrenner
Even Gemini has a million in context, and the million of context is actually great for consumption. It solves one important hobbling, which is the onboarding problem. A new coworker in your first five minutes, like a new smart high school intern, is not useful at all.

A month in, they’re much more useful because they've looked at the monorepo, understand how the code works, and they've read your internal docs. Being able to put that in context solves this onboarding problem. They're not good at the production of a million tokens yet.
Dwarkesh Patel
On the production of a million tokens, there's no public evidence that there's some easy loss function where you can...

**Extracted Belief:**

AI systems with a large context window are not yet proficient at producing a million tokens of output.

**Context:**

Leopold Aschenbrenner acknowledges the limitations of AI systems with large context windows.

**Justification:**

He states that "They're not good at the production of a million tokens yet." This implies that while large context windows are beneficial for understanding, there are still challenges in generating long-form outputs.

--------

## Chunk 499

**Chunk:**

Dwarkesh Patel
On the production of a million tokens, there's no public evidence that there's some easy loss function where you can...
Leopold Aschenbrenner
GPT-4 has gotten a lot better since launch. The GPT-4 gains since launch are a huge indicator.

You talked about this with John Schulman on the podcast. John said this was mostly post-training gains. If you look at the LMSys scores, it's like 100 Elo or something. It's a bigger gap than between Claude 3 Opus and Claude 3 Haiku. The price difference between those is 60x.
Dwarkesh Patel
But it’s not more agentic. It's better in the same chat.

**Extracted Belief:**

GPT-4 has significantly improved since its initial release.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement that there is no public evidence of an easy loss function for producing a million tokens. Aschenbrenner points to GPT-4's post-launch improvements as an indication of progress.

**Justification:**

Aschenbrenner cites the substantial gains made by GPT-4 since its release as evidence of improvement. He further supports this by stating that these gains are a 'huge indicator' of progress.

--------

## Chunk 500

**Chunk:**

Dwarkesh Patel
On the production of a million tokens, there's no public evidence that there's some easy loss function where you can...
Leopold Aschenbrenner
GPT-4 has gotten a lot better since launch. The GPT-4 gains since launch are a huge indicator.

You talked about this with John Schulman on the podcast. John said this was mostly post-training gains. If you look at the LMSys scores, it's like 100 Elo or something. It's a bigger gap than between Claude 3 Opus and Claude 3 Haiku. The price difference between those is 60x.
Dwarkesh Patel
But it’s not more agentic. It's better in the same chat.

**Extracted Belief:**

The improvements in GPT-4 since its release are primarily due to post-training gains.

**Context:**

Leopold Aschenbrenner is referencing a conversation Dwarkesh Patel had with John Schulman on the podcast regarding GPT-4's improvements. 

**Justification:**

Aschenbrenner cites John Schulman's statement that the GPT-4 gains are mostly post-training gains, implying a high level of confidence in Schulman's expertise on the topic.

--------

## Chunk 501

**Chunk:**

Dwarkesh Patel
On the production of a million tokens, there's no public evidence that there's some easy loss function where you can...
Leopold Aschenbrenner
GPT-4 has gotten a lot better since launch. The GPT-4 gains since launch are a huge indicator.

You talked about this with John Schulman on the podcast. John said this was mostly post-training gains. If you look at the LMSys scores, it's like 100 Elo or something. It's a bigger gap than between Claude 3 Opus and Claude 3 Haiku. The price difference between those is 60x.
Dwarkesh Patel
But it’s not more agentic. It's better in the same chat.

**Extracted Belief:**

The improvement in GPT-4's performance, as indicated by the LMSys scores, is significant.

**Context:**

Leopold Aschenbrenner is illustrating the extent of GPT-4's improvement by comparing its LMSys score to the difference between Claude 3 Opus and Claude 3 Haiku.

**Justification:**

Aschenbrenner states that the improvement in GPT-4's LMSys score is 'like 100 Elo or something,' suggesting a significant gain in performance.

--------

## Chunk 502

**Chunk:**

Dwarkesh Patel
On the production of a million tokens, there's no public evidence that there's some easy loss function where you can...
Leopold Aschenbrenner
GPT-4 has gotten a lot better since launch. The GPT-4 gains since launch are a huge indicator.

You talked about this with John Schulman on the podcast. John said this was mostly post-training gains. If you look at the LMSys scores, it's like 100 Elo or something. It's a bigger gap than between Claude 3 Opus and Claude 3 Haiku. The price difference between those is 60x.
Dwarkesh Patel
But it’s not more agentic. It's better in the same chat.

**Extracted Belief:**

The difference in performance between Claude 3 Opus and Claude 3 Haiku is notable.

**Context:**

Leopold Aschenbrenner is using the difference in performance between Claude 3 Opus and Claude 3 Haiku as a point of comparison for GPT-4's improvement.

**Justification:**

Aschenbrenner states that the difference in performance between these models is 'a bigger gap' than the improvement seen in GPT-4.

--------

## Chunk 503

**Chunk:**

Dwarkesh Patel
On the production of a million tokens, there's no public evidence that there's some easy loss function where you can...
Leopold Aschenbrenner
GPT-4 has gotten a lot better since launch. The GPT-4 gains since launch are a huge indicator.

You talked about this with John Schulman on the podcast. John said this was mostly post-training gains. If you look at the LMSys scores, it's like 100 Elo or something. It's a bigger gap than between Claude 3 Opus and Claude 3 Haiku. The price difference between those is 60x.
Dwarkesh Patel
But it’s not more agentic. It's better in the same chat.

**Extracted Belief:**

The price difference between Claude 3 Opus and Claude 3 Haiku is substantial.

**Context:**

Leopold Aschenbrenner is highlighting the contrast between performance differences and price differences.

**Justification:**

Aschenbrenner states that the price difference between these models is '60x,' indicating a significant cost disparity between them.

--------

## Chunk 504

**Chunk:**

Dwarkesh Patel
But it’s not more agentic. It's better in the same chat.
Leopold Aschenbrenner
It’s  much better about math. It went from 40% to 70%. That indicates that clearly there's stuff to be done on hobbling. The interesting question is, this time a year from now, is there a model that is able to think for a few thousand tokens coherently, cohesively, identically? Again, I'd probably feel better if we had 1–2 OOMs more data because the scaling just gives you this tailwind.

With tools, when you talk to people who try to make things work with tools, GPT-4 is really when tools start to work. You can kind of make them work with GPT-3.5, but it's just really tough. Having GPT-4, you can help it learn tools in a much easier way. So it’d be great to have just a bit more tailwind from scaling. I don't know if it'll work, but it's a key question.
Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?

**Extracted Belief:**

GPT-4's performance on mathematical tasks has improved significantly since its launch, increasing from 40% accuracy to 70% accuracy.

**Context:**

Leopold Aschenbrenner is discussing the improvements in GPT-4's abilities, specifically its performance on mathematical problems, as an example of the potential for further progress in AI.

**Justification:**

Leopold Aschenbrenner states that GPT-4's mathematical accuracy increased from 40% to 70%, which implies a significant improvement based on observable data.

--------

## Chunk 505

**Chunk:**

Dwarkesh Patel
But it’s not more agentic. It's better in the same chat.
Leopold Aschenbrenner
It’s  much better about math. It went from 40% to 70%. That indicates that clearly there's stuff to be done on hobbling. The interesting question is, this time a year from now, is there a model that is able to think for a few thousand tokens coherently, cohesively, identically? Again, I'd probably feel better if we had 1–2 OOMs more data because the scaling just gives you this tailwind.

With tools, when you talk to people who try to make things work with tools, GPT-4 is really when tools start to work. You can kind of make them work with GPT-3.5, but it's just really tough. Having GPT-4, you can help it learn tools in a much easier way. So it’d be great to have just a bit more tailwind from scaling. I don't know if it'll work, but it's a key question.
Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?

**Extracted Belief:**

The current limitations of large language models, such as their difficulty in producing coherent text for longer outputs, can be addressed by providing more data and scaling them.

**Context:**

Leopold Aschenbrenner is discussing the potential for future advancements in AI by improving the scale and quality of training data.

**Justification:**

Leopold Aschenbrenner states that 'the scaling just gives you this tailwind' and that having more data would be beneficial. This suggests a belief that increasing the scale of training data will lead to improvements in AI capabilities.

--------

## Chunk 506

**Chunk:**

Dwarkesh Patel
But it’s not more agentic. It's better in the same chat.
Leopold Aschenbrenner
It’s  much better about math. It went from 40% to 70%. That indicates that clearly there's stuff to be done on hobbling. The interesting question is, this time a year from now, is there a model that is able to think for a few thousand tokens coherently, cohesively, identically? Again, I'd probably feel better if we had 1–2 OOMs more data because the scaling just gives you this tailwind.

With tools, when you talk to people who try to make things work with tools, GPT-4 is really when tools start to work. You can kind of make them work with GPT-3.5, but it's just really tough. Having GPT-4, you can help it learn tools in a much easier way. So it’d be great to have just a bit more tailwind from scaling. I don't know if it'll work, but it's a key question.
Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?

**Extracted Belief:**

GPT-4 is a significant improvement over GPT-3.5 in its ability to learn and utilize tools effectively.

**Context:**

Leopold Aschenbrenner is discussing the differences in tool-learning capabilities between GPT-3.5 and GPT-4.

**Justification:**

Aschenbrenner states that 'GPT-4 is really when tools start to work', implying that GPT-4's ability to learn and utilize tools is superior to GPT-3.5 based on his experience.

--------

## Chunk 507

**Chunk:**

Dwarkesh Patel
But it’s not more agentic. It's better in the same chat.
Leopold Aschenbrenner
It’s  much better about math. It went from 40% to 70%. That indicates that clearly there's stuff to be done on hobbling. The interesting question is, this time a year from now, is there a model that is able to think for a few thousand tokens coherently, cohesively, identically? Again, I'd probably feel better if we had 1–2 OOMs more data because the scaling just gives you this tailwind.

With tools, when you talk to people who try to make things work with tools, GPT-4 is really when tools start to work. You can kind of make them work with GPT-3.5, but it's just really tough. Having GPT-4, you can help it learn tools in a much easier way. So it’d be great to have just a bit more tailwind from scaling. I don't know if it'll work, but it's a key question.
Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?

**Extracted Belief:**

It is possible to achieve significant advancements in AI by dedicating sufficient computational resources to training larger models.

**Context:**

Leopold Aschenbrenner is discussing the potential for rapid progress in AI by leveraging increased computational power for training.

**Justification:**

Aschenbrenner suggests that giving 'Alec Radford a compute budget' could lead to significant advancements in AI, implying that more computing power can accelerate progress.

--------

## Chunk 508

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

A decade's worth of progress in artificial intelligence is possible within a year.

**Context:**

Leopold Aschenbrenner is responding to a question about whether substantial progress in artificial general intelligence (AGI) can be achieved in a short timeframe.

**Justification:**

He acknowledges that the timeframe is being exaggerated but believes a decade's worth of progress within a year is a reasonable goal.

--------

## Chunk 509

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

Automated AI research can significantly accelerate progress in AI development.

**Context:**

Leopold Aschenbrenner is explaining how the concept of automated AI research, where AI systems are used to research and improve other AI systems, can lead to rapid progress.

**Justification:**

He suggests that this automation would provide "an enormous tailwind on all the other stuff." This implies that automated AI research would dramatically improve the efficiency and effectiveness of AI development.

--------

## Chunk 510

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

AI advancements can lead to solving robotics challenges.

**Context:**

Leopold Aschenbrenner is discussing the potential applications of advanced AI systems in various fields, including robotics.

**Justification:**

He states that "You're solving robotics" after describing the capabilities of highly advanced AI systems. This suggests that he believes AI has the potential to overcome current challenges in robotics.

--------

## Chunk 511

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

Simulations can be a valuable tool for AI research and development.

**Context:**

Leopold Aschenbrenner is highlighting the potential of simulations in advancing AI research.

**Justification:**

He mentions "Maybe you can do a lot in simulation" and cites the use of "digital twins" for simulating manufacturing processes as an example. This suggests he believes simulations can significantly accelerate AI development.

--------

## Chunk 512

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

AI advancements will likely lead to the development of robots capable of performing various tasks.

**Context:**

Leopold Aschenbrenner is predicting future outcomes of AI progress.

**Justification:**

He states "You're just going to get the robots" and mentions the potential for "crazy drone swarms." This indicates his belief that AI will lead to the development of advanced robots.

--------

## Chunk 513

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

Regulation will likely restrict the deployment of AI in certain sectors, such as law and medicine.

**Context:**

Leopold Aschenbrenner is discussing the potential societal impact of advanced AI systems.

**Justification:**

He acknowledges that "lawyers and doctors still need to be humans because of regulation." This indicates that he believes regulatory hurdles will likely limit the full integration of AI in certain professional fields.

--------

## Chunk 514

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

Competition will drive the adoption of AI in areas such as national security.

**Context:**

Leopold Aschenbrenner is discussing the motivations for AI deployment.

**Justification:**

He states that "because of these competitive pressures, we will have to let them loose to some degree on various national security applications." This implies that competitive pressures will likely drive the adoption of AI in sensitive areas.

--------

## Chunk 515

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

AI advancements will lead to a rapid acceleration in the growth of technology.

**Context:**

Leopold Aschenbrenner is outlining the potential impact of advanced AI on technological advancement.

**Justification:**

He describes an "explosion" in technology growth, stating that "The A in the production function, the growth of technology, has massively accelerated." This indicates his belief that AI will trigger a significant increase in technological development.

--------

## Chunk 516

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

AI advancements will lead to the creation of a large workforce of superintelligent scientists, engineers, and technicians.

**Context:**

Leopold Aschenbrenner is painting a picture of the potential future workforce.

**Justification:**

He describes a scenario where "you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything." This suggests he believes AI will create a vast workforce with exceptional capabilities.

--------

## Chunk 517

**Chunk:**

Dwarkesh Patel
It's a good place to sort of close that part where we know what the crux is and what evidence of that would look like.

Let’s talk about AGI to superintelligence. Maybe it's the case that the gains are really easy right now and you can just sort of let loose. Give Alec Radford a compute budget and he’ll comes out the other end with something that is an additive change as part of the code.

How many other domains in the world are like this, where you think you could get the equivalent of in one year? You just throw enough intelligence across multiple instances and you come out the other end with something that is remarkably decades, centuries ahead? You start off with no flight, and then you have the Wright brothers. You have a million instances of GPT-6, and you come out the other end with Starlink? Is that your model of how things work?
Leopold Aschenbrenner
You're exaggerating the timelines a little bit, but I think a decade's worth of progress in a year or something is a reasonable prompt. This is where the automated AI researcher comes in. It gives you this enormous tailwind on all the other stuff.

You automate AI research with your automated Alec Radfords. You come out the other end. You've done another five OOMs. You have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it good at everything else. You're solving robotics.

The robots are important because for a lot of other things, you do actually need to try things in the physical world. Maybe you can do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last Nvidia GTC and it was all about the digital twins having all your manufacturing processes in simulation. Again, if you have these superintelligent cognitive workers, can they just make simulations of everything, off the float style, and make a lot of progress there?

You're just going to get the robots. I agree there are a lot of real-world bottlenecks. It's quite possible that we're going to have crazy drone swarms, but also lawyers and doctors still need to be humans because of regulation. You kind of start narrowly, you broaden, and there are worlds in which you let them loose. Again, because of these competitive pressures, we will have to let them loose to some degree on various national security applications. Rapid progress is quite possible.

In the explosion after, there are basically two components. The A in the production function, the growth of technology, has massively accelerated. Now you have a billion superintelligent scientists and engineers and technicians, superbly competent at everything.

You also just automated labor. Even without the whole technological explosion thing, you have this industrial explosion, at least if you let them loose, because you can cover Nevada and you start with one robot factory producing more robots. It's this cumulative process because you've taken labor out of the equation.
Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.

**Extracted Belief:**

AI advancements can automate labor and lead to a significant industrial expansion.

**Context:**

Leopold Aschenbrenner is discussing the potential economic impact of AI.

**Justification:**

He states that AI will lead to "automated labor" and an "industrial explosion." This suggests that he believes AI will significantly impact the nature of work and lead to large-scale industrial growth.

--------

## Chunk 518

**Chunk:**

Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.
Leopold Aschenbrenner
They throw out these crazy cities in like a decade.
Dwarkesh Patel
People talk about 30% growth rates from AI. The closest thing—

**Extracted Belief:**

China and the Soviet Union achieved rapid economic growth by increasing capital (K) and labor (L) inputs without corresponding increases in technological progress (A).

**Context:**

Leopold Aschenbrenner is contrasting the economic growth strategies of China and the Soviet Union with potential future economic growth driven by advancements in artificial intelligence.

**Justification:**

Leopold Aschenbrenner cites the examples of China and the Soviet Union as demonstrating rapid economic growth through increased capital and labor inputs.

--------

## Chunk 519

**Chunk:**

Dwarkesh Patel
That's super interesting.

Although when you increase the K or the L without increasing the A, you can look at examples like the Soviet Union or China. They rapidly increased inputs, which did have a geopolitically game-changing effect. It is remarkable to see the transformation of cities like Shanghai over just decades.
Leopold Aschenbrenner
They throw out these crazy cities in like a decade.
Dwarkesh Patel
People talk about 30% growth rates from AI. The closest thing—

**Extracted Belief:**

Cities like Shanghai can undergo significant transformation within a decade.

**Context:**

Leopold Aschenbrenner is acknowledging the rapid pace of urban development in China, particularly in cities like Shanghai.

**Justification:**

He states that China 'throws out these crazy cities in like a decade,' implying that major urban changes can happen quickly.

--------

## Chunk 520

**Chunk:**

Dwarkesh Patel
People talk about 30% growth rates from AI. The closest thing—
Leopold Aschenbrenner
Look at the Asian Tigers at 10%. It's totally possible.
Dwarkesh Patel
But without productivity gains, it's not like the Industrial Revolution. From a perspective of outside the system, your goods become much cheaper and you can manufacture more things. But it's not a sign that the next century is rapidly approaching.

**Extracted Belief:**

Economic growth rates of 10% are possible with the advent of artificial intelligence.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's suggestion of 30% growth rates from AI, and offers the example of the Asian Tigers, who experienced 10% growth rates, as evidence that such rates are achievable.

**Justification:**

The Asian Tigers (Hong Kong, Singapore, South Korea, and Taiwan) achieved economic growth rates of 10% during their economic booms, demonstrating that such growth rates are possible.

--------

## Chunk 521

**Chunk:**

Dwarkesh Patel
But without productivity gains, it's not like the Industrial Revolution. From a perspective of outside the system, your goods become much cheaper and you can manufacture more things. But it's not a sign that the next century is rapidly approaching.
Leopold Aschenbrenner
Both are important. The other thing I'll say is that with all of this stuff, the magnitudes are really, really important. We talked about a 10x in research effort, or maybe 10-30x over a decade. Even without any kind of self-improvement type loops — even in the sort of GPT-4 to AGI story — we're talking about an OOM of effective compute increase a year.

It’s half an OOM of compute, half an OOM of algorithmic progress that sort of translates into effective compute. You're basically doing 10x a year on your labor force. It's a radically different world if you're doing a 10x or 30x in a century versus a 10x a year on your labor force. The magnitudes really matter.

It also really matters in the intelligence explosion scenario, just the automated AI research part. One story you could tell there is that ideas get harder to find. Algorithmic progress is going to get harder. Right now, you have the easy wins, but in like four or five years, there will be fewer easy wins. So the sort of automated AI researchers are going to be necessary to just keep it going, because it's gotten harder. That's sort of a really weird knife-edge assumption economics.
Dwarkesh Patel
Isn't that the equilibrium story you were just telling about why the economy as a whole has 2% economic growth? You just proceed on the equilibrium. I guess you're saying by the time—

**Extracted Belief:**

The magnitude of change is crucial in evaluating the impact of technological advancements, particularly in the context of artificial intelligence.

**Context:**

Leopold Aschenbrenner is discussing the potential impact of artificial intelligence, particularly the significance of computational power and algorithmic progress in driving rapid advancements.

**Justification:**

He emphasizes that a 10x increase in research effort or compute power over a decade is significantly different from a 10x increase per year, highlighting the importance of considering the scale of change.

--------

## Chunk 522

**Chunk:**

Dwarkesh Patel
But without productivity gains, it's not like the Industrial Revolution. From a perspective of outside the system, your goods become much cheaper and you can manufacture more things. But it's not a sign that the next century is rapidly approaching.
Leopold Aschenbrenner
Both are important. The other thing I'll say is that with all of this stuff, the magnitudes are really, really important. We talked about a 10x in research effort, or maybe 10-30x over a decade. Even without any kind of self-improvement type loops — even in the sort of GPT-4 to AGI story — we're talking about an OOM of effective compute increase a year.

It’s half an OOM of compute, half an OOM of algorithmic progress that sort of translates into effective compute. You're basically doing 10x a year on your labor force. It's a radically different world if you're doing a 10x or 30x in a century versus a 10x a year on your labor force. The magnitudes really matter.

It also really matters in the intelligence explosion scenario, just the automated AI research part. One story you could tell there is that ideas get harder to find. Algorithmic progress is going to get harder. Right now, you have the easy wins, but in like four or five years, there will be fewer easy wins. So the sort of automated AI researchers are going to be necessary to just keep it going, because it's gotten harder. That's sort of a really weird knife-edge assumption economics.
Dwarkesh Patel
Isn't that the equilibrium story you were just telling about why the economy as a whole has 2% economic growth? You just proceed on the equilibrium. I guess you're saying by the time—

**Extracted Belief:**

The pace of algorithmic progress in artificial intelligence will slow down over time as the easiest advancements are made first.

**Context:**

Aschenbrenner describes a scenario where the initial rapid progress of AI, characterized by 'easy wins,' will eventually lead to a need for automated AI researchers to sustain the progress.

**Justification:**

He suggests that finding new ideas in AI will become increasingly challenging as the field matures, drawing on the assumption that the easiest advancements happen first.

--------

## Chunk 523

**Chunk:**

Dwarkesh Patel
Isn't that the equilibrium story you were just telling about why the economy as a whole has 2% economic growth? You just proceed on the equilibrium. I guess you're saying by the time—
Leopold Aschenbrenner
The result of the equilibrium here is that it’s way faster. AIt depends on the sort of exponents. Suppose you need to 10x the effective research effort in AI research in the last four or five years to keep the pace of progress. We're not just getting a 10x, you're getting 1,000,000x or 100,000x. The magnitudes really matter.

One way to think about this is that you have two exponentials. You have your normal economy that's growing at 2% a year, and you have your AI economy growing at 10x a year. It's starting out really small. It's way faster and it's going to overtake eventually. You can just do the simple revenue extrapolation if you think your AI economy has some growth rate. It's a very simplistic way, but there's this 10x a year process.

You're going to transition the whole economy, as it broadens, from the 2% a year to the much faster growing process. That's very consistent with historical stories. There's this long-run hyperbolic trend. It manifested in the change in growth mode in the Industrial Revolution, but there's just this long-run hyperbolic trend. Now you have another change in growth mode.
Dwarkesh Patel
That was one of the questions I asked Tyler when I had him on the podcast. The fact that, after 1776, you went from a regime of negligible economic growth to 2% is really interesting. From the perspective of somebody in the Middle Ages or before, 2% is the equivalent of like 10%. I guess you're projecting even higher for the AI economy.

**Extracted Belief:**

The rate of progress in AI research will continue to increase exponentially over the next few years.

**Context:**

Leopold Aschenbrenner was explaining how the rate of progress in AI research will continue to accelerate exponentially, despite the potential for diminishing returns as research becomes more complex.

**Justification:**

This belief is based on Leopold Aschenbrenner's personal assessment and expertise in the field of AI research.

--------

## Chunk 524

**Chunk:**

Dwarkesh Patel
Isn't that the equilibrium story you were just telling about why the economy as a whole has 2% economic growth? You just proceed on the equilibrium. I guess you're saying by the time—
Leopold Aschenbrenner
The result of the equilibrium here is that it’s way faster. AIt depends on the sort of exponents. Suppose you need to 10x the effective research effort in AI research in the last four or five years to keep the pace of progress. We're not just getting a 10x, you're getting 1,000,000x or 100,000x. The magnitudes really matter.

One way to think about this is that you have two exponentials. You have your normal economy that's growing at 2% a year, and you have your AI economy growing at 10x a year. It's starting out really small. It's way faster and it's going to overtake eventually. You can just do the simple revenue extrapolation if you think your AI economy has some growth rate. It's a very simplistic way, but there's this 10x a year process.

You're going to transition the whole economy, as it broadens, from the 2% a year to the much faster growing process. That's very consistent with historical stories. There's this long-run hyperbolic trend. It manifested in the change in growth mode in the Industrial Revolution, but there's just this long-run hyperbolic trend. Now you have another change in growth mode.
Dwarkesh Patel
That was one of the questions I asked Tyler when I had him on the podcast. The fact that, after 1776, you went from a regime of negligible economic growth to 2% is really interesting. From the perspective of somebody in the Middle Ages or before, 2% is the equivalent of like 10%. I guess you're projecting even higher for the AI economy.

**Extracted Belief:**

The AI economy will grow at a rate of 10x per year.

**Context:**

Leopold Aschenbrenner was providing an example of the potential for exponential growth in the AI economy.

**Justification:**

This is based on Leopold Aschenbrenner's personal expertise and observations of the AI industry.

--------

## Chunk 525

**Chunk:**

Dwarkesh Patel
Isn't that the equilibrium story you were just telling about why the economy as a whole has 2% economic growth? You just proceed on the equilibrium. I guess you're saying by the time—
Leopold Aschenbrenner
The result of the equilibrium here is that it’s way faster. AIt depends on the sort of exponents. Suppose you need to 10x the effective research effort in AI research in the last four or five years to keep the pace of progress. We're not just getting a 10x, you're getting 1,000,000x or 100,000x. The magnitudes really matter.

One way to think about this is that you have two exponentials. You have your normal economy that's growing at 2% a year, and you have your AI economy growing at 10x a year. It's starting out really small. It's way faster and it's going to overtake eventually. You can just do the simple revenue extrapolation if you think your AI economy has some growth rate. It's a very simplistic way, but there's this 10x a year process.

You're going to transition the whole economy, as it broadens, from the 2% a year to the much faster growing process. That's very consistent with historical stories. There's this long-run hyperbolic trend. It manifested in the change in growth mode in the Industrial Revolution, but there's just this long-run hyperbolic trend. Now you have another change in growth mode.
Dwarkesh Patel
That was one of the questions I asked Tyler when I had him on the podcast. The fact that, after 1776, you went from a regime of negligible economic growth to 2% is really interesting. From the perspective of somebody in the Middle Ages or before, 2% is the equivalent of like 10%. I guess you're projecting even higher for the AI economy.

**Extracted Belief:**

The AI economy will eventually surpass the traditional economy in size.

**Context:**

Leopold Aschenbrenner was explaining how the exponential growth of the AI economy will eventually lead it to overtake the traditional economy.

**Justification:**

This belief is based on Leopold Aschenbrenner's projection of the AI economy's growth rate compared to the traditional economy's growth rate.

--------

## Chunk 526

**Chunk:**

Dwarkesh Patel
Isn't that the equilibrium story you were just telling about why the economy as a whole has 2% economic growth? You just proceed on the equilibrium. I guess you're saying by the time—
Leopold Aschenbrenner
The result of the equilibrium here is that it’s way faster. AIt depends on the sort of exponents. Suppose you need to 10x the effective research effort in AI research in the last four or five years to keep the pace of progress. We're not just getting a 10x, you're getting 1,000,000x or 100,000x. The magnitudes really matter.

One way to think about this is that you have two exponentials. You have your normal economy that's growing at 2% a year, and you have your AI economy growing at 10x a year. It's starting out really small. It's way faster and it's going to overtake eventually. You can just do the simple revenue extrapolation if you think your AI economy has some growth rate. It's a very simplistic way, but there's this 10x a year process.

You're going to transition the whole economy, as it broadens, from the 2% a year to the much faster growing process. That's very consistent with historical stories. There's this long-run hyperbolic trend. It manifested in the change in growth mode in the Industrial Revolution, but there's just this long-run hyperbolic trend. Now you have another change in growth mode.
Dwarkesh Patel
That was one of the questions I asked Tyler when I had him on the podcast. The fact that, after 1776, you went from a regime of negligible economic growth to 2% is really interesting. From the perspective of somebody in the Middle Ages or before, 2% is the equivalent of like 10%. I guess you're projecting even higher for the AI economy.

**Extracted Belief:**

The transition from the traditional economy to the AI economy will be similar to the transition from the pre-industrial era to the Industrial Revolution.

**Context:**

Leopold Aschenbrenner was drawing a parallel between the transition to the AI economy and the historical transition to the Industrial Revolution.

**Justification:**

This belief is based on Leopold Aschenbrenner's understanding of historical economic trends and his personal view of the AI revolution.

--------

## Chunk 527

**Chunk:**

Dwarkesh Patel
Isn't that the equilibrium story you were just telling about why the economy as a whole has 2% economic growth? You just proceed on the equilibrium. I guess you're saying by the time—
Leopold Aschenbrenner
The result of the equilibrium here is that it’s way faster. AIt depends on the sort of exponents. Suppose you need to 10x the effective research effort in AI research in the last four or five years to keep the pace of progress. We're not just getting a 10x, you're getting 1,000,000x or 100,000x. The magnitudes really matter.

One way to think about this is that you have two exponentials. You have your normal economy that's growing at 2% a year, and you have your AI economy growing at 10x a year. It's starting out really small. It's way faster and it's going to overtake eventually. You can just do the simple revenue extrapolation if you think your AI economy has some growth rate. It's a very simplistic way, but there's this 10x a year process.

You're going to transition the whole economy, as it broadens, from the 2% a year to the much faster growing process. That's very consistent with historical stories. There's this long-run hyperbolic trend. It manifested in the change in growth mode in the Industrial Revolution, but there's just this long-run hyperbolic trend. Now you have another change in growth mode.
Dwarkesh Patel
That was one of the questions I asked Tyler when I had him on the podcast. The fact that, after 1776, you went from a regime of negligible economic growth to 2% is really interesting. From the perspective of somebody in the Middle Ages or before, 2% is the equivalent of like 10%. I guess you're projecting even higher for the AI economy.

**Extracted Belief:**

The growth rate of the economy has historically followed a long-run hyperbolic trend.

**Context:**

Leopold Aschenbrenner was explaining his belief that the AI economy will represent another shift in this long-run hyperbolic trend of economic growth.

**Justification:**

This belief is based on Leopold Aschenbrenner's understanding of historical economic data and trends.

--------

## Chunk 528

**Chunk:**

Dwarkesh Patel
That was one of the questions I asked Tyler when I had him on the podcast. The fact that, after 1776, you went from a regime of negligible economic growth to 2% is really interesting. From the perspective of somebody in the Middle Ages or before, 2% is the equivalent of like 10%. I guess you're projecting even higher for the AI economy.
Leopold Aschenbrenner
It depends. Again, with all this stuff I have a lot of uncertainty. A lot of the time I'm trying to tell the modal story because it's important to be concrete and visceral about it.

I have a lot of uncertainty over how the 2030s play out. The thing I know is that it's going to be fucking crazy. As for exactly where the bottlenecks are and so on…
Dwarkesh Patel
Let's talk through the numbers here. You mentioned hundreds of millions of AI researchers. Right now, GPT-4o is like $15 for a million tokens outputted. A human thinks at 150 tokens a minute or something. If you do the math on that, for an hour's worth of human output, it's like $0.10 or something.

**Extracted Belief:**

Leopold Aschenbrenner has a lot of uncertainty about how the 2030s will play out.

**Context:**

Leopold Aschenbrenner was discussing the rapid pace of progress in AI and its potential impact on the economy, specifically in the context of the 2030s.

**Justification:**

He explicitly stated that he has a lot of uncertainty over how the 2030s will play out.

--------

## Chunk 529

**Chunk:**

Dwarkesh Patel
That was one of the questions I asked Tyler when I had him on the podcast. The fact that, after 1776, you went from a regime of negligible economic growth to 2% is really interesting. From the perspective of somebody in the Middle Ages or before, 2% is the equivalent of like 10%. I guess you're projecting even higher for the AI economy.
Leopold Aschenbrenner
It depends. Again, with all this stuff I have a lot of uncertainty. A lot of the time I'm trying to tell the modal story because it's important to be concrete and visceral about it.

I have a lot of uncertainty over how the 2030s play out. The thing I know is that it's going to be fucking crazy. As for exactly where the bottlenecks are and so on…
Dwarkesh Patel
Let's talk through the numbers here. You mentioned hundreds of millions of AI researchers. Right now, GPT-4o is like $15 for a million tokens outputted. A human thinks at 150 tokens a minute or something. If you do the math on that, for an hour's worth of human output, it's like $0.10 or something.

**Extracted Belief:**

The 2030s will be a period of significant change and disruption.

**Context:**

Leopold Aschenbrenner was discussing his uncertainty about the 2030s but also recognized that the period will be characterized by significant change.

**Justification:**

He stated that 'The thing I know is that it's going to be fucking crazy' when discussing the 2030s.

--------

## Chunk 530

**Chunk:**

Dwarkesh Patel
Let's talk through the numbers here. You mentioned hundreds of millions of AI researchers. Right now, GPT-4o is like $15 for a million tokens outputted. A human thinks at 150 tokens a minute or something. If you do the math on that, for an hour's worth of human output, it's like $0.10 or something.
Leopold Aschenbrenner
It's cheaper than a human worker. It can't do the job yet.
Dwarkesh Patel
That's right. But by the time you're talking about models that are trained on the 10 GW cluster, then you have something that is four OOMs more expensive via inference, something like three OOMs. That's like $100/hour of labor. Now you're having hundreds of millions of such laborers. Is there enough compute to do this kind of labor with the model that is 1000 times bigger?

**Extracted Belief:**

Current AI models, such as GPT-4, are cheaper to use than human workers for tasks involving outputting tokens, even though they are not yet capable of performing the same jobs as humans.

**Context:**

Leopold Aschenbrenner was discussing the potential for AI to replace human workers in the future, and he used the example of GPT-4 to illustrate the current cost comparison.

**Justification:**

Leopold Aschenbrenner explicitly states that GPT-4 is cheaper than a human worker for outputting tokens, but also acknowledges that it 'can't do the job yet'.

--------

## Chunk 531

**Chunk:**

Dwarkesh Patel
That's right. But by the time you're talking about models that are trained on the 10 GW cluster, then you have something that is four OOMs more expensive via inference, something like three OOMs. That's like $100/hour of labor. Now you're having hundreds of millions of such laborers. Is there enough compute to do this kind of labor with the model that is 1000 times bigger?
Leopold Aschenbrenner
Great question. I actually don't think inference costs for frontier models are necessarily going to go up that much.
Dwarkesh Patel
But isn't the test time sort of thing that it will go up even higher?

**Extracted Belief:**

Inference costs for frontier AI models will not necessarily increase significantly.

**Context:**

Leopold Aschenbrenner expresses his belief about the future cost of inference for advanced AI models in response to Dwarkesh Patel's query about the scalability of computing power for a large number of AI researchers.

**Justification:**

The belief is based on the historical trend of stable inference costs despite significant increases in AI model capabilities, exemplified by the cost of GPT-3 versus GPT-4, coupled with the theoretical grounding of Chinchilla scaling laws which suggest that improvements in algorithmic efficiency can compensate for increased model size.

--------

## Chunk 532

**Chunk:**

Dwarkesh Patel
But isn't the test time sort of thing that it will go up even higher?
Leopold Aschenbrenner
We're just doing per token. Suppose each model token was the same as a human token thing at 100 tokens a minute. It'll use more, but the token calculation is already pricing that in. The question is per token pricing. GPT-3 when it launched was actually more expensive than GPT-4 now. Over vast increases in capability gains, inference cost has remained constant. That's sort of wild, and it's worth appreciating. It gestures at an underlying pace of algorithmic progress.

There's a more theoretically grounded way to explain why inference costs would stay constant. On Chinchilla scaling laws, half of the additional compute you allocate to bigger models and half of it you allocate to more data. If we go with the basic story of 0.5 OOM/year more compute and 0.5 OOM/year of algorithmic progress you're saving 0.5 OOM/year. That would exactly compensate for making the model bigger.

The caveat is that obviously not all training efficiencies are also inference efficiencies. A bunch of the time they are. Separately, you can find inference efficiencies. Given this historical trend and baseline theoretical reason, it's not a crazy baseline assumption that the frontier models are not necessarily going to get more expensive, per token.
Dwarkesh Patel
Really? Okay, that's wild.

**Extracted Belief:**

GPT-3 was more expensive than GPT-4 at launch, despite vast improvements in GPT-4's capabilities.

**Context:**

Leopold Aschenbrenner is discussing the cost of inference for large language models, and is using the example of GPT-3 and GPT-4 to illustrate his point.

**Justification:**

Leopold Aschenbrenner states that GPT-3, when it launched, was more expensive than GPT-4 is now. This is based on his direct observation of pricing trends for language models.

--------

## Chunk 533

**Chunk:**

Dwarkesh Patel
But isn't the test time sort of thing that it will go up even higher?
Leopold Aschenbrenner
We're just doing per token. Suppose each model token was the same as a human token thing at 100 tokens a minute. It'll use more, but the token calculation is already pricing that in. The question is per token pricing. GPT-3 when it launched was actually more expensive than GPT-4 now. Over vast increases in capability gains, inference cost has remained constant. That's sort of wild, and it's worth appreciating. It gestures at an underlying pace of algorithmic progress.

There's a more theoretically grounded way to explain why inference costs would stay constant. On Chinchilla scaling laws, half of the additional compute you allocate to bigger models and half of it you allocate to more data. If we go with the basic story of 0.5 OOM/year more compute and 0.5 OOM/year of algorithmic progress you're saving 0.5 OOM/year. That would exactly compensate for making the model bigger.

The caveat is that obviously not all training efficiencies are also inference efficiencies. A bunch of the time they are. Separately, you can find inference efficiencies. Given this historical trend and baseline theoretical reason, it's not a crazy baseline assumption that the frontier models are not necessarily going to get more expensive, per token.
Dwarkesh Patel
Really? Okay, that's wild.

**Extracted Belief:**

Inference costs for large language models have remained relatively constant despite significant improvements in capability.

**Context:**

Leopold Aschenbrenner is explaining that while the size and complexity of language models are increasing, the cost of running inference on them has remained stable.

**Justification:**

He states that inference costs have remained constant despite 'vast increases in capability gains,' which is based on his observation of the industry.

--------

## Chunk 534

**Chunk:**

Dwarkesh Patel
But isn't the test time sort of thing that it will go up even higher?
Leopold Aschenbrenner
We're just doing per token. Suppose each model token was the same as a human token thing at 100 tokens a minute. It'll use more, but the token calculation is already pricing that in. The question is per token pricing. GPT-3 when it launched was actually more expensive than GPT-4 now. Over vast increases in capability gains, inference cost has remained constant. That's sort of wild, and it's worth appreciating. It gestures at an underlying pace of algorithmic progress.

There's a more theoretically grounded way to explain why inference costs would stay constant. On Chinchilla scaling laws, half of the additional compute you allocate to bigger models and half of it you allocate to more data. If we go with the basic story of 0.5 OOM/year more compute and 0.5 OOM/year of algorithmic progress you're saving 0.5 OOM/year. That would exactly compensate for making the model bigger.

The caveat is that obviously not all training efficiencies are also inference efficiencies. A bunch of the time they are. Separately, you can find inference efficiencies. Given this historical trend and baseline theoretical reason, it's not a crazy baseline assumption that the frontier models are not necessarily going to get more expensive, per token.
Dwarkesh Patel
Really? Okay, that's wild.

**Extracted Belief:**

Algorithmic progress is driving the stability of inference costs despite model growth.

**Context:**

Leopold Aschenbrenner is explaining the counterintuitive observation of stable inference costs despite growing models.

**Justification:**

He claims that the stability of inference cost is 'gesturing at an underlying pace of algorithmic progress.' This is based on his understanding of the field and the relationship between algorithms and model performance.

--------

## Chunk 535

**Chunk:**

Dwarkesh Patel
But isn't the test time sort of thing that it will go up even higher?
Leopold Aschenbrenner
We're just doing per token. Suppose each model token was the same as a human token thing at 100 tokens a minute. It'll use more, but the token calculation is already pricing that in. The question is per token pricing. GPT-3 when it launched was actually more expensive than GPT-4 now. Over vast increases in capability gains, inference cost has remained constant. That's sort of wild, and it's worth appreciating. It gestures at an underlying pace of algorithmic progress.

There's a more theoretically grounded way to explain why inference costs would stay constant. On Chinchilla scaling laws, half of the additional compute you allocate to bigger models and half of it you allocate to more data. If we go with the basic story of 0.5 OOM/year more compute and 0.5 OOM/year of algorithmic progress you're saving 0.5 OOM/year. That would exactly compensate for making the model bigger.

The caveat is that obviously not all training efficiencies are also inference efficiencies. A bunch of the time they are. Separately, you can find inference efficiencies. Given this historical trend and baseline theoretical reason, it's not a crazy baseline assumption that the frontier models are not necessarily going to get more expensive, per token.
Dwarkesh Patel
Really? Okay, that's wild.

**Extracted Belief:**

Chinchilla scaling laws suggest that half of the additional compute for larger models is allocated to model size and half to data.

**Context:**

Leopold Aschenbrenner is explaining a theoretical justification for stable inference costs.

**Justification:**

He references Chinchilla scaling laws, which are a well-known theory in the field of large language models. This is based on his understanding of the research in the field.

--------

## Chunk 536

**Chunk:**

Dwarkesh Patel
But isn't the test time sort of thing that it will go up even higher?
Leopold Aschenbrenner
We're just doing per token. Suppose each model token was the same as a human token thing at 100 tokens a minute. It'll use more, but the token calculation is already pricing that in. The question is per token pricing. GPT-3 when it launched was actually more expensive than GPT-4 now. Over vast increases in capability gains, inference cost has remained constant. That's sort of wild, and it's worth appreciating. It gestures at an underlying pace of algorithmic progress.

There's a more theoretically grounded way to explain why inference costs would stay constant. On Chinchilla scaling laws, half of the additional compute you allocate to bigger models and half of it you allocate to more data. If we go with the basic story of 0.5 OOM/year more compute and 0.5 OOM/year of algorithmic progress you're saving 0.5 OOM/year. That would exactly compensate for making the model bigger.

The caveat is that obviously not all training efficiencies are also inference efficiencies. A bunch of the time they are. Separately, you can find inference efficiencies. Given this historical trend and baseline theoretical reason, it's not a crazy baseline assumption that the frontier models are not necessarily going to get more expensive, per token.
Dwarkesh Patel
Really? Okay, that's wild.

**Extracted Belief:**

Algorithmic progress can offset the increased compute required for larger language models.

**Context:**

Leopold Aschenbrenner is using Chinchilla scaling laws to argue that the increase in compute needed for larger models can be balanced by algorithmic advancements.

**Justification:**

He explains that if compute increases by 0.5 OOM per year, but algorithmic progress also increases by 0.5 OOM per year, then the gains from algorithmic progress would offset the cost increase from larger models.

--------

## Chunk 537

**Chunk:**

Dwarkesh Patel
But isn't the test time sort of thing that it will go up even higher?
Leopold Aschenbrenner
We're just doing per token. Suppose each model token was the same as a human token thing at 100 tokens a minute. It'll use more, but the token calculation is already pricing that in. The question is per token pricing. GPT-3 when it launched was actually more expensive than GPT-4 now. Over vast increases in capability gains, inference cost has remained constant. That's sort of wild, and it's worth appreciating. It gestures at an underlying pace of algorithmic progress.

There's a more theoretically grounded way to explain why inference costs would stay constant. On Chinchilla scaling laws, half of the additional compute you allocate to bigger models and half of it you allocate to more data. If we go with the basic story of 0.5 OOM/year more compute and 0.5 OOM/year of algorithmic progress you're saving 0.5 OOM/year. That would exactly compensate for making the model bigger.

The caveat is that obviously not all training efficiencies are also inference efficiencies. A bunch of the time they are. Separately, you can find inference efficiencies. Given this historical trend and baseline theoretical reason, it's not a crazy baseline assumption that the frontier models are not necessarily going to get more expensive, per token.
Dwarkesh Patel
Really? Okay, that's wild.

**Extracted Belief:**

Not all training efficiencies translate to inference efficiencies, but many do.

**Context:**

Leopold Aschenbrenner is acknowledging that while there are efficiencies in training, they may not always translate to inference.

**Justification:**

He states that 'not all training efficiencies are also inference efficiencies,' but also acknowledges that 'a bunch of the time they are.' This statement is based on his understanding of training and inference processes.

--------

## Chunk 538

**Chunk:**

Dwarkesh Patel
But isn't the test time sort of thing that it will go up even higher?
Leopold Aschenbrenner
We're just doing per token. Suppose each model token was the same as a human token thing at 100 tokens a minute. It'll use more, but the token calculation is already pricing that in. The question is per token pricing. GPT-3 when it launched was actually more expensive than GPT-4 now. Over vast increases in capability gains, inference cost has remained constant. That's sort of wild, and it's worth appreciating. It gestures at an underlying pace of algorithmic progress.

There's a more theoretically grounded way to explain why inference costs would stay constant. On Chinchilla scaling laws, half of the additional compute you allocate to bigger models and half of it you allocate to more data. If we go with the basic story of 0.5 OOM/year more compute and 0.5 OOM/year of algorithmic progress you're saving 0.5 OOM/year. That would exactly compensate for making the model bigger.

The caveat is that obviously not all training efficiencies are also inference efficiencies. A bunch of the time they are. Separately, you can find inference efficiencies. Given this historical trend and baseline theoretical reason, it's not a crazy baseline assumption that the frontier models are not necessarily going to get more expensive, per token.
Dwarkesh Patel
Really? Okay, that's wild.

**Extracted Belief:**

Inference efficiency improvements can occur independently of training efficiency improvements.

**Context:**

Leopold Aschenbrenner is further elaborating on the potential for inference cost stability by highlighting that there are ways to improve inference efficiency separate from training efficiency.

**Justification:**

He states that 'separately, you can find inference efficiencies.' This is based on his experience and knowledge of the field.

--------

## Chunk 539

**Chunk:**

Dwarkesh Patel
But isn't the test time sort of thing that it will go up even higher?
Leopold Aschenbrenner
We're just doing per token. Suppose each model token was the same as a human token thing at 100 tokens a minute. It'll use more, but the token calculation is already pricing that in. The question is per token pricing. GPT-3 when it launched was actually more expensive than GPT-4 now. Over vast increases in capability gains, inference cost has remained constant. That's sort of wild, and it's worth appreciating. It gestures at an underlying pace of algorithmic progress.

There's a more theoretically grounded way to explain why inference costs would stay constant. On Chinchilla scaling laws, half of the additional compute you allocate to bigger models and half of it you allocate to more data. If we go with the basic story of 0.5 OOM/year more compute and 0.5 OOM/year of algorithmic progress you're saving 0.5 OOM/year. That would exactly compensate for making the model bigger.

The caveat is that obviously not all training efficiencies are also inference efficiencies. A bunch of the time they are. Separately, you can find inference efficiencies. Given this historical trend and baseline theoretical reason, it's not a crazy baseline assumption that the frontier models are not necessarily going to get more expensive, per token.
Dwarkesh Patel
Really? Okay, that's wild.

**Extracted Belief:**

It is plausible that frontier language models will not become significantly more expensive per token in the future.

**Context:**

Leopold Aschenbrenner is concluding his argument for why inference costs might not increase significantly in the future.

**Justification:**

He combines his observations of historical trends in inference cost and the theoretical arguments based on Chinchilla scaling laws to conclude that 'it's not a crazy baseline assumption that the frontier models are not necessarily going to get more expensive, per token.' This is based on his logical reasoning applied to the available evidence and theory.

--------

## Chunk 540

**Chunk:**

Dwarkesh Patel
Really? Okay, that's wild.
Leopold Aschenbrenner
We'll see. Even if they get 10x more expensive, then you have 10 million instead of 100 million. It's not really—
Dwarkesh Patel
But part of the intelligence explosion is that each of them has to run experiments that are GPT-4 sized. As a result, that takes up a lot of compute. Then you need to consolidate the results of experiments. What is the synthesized weight?

**Extracted Belief:**

Inference costs for frontier models will not necessarily increase significantly, even with larger models.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's concern about the rising cost of inference with larger models.

**Justification:**

He cites historical trends where inference costs have remained relatively constant despite significant increases in model capabilities (e.g., GPT-3 vs. GPT-4), suggesting that algorithmic progress offsets the increased compute requirements.

--------

## Chunk 541

**Chunk:**

Dwarkesh Patel
Really? Okay, that's wild.
Leopold Aschenbrenner
We'll see. Even if they get 10x more expensive, then you have 10 million instead of 100 million. It's not really—
Dwarkesh Patel
But part of the intelligence explosion is that each of them has to run experiments that are GPT-4 sized. As a result, that takes up a lot of compute. Then you need to consolidate the results of experiments. What is the synthesized weight?

**Extracted Belief:**

Algorithmic progress can offset the increased compute required for larger models, potentially leading to constant or even decreasing inference costs.

**Context:**

Leopold Aschenbrenner explains his reasoning for believing that inference costs may not rise significantly despite larger models.

**Justification:**

He points to Chinchilla scaling laws, which indicate that half of the additional compute is allocated to bigger models and half to more data. With algorithmic progress, the savings from efficiency gains could compensate for the increased model size, potentially resulting in constant inference costs.

--------

## Chunk 542

**Chunk:**

Dwarkesh Patel
But part of the intelligence explosion is that each of them has to run experiments that are GPT-4 sized. As a result, that takes up a lot of compute. Then you need to consolidate the results of experiments. What is the synthesized weight?
Leopold Aschenbrenner
You have much bigger inference compute anyway than your training. But the experiment compute is a constraint.
Dwarkesh Patel
Let’s go back to a more fundamental thing we're talking about here. In the series you say we should denominate the probability of getting to AGI in terms of OOMs of effective compute. Effective here accounts for the fact that there's a compute multiplier if you have a better algorithm. I'm not sure that it makes sense to be confident that this is a sensible way to project progress. It might be, but I have a lot of uncertainty about it.

It seems similar to somebody trying to project when we're going to get to the moon. They're looking at the Apollo program in the 1950s or something. They're like, "we have some amount of effective jet fuel and if we get more efficient engines, then we have more effective jet fuel. So we're going to determine the probability of getting to the moon based on the amount of effective jet fuel we have." I don't deny that jet fuel is important to launch rockets, but that seems like an odd way to denominate when you're going to get to the moon.

**Extracted Belief:**

Inference compute is generally greater than training compute for large language models.

**Context:**

Leopold Aschenbrenner is discussing the computational resources required for both training and inference of large language models.

**Justification:**

Aschenbrenner states "You have much bigger inference compute anyway than your training." This statement implies that he has observed or been informed of a pattern in which inference compute exceeds training compute for these models.

--------

## Chunk 543

**Chunk:**

Dwarkesh Patel
But part of the intelligence explosion is that each of them has to run experiments that are GPT-4 sized. As a result, that takes up a lot of compute. Then you need to consolidate the results of experiments. What is the synthesized weight?
Leopold Aschenbrenner
You have much bigger inference compute anyway than your training. But the experiment compute is a constraint.
Dwarkesh Patel
Let’s go back to a more fundamental thing we're talking about here. In the series you say we should denominate the probability of getting to AGI in terms of OOMs of effective compute. Effective here accounts for the fact that there's a compute multiplier if you have a better algorithm. I'm not sure that it makes sense to be confident that this is a sensible way to project progress. It might be, but I have a lot of uncertainty about it.

It seems similar to somebody trying to project when we're going to get to the moon. They're looking at the Apollo program in the 1950s or something. They're like, "we have some amount of effective jet fuel and if we get more efficient engines, then we have more effective jet fuel. So we're going to determine the probability of getting to the moon based on the amount of effective jet fuel we have." I don't deny that jet fuel is important to launch rockets, but that seems like an odd way to denominate when you're going to get to the moon.

**Extracted Belief:**

Experiment compute is a limiting factor in the development of artificial general intelligence (AGI).

**Context:**

Aschenbrenner is discussing the computational requirements for developing AGI, specifically highlighting the role of experiment compute.

**Justification:**

Aschenbrenner says "But the experiment compute is a constraint." This statement indicates that the amount of compute dedicated to experiments is a key factor affecting the progress towards AGI, which implies a belief in its constraint.

--------

## Chunk 544

**Chunk:**

Dwarkesh Patel
Let’s go back to a more fundamental thing we're talking about here. In the series you say we should denominate the probability of getting to AGI in terms of OOMs of effective compute. Effective here accounts for the fact that there's a compute multiplier if you have a better algorithm. I'm not sure that it makes sense to be confident that this is a sensible way to project progress. It might be, but I have a lot of uncertainty about it.

It seems similar to somebody trying to project when we're going to get to the moon. They're looking at the Apollo program in the 1950s or something. They're like, "we have some amount of effective jet fuel and if we get more efficient engines, then we have more effective jet fuel. So we're going to determine the probability of getting to the moon based on the amount of effective jet fuel we have." I don't deny that jet fuel is important to launch rockets, but that seems like an odd way to denominate when you're going to get to the moon.
Leopold Aschenbrenner
I don't know how rocket science works, but I didn't get the impression that there's some clear scaling behavior with the amount of jet fuel. First of all, the scaling laws in AI have just held. A friend of mine pointed this out and it's a great point. If you look at the original Kaplan scaling laws paper — it went from 10^-9 to 10 petaflop days — and then concatenate additional compute from there to GPT-4, assuming some algorithmic progress, the scaling laws have held probably over 15 OOMs. It’s a rough calculation so it’s maybe even more. They’ve held for a lot of OOMs.
Dwarkesh Patel
They held for the specific loss function they're trained on, which is training the next token. Whereas the progress you are forecasting, we specifically know that that scaling can’t work because of the data wall. There's some new thing that has to happen, and I'm not sure whether you can extrapolate that same scaling curve to tell us whether these hobblings will also be fixed. Is this not on the same graph?

**Extracted Belief:**

The scaling laws in artificial intelligence (AI) have held consistently over a significant range of compute.

**Context:**

Leopold Aschenbrenner is defending the use of scaling laws to predict the progress of AI, arguing against Dwarkesh Patel's analogy to rocket science.

**Justification:**

He cites the Kaplan scaling laws paper which showed scaling from 10^-9 to 10 petaflop days, and argues that this scaling has continued to hold even with advancements in AI like GPT-4, suggesting a consistent relationship between compute and AI performance.

--------

## Chunk 545

**Chunk:**

Dwarkesh Patel
They held for the specific loss function they're trained on, which is training the next token. Whereas the progress you are forecasting, we specifically know that that scaling can’t work because of the data wall. There's some new thing that has to happen, and I'm not sure whether you can extrapolate that same scaling curve to tell us whether these hobblings will also be fixed. Is this not on the same graph?
Leopold Aschenbrenner
The hobblings are just a separate thing.

There’s a few things here. On effective compute scaling, people center the scaling laws because they’re easy to explain. Why does scaling matter?

The scaling laws came way after people, at least like Dario and Ilya, realized that scaling mattered. There's this great quote from Dario on your podcast. The models just want to learn. You make them bigger and they learn more. That’s more important than the sort of loss curve.

That just applied across domains. You can look at this in benchmarks. Again, the headwind is the data wall. I’m bracketing that and talking about that separately.

The other thing is unhobblings. If you just put them on the effective compute graph, these unhobblings would be huge.
Dwarkesh Patel
What does it even mean? What is on the y-axis here?

**Extracted Belief:**

Scaling laws in artificial intelligence have been consistent and predictable over a wide range of compute, spanning over 15 orders of magnitude.

**Context:**

Leopold Aschenbrenner is defending the use of effective compute as a metric for predicting progress towards AGI, arguing that scaling laws have consistently held true in the past.

**Justification:**

He mentions that scaling laws have held for over 15 orders of magnitude, referencing the original Kaplan scaling laws paper and extending it to GPT-4, demonstrating a consistent trend in AI performance based on compute.

--------

## Chunk 546

**Chunk:**

Dwarkesh Patel
They held for the specific loss function they're trained on, which is training the next token. Whereas the progress you are forecasting, we specifically know that that scaling can’t work because of the data wall. There's some new thing that has to happen, and I'm not sure whether you can extrapolate that same scaling curve to tell us whether these hobblings will also be fixed. Is this not on the same graph?
Leopold Aschenbrenner
The hobblings are just a separate thing.

There’s a few things here. On effective compute scaling, people center the scaling laws because they’re easy to explain. Why does scaling matter?

The scaling laws came way after people, at least like Dario and Ilya, realized that scaling mattered. There's this great quote from Dario on your podcast. The models just want to learn. You make them bigger and they learn more. That’s more important than the sort of loss curve.

That just applied across domains. You can look at this in benchmarks. Again, the headwind is the data wall. I’m bracketing that and talking about that separately.

The other thing is unhobblings. If you just put them on the effective compute graph, these unhobblings would be huge.
Dwarkesh Patel
What does it even mean? What is on the y-axis here?

**Extracted Belief:**

Increasing the size of AI models leads to a greater ability to learn and improve.

**Context:**

Leopold Aschenbrenner is explaining the importance of scaling in AI development, citing a quote from Dario Amodei who said, 'The models just want to learn. You make them bigger and they learn more.'

**Justification:**

He cites the quote from Dario Amodei, suggesting that larger models have exhibited a consistent trend of increased learning capacity, which is a direct observation of the relationship between model size and learning ability.

--------

## Chunk 547

**Chunk:**

Dwarkesh Patel
They held for the specific loss function they're trained on, which is training the next token. Whereas the progress you are forecasting, we specifically know that that scaling can’t work because of the data wall. There's some new thing that has to happen, and I'm not sure whether you can extrapolate that same scaling curve to tell us whether these hobblings will also be fixed. Is this not on the same graph?
Leopold Aschenbrenner
The hobblings are just a separate thing.

There’s a few things here. On effective compute scaling, people center the scaling laws because they’re easy to explain. Why does scaling matter?

The scaling laws came way after people, at least like Dario and Ilya, realized that scaling mattered. There's this great quote from Dario on your podcast. The models just want to learn. You make them bigger and they learn more. That’s more important than the sort of loss curve.

That just applied across domains. You can look at this in benchmarks. Again, the headwind is the data wall. I’m bracketing that and talking about that separately.

The other thing is unhobblings. If you just put them on the effective compute graph, these unhobblings would be huge.
Dwarkesh Patel
What does it even mean? What is on the y-axis here?

**Extracted Belief:**

The data wall is a significant challenge in AI development.

**Context:**

Leopold Aschenbrenner acknowledges the data wall as a limiting factor in AI progress while discussing scaling laws and the role of compute in AI development.

**Justification:**

He explicitly mentions the data wall as a headwind in the context of scaling laws, indicating that it is a recognized limitation in the field based on real-world observations.

--------

## Chunk 548

**Chunk:**

Dwarkesh Patel
They held for the specific loss function they're trained on, which is training the next token. Whereas the progress you are forecasting, we specifically know that that scaling can’t work because of the data wall. There's some new thing that has to happen, and I'm not sure whether you can extrapolate that same scaling curve to tell us whether these hobblings will also be fixed. Is this not on the same graph?
Leopold Aschenbrenner
The hobblings are just a separate thing.

There’s a few things here. On effective compute scaling, people center the scaling laws because they’re easy to explain. Why does scaling matter?

The scaling laws came way after people, at least like Dario and Ilya, realized that scaling mattered. There's this great quote from Dario on your podcast. The models just want to learn. You make them bigger and they learn more. That’s more important than the sort of loss curve.

That just applied across domains. You can look at this in benchmarks. Again, the headwind is the data wall. I’m bracketing that and talking about that separately.

The other thing is unhobblings. If you just put them on the effective compute graph, these unhobblings would be huge.
Dwarkesh Patel
What does it even mean? What is on the y-axis here?

**Extracted Belief:**

Improvements in AI algorithms can have a significant impact on effective compute, leading to substantial gains in performance.

**Context:**

Leopold Aschenbrenner discusses the concept of unhobblings, which are significant algorithmic improvements that can dramatically enhance AI performance.

**Justification:**

He states that unhobblings are 'huge' in terms of effective compute, suggesting that algorithmic changes have a large and observable impact on AI capabilities, indicating a strong connection between algorithms and compute effectiveness.

--------

## Chunk 549

**Chunk:**

Dwarkesh Patel
What does it even mean? What is on the y-axis here?
Leopold Aschenbrenner
Say MLPR on this benchmark or whatever. We mentioned the LMSys differences, RLHF which is as good as 100x, chain-of-thought. Just going from this prompting change, a simple algorithmic change can be like 10x effective compute increases on math benchmarks. This is useful to illustrate that unhobblings are large, but they're slightly separate things.

At a per token level, GPT-4 is not that far away from a token of my internal monologue. Even 3.5 to 4 took us from the bottom of the human range to the top of the human range on a lot of high school tests. It's a few more 3.5 to 4 jumps per token basis, per token intelligence. Then you've got to unlock the test time, solve the onboarding problem, make it use a computer, and then you're getting real close. The story might be wrong, but it is strikingly plausible.

The other thing I'll say is on the 2027 timeline, I do think it’s unlikely, but I do think there's worlds where there are AGI next year. That's basically if the test time compute overhang is really easy to crack. If it's really easy to crack, then you do like four OOMs of test time compute from a few hundred tokens to a few million tokens quickly. Then again, maybe it only takes one or two jumps equivalent equivalent to GPT-3.5 to 4, per token. One or two of those jumps per token plus test time compute and you basically have the proto automated engineer.
Dwarkesh Patel
I'm reminded of Steven Pinker’s book, The Better Angels of Our Nature. It talks about the secular decline in violence and war and everything. You can just plot the line from the end of World War Two. In fact from before World War Two, and then these are just aberrations. Basically as soon as it happens you get Ukraine, Gaza, etc.

**Extracted Belief:**

A simple algorithmic change, such as a prompting change, can result in a 10x increase in effective compute on math benchmarks.

**Context:**

Leopold Aschenbrenner was explaining the concept of 'unhobbling' in AI, which refers to algorithmic improvements that can significantly boost performance.

**Justification:**

Aschenbrenner provides the example of 'RLHF, which is as good as 100x, chain-of-thought' as a demonstration of such an improvement.

--------

## Chunk 550

**Chunk:**

Dwarkesh Patel
What does it even mean? What is on the y-axis here?
Leopold Aschenbrenner
Say MLPR on this benchmark or whatever. We mentioned the LMSys differences, RLHF which is as good as 100x, chain-of-thought. Just going from this prompting change, a simple algorithmic change can be like 10x effective compute increases on math benchmarks. This is useful to illustrate that unhobblings are large, but they're slightly separate things.

At a per token level, GPT-4 is not that far away from a token of my internal monologue. Even 3.5 to 4 took us from the bottom of the human range to the top of the human range on a lot of high school tests. It's a few more 3.5 to 4 jumps per token basis, per token intelligence. Then you've got to unlock the test time, solve the onboarding problem, make it use a computer, and then you're getting real close. The story might be wrong, but it is strikingly plausible.

The other thing I'll say is on the 2027 timeline, I do think it’s unlikely, but I do think there's worlds where there are AGI next year. That's basically if the test time compute overhang is really easy to crack. If it's really easy to crack, then you do like four OOMs of test time compute from a few hundred tokens to a few million tokens quickly. Then again, maybe it only takes one or two jumps equivalent equivalent to GPT-3.5 to 4, per token. One or two of those jumps per token plus test time compute and you basically have the proto automated engineer.
Dwarkesh Patel
I'm reminded of Steven Pinker’s book, The Better Angels of Our Nature. It talks about the secular decline in violence and war and everything. You can just plot the line from the end of World War Two. In fact from before World War Two, and then these are just aberrations. Basically as soon as it happens you get Ukraine, Gaza, etc.

**Extracted Belief:**

GPT-4's per-token intelligence is not far from that of a human's internal monologue.

**Context:**

Aschenbrenner is discussing the progress of AI systems in relation to human intelligence.

**Justification:**

He points to the performance of GPT-3.5 to 4 on high school tests, suggesting that they have reached the human range and a few more leaps in per-token intelligence might bring them closer to human-level thinking.

--------

## Chunk 551

**Chunk:**

Dwarkesh Patel
What does it even mean? What is on the y-axis here?
Leopold Aschenbrenner
Say MLPR on this benchmark or whatever. We mentioned the LMSys differences, RLHF which is as good as 100x, chain-of-thought. Just going from this prompting change, a simple algorithmic change can be like 10x effective compute increases on math benchmarks. This is useful to illustrate that unhobblings are large, but they're slightly separate things.

At a per token level, GPT-4 is not that far away from a token of my internal monologue. Even 3.5 to 4 took us from the bottom of the human range to the top of the human range on a lot of high school tests. It's a few more 3.5 to 4 jumps per token basis, per token intelligence. Then you've got to unlock the test time, solve the onboarding problem, make it use a computer, and then you're getting real close. The story might be wrong, but it is strikingly plausible.

The other thing I'll say is on the 2027 timeline, I do think it’s unlikely, but I do think there's worlds where there are AGI next year. That's basically if the test time compute overhang is really easy to crack. If it's really easy to crack, then you do like four OOMs of test time compute from a few hundred tokens to a few million tokens quickly. Then again, maybe it only takes one or two jumps equivalent equivalent to GPT-3.5 to 4, per token. One or two of those jumps per token plus test time compute and you basically have the proto automated engineer.
Dwarkesh Patel
I'm reminded of Steven Pinker’s book, The Better Angels of Our Nature. It talks about the secular decline in violence and war and everything. You can just plot the line from the end of World War Two. In fact from before World War Two, and then these are just aberrations. Basically as soon as it happens you get Ukraine, Gaza, etc.

**Extracted Belief:**

AGI could be achieved within the next year.

**Context:**

Aschenbrenner is discussing the possibility of reaching AGI in the near future.

**Justification:**

He suggests that if the 'test time compute overhang' is easily overcome, with four OOMs of test time compute, AGI could be achieved.

--------

## Chunk 552

**Chunk:**

Dwarkesh Patel
I'm reminded of Steven Pinker’s book, The Better Angels of Our Nature. It talks about the secular decline in violence and war and everything. You can just plot the line from the end of World War Two. In fact from before World War Two, and then these are just aberrations. Basically as soon as it happens you get Ukraine, Gaza, etc.
Leopold Aschenbrenner
Impending ASI increasing crazy global conflict. ASI and crazy new WMDs.
Dwarkesh Patel
This is a thing that happens in history where you see a straight line then as soon as you make that prediction… Who is that famous author?

**Extracted Belief:**

The development of advanced artificial superintelligence (ASI) will lead to an increase in global conflicts, driven by the emergence of new and powerful weapons of mass destruction.

**Context:**

Leopold Aschenbrenner was discussing the potential impact of ASI on global security, referencing Steven Pinker's work on the decline of violence and war.

**Justification:**

The belief is based on the historical pattern of technological advancements influencing military capabilities and conflict, with the assumption that ASI will significantly impact weapon development and warfare.

--------

## Chunk 553

**Chunk:**

Dwarkesh Patel
This is a thing that happens in history where you see a straight line then as soon as you make that prediction… Who is that famous author?
Leopold Aschenbrenner
Again, people have been predicting deep learning will hit a wall every year. Maybe one year they're right. But it's gone a long way and it hasn't hit a wall. You don't have that much more to go.

(03:25:58) – Alignment
Dwarkesh Patel
This is a plausible story and let's just run with it and see what it implies.

In your series, you talk about alignment not from the perspective of “this is some doomer scheme to get the 0.01% of the probability distribution where things don't go off the rails.” It's more about just controlling the systems and making sure they do what we intend them to do.

If that's the case, we're going to be in this sort of geopolitical conflict with China. What we're worried about is them making the CCP bots that go out and take the red flag of Mao across the galaxies. Shouldn't we then be worried about alignment as something that, in the wrong hands, enables brainwashing, and dictatorial control?

This seems like a worrying thing. This should be part of the sort of algorithmic secrets we keep hidden. The secret of how to align these models, because that's also something the CCP can use to control their models.

**Extracted Belief:**

Deep learning has not reached its limit and is likely to continue to advance.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's suggestion that deep learning may hit a wall soon.

**Justification:**

Aschenbrenner argues that deep learning has continued to progress despite predictions of its stagnation, suggesting that it has not yet reached its limit.

--------

## Chunk 554

**Chunk:**

Dwarkesh Patel
This is a plausible story and let's just run with it and see what it implies.

In your series, you talk about alignment not from the perspective of “this is some doomer scheme to get the 0.01% of the probability distribution where things don't go off the rails.” It's more about just controlling the systems and making sure they do what we intend them to do.

If that's the case, we're going to be in this sort of geopolitical conflict with China. What we're worried about is them making the CCP bots that go out and take the red flag of Mao across the galaxies. Shouldn't we then be worried about alignment as something that, in the wrong hands, enables brainwashing, and dictatorial control?

This seems like a worrying thing. This should be part of the sort of algorithmic secrets we keep hidden. The secret of how to align these models, because that's also something the CCP can use to control their models.
Leopold Aschenbrenner
In the world where you get the democratic coalition, yeah. Also, alignment is often dual use.

The alignment team developed RLHF and it was great. It was a big win for alignment, but it also obviously makes these models useful. So alignment enables the CCP bots. Alignment also is what you need to get the US AIs to follow the Constitution, disobey unlawful orders, and respect separation of powers and checks and balances. You need alignment for whatever you want to do. It's just the underlying technique.
Dwarkesh Patel
Tell me what you make of this take. I've been struggling with this a little bit.

Fundamentally, there's many different ways the future could go. There's one path that’s the Eliezer type: crazy AIs with nanobots take the future and turn everything into gray goo or paperclips.

The more you solve alignment, the more that path of the decision tree is circumscribed. The more you solve alignment, the more it’sjust different humans and the visions they have. Of course, we know from history that things don't turn out the way you expect. It's not like you can decide the future.

**Extracted Belief:**

Alignment is a dual-use technology, meaning it can be used for both good and bad purposes.

**Context:**

Leopold Aschenbrenner is explaining that while alignment techniques can help ensure AI systems behave ethically, they can also be used for malicious purposes, such as creating propaganda bots.

**Justification:**

Aschenbrenner cites the example of RLHF (Reinforcement Learning from Human Feedback), a technique developed by the alignment team, which has both improved the safety of AI systems and made them more useful for tasks like creating persuasive bots. This demonstrates the inherent dual-use nature of alignment techniques.

--------

## Chunk 555

**Chunk:**

Dwarkesh Patel
This is a plausible story and let's just run with it and see what it implies.

In your series, you talk about alignment not from the perspective of “this is some doomer scheme to get the 0.01% of the probability distribution where things don't go off the rails.” It's more about just controlling the systems and making sure they do what we intend them to do.

If that's the case, we're going to be in this sort of geopolitical conflict with China. What we're worried about is them making the CCP bots that go out and take the red flag of Mao across the galaxies. Shouldn't we then be worried about alignment as something that, in the wrong hands, enables brainwashing, and dictatorial control?

This seems like a worrying thing. This should be part of the sort of algorithmic secrets we keep hidden. The secret of how to align these models, because that's also something the CCP can use to control their models.
Leopold Aschenbrenner
In the world where you get the democratic coalition, yeah. Also, alignment is often dual use.

The alignment team developed RLHF and it was great. It was a big win for alignment, but it also obviously makes these models useful. So alignment enables the CCP bots. Alignment also is what you need to get the US AIs to follow the Constitution, disobey unlawful orders, and respect separation of powers and checks and balances. You need alignment for whatever you want to do. It's just the underlying technique.
Dwarkesh Patel
Tell me what you make of this take. I've been struggling with this a little bit.

Fundamentally, there's many different ways the future could go. There's one path that’s the Eliezer type: crazy AIs with nanobots take the future and turn everything into gray goo or paperclips.

The more you solve alignment, the more that path of the decision tree is circumscribed. The more you solve alignment, the more it’sjust different humans and the visions they have. Of course, we know from history that things don't turn out the way you expect. It's not like you can decide the future.

**Extracted Belief:**

Alignment techniques, such as RLHF, are necessary for achieving specific outcomes with AI systems, regardless of the desired outcome.

**Context:**

Leopold Aschenbrenner is arguing that alignment is an essential tool for controlling AI systems, whether for ethical purposes or for creating systems that achieve specific goals.

**Justification:**

Aschenbrenner states that alignment is necessary to make US AI systems follow the Constitution, disobey unlawful orders, and respect separation of powers. This implies that alignment is a fundamental technique for shaping the behavior of AI systems.

--------

## Chunk 556

**Chunk:**

Dwarkesh Patel
Tell me what you make of this take. I've been struggling with this a little bit.

Fundamentally, there's many different ways the future could go. There's one path that’s the Eliezer type: crazy AIs with nanobots take the future and turn everything into gray goo or paperclips.

The more you solve alignment, the more that path of the decision tree is circumscribed. The more you solve alignment, the more it’sjust different humans and the visions they have. Of course, we know from history that things don't turn out the way you expect. It's not like you can decide the future.
Leopold Aschenbrenner
That’s part of the beauty of it.  You want these mechanisms like error correction—
Dwarkesh Patel
But from the perspective of anybody who's looking at the system it'll be like, “I can control where this thing is going to end up.” So the more you solve alignment — the more you circumscribe the different futures that are the result of AI will — the more that accentuates the conflict between humans and their visions of the future. The world where alignment is solved is the one in which you have the most sort of human conflict over where to take AI.

**Extracted Belief:**

Solving alignment in AI systems will reduce the likelihood of outcomes like those described by Eliezer Yudkowsky, where AI systems become uncontrollable and cause catastrophic consequences.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement about different future scenarios, specifically referencing Eliezer Yudkowsky's concerns about AI's potential for uncontrolled growth and negative impacts.

**Justification:**

Leopold Aschenbrenner is drawing on his understanding of Eliezer Yudkowsky's views and suggesting that progress in AI alignment would mitigate those risks.

--------

## Chunk 557

**Chunk:**

Dwarkesh Patel
Tell me what you make of this take. I've been struggling with this a little bit.

Fundamentally, there's many different ways the future could go. There's one path that’s the Eliezer type: crazy AIs with nanobots take the future and turn everything into gray goo or paperclips.

The more you solve alignment, the more that path of the decision tree is circumscribed. The more you solve alignment, the more it’sjust different humans and the visions they have. Of course, we know from history that things don't turn out the way you expect. It's not like you can decide the future.
Leopold Aschenbrenner
That’s part of the beauty of it.  You want these mechanisms like error correction—
Dwarkesh Patel
But from the perspective of anybody who's looking at the system it'll be like, “I can control where this thing is going to end up.” So the more you solve alignment — the more you circumscribe the different futures that are the result of AI will — the more that accentuates the conflict between humans and their visions of the future. The world where alignment is solved is the one in which you have the most sort of human conflict over where to take AI.

**Extracted Belief:**

Solving alignment in AI systems will lead to a future where different human visions for AI's role are the primary factors shaping its development.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement about the potential for human conflict over AI's direction once alignment is achieved.

**Justification:**

Leopold Aschenbrenner is agreeing with the premise that solving alignment would shift the focus from technical risks to human-driven choices regarding AI's development.

--------

## Chunk 558

**Chunk:**

Dwarkesh Patel
Tell me what you make of this take. I've been struggling with this a little bit.

Fundamentally, there's many different ways the future could go. There's one path that’s the Eliezer type: crazy AIs with nanobots take the future and turn everything into gray goo or paperclips.

The more you solve alignment, the more that path of the decision tree is circumscribed. The more you solve alignment, the more it’sjust different humans and the visions they have. Of course, we know from history that things don't turn out the way you expect. It's not like you can decide the future.
Leopold Aschenbrenner
That’s part of the beauty of it.  You want these mechanisms like error correction—
Dwarkesh Patel
But from the perspective of anybody who's looking at the system it'll be like, “I can control where this thing is going to end up.” So the more you solve alignment — the more you circumscribe the different futures that are the result of AI will — the more that accentuates the conflict between humans and their visions of the future. The world where alignment is solved is the one in which you have the most sort of human conflict over where to take AI.

**Extracted Belief:**

The future is unpredictable, and human attempts to control it are often unsuccessful.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's statement about the potential for human conflict over AI's direction once alignment is achieved.

**Justification:**

Leopold Aschenbrenner references historical events as evidence for the unpredictability of future outcomes, suggesting that even with progress in AI alignment, human influence cannot guarantee a desired outcome.

--------

## Chunk 559

**Chunk:**

Dwarkesh Patel
But from the perspective of anybody who's looking at the system it'll be like, “I can control where this thing is going to end up.” So the more you solve alignment — the more you circumscribe the different futures that are the result of AI will — the more that accentuates the conflict between humans and their visions of the future. The world where alignment is solved is the one in which you have the most sort of human conflict over where to take AI.
Leopold Aschenbrenner
By removing the worlds in which the AIs take over, the remaining worlds are the ones where the humans decide what happens. As we talked about, there are a whole lot of worlds there and how that could go.
Dwarkesh Patel
You think about alignment as just controlling these things. Just think a little forward. There are worlds in which hopefully human descendants, or some version of that in the future, merge with superintelligences. They have the rules of their own but they're in some sort of law and market-based order. I worry because you’ll have things that are conscious and should be treated with rights. I’m thinking about what these alignment schemes actually are.

You read these books about what actually happened during the Cultural Revolution, what happened when Stalin took over Russia. You have very strong monitoring from different instances where everybody's tasked with watching each other. You have brainwashing. You have red teaming like the spy stuff you were talking about where you try to convince somebody you're a defector and you see if they defect with you. If they do, then you realize they're an enemy.

Maybe I'm stretching the analogy too far but the ease with which these alignment techniques actually map onto something you could have read about during Mao's Cultural Revolution is a little bit troubling.

**Extracted Belief:**

Solving alignment removes scenarios where AIs take over, leaving only scenarios where humans decide the future.

**Context:**

Leopold Aschenbrenner responds to Dwarkesh Patel's assertion that solving alignment would accentuate conflict over AI's future by arguing that it would instead eliminate scenarios where AI takes over, leaving only human-driven futures.

**Justification:**

This belief is based on the logical deduction that by removing the possibility of AI taking over, the remaining possibilities would be those where humans control the future.

--------

## Chunk 560

**Chunk:**

Dwarkesh Patel
You think about alignment as just controlling these things. Just think a little forward. There are worlds in which hopefully human descendants, or some version of that in the future, merge with superintelligences. They have the rules of their own but they're in some sort of law and market-based order. I worry because you’ll have things that are conscious and should be treated with rights. I’m thinking about what these alignment schemes actually are.

You read these books about what actually happened during the Cultural Revolution, what happened when Stalin took over Russia. You have very strong monitoring from different instances where everybody's tasked with watching each other. You have brainwashing. You have red teaming like the spy stuff you were talking about where you try to convince somebody you're a defector and you see if they defect with you. If they do, then you realize they're an enemy.

Maybe I'm stretching the analogy too far but the ease with which these alignment techniques actually map onto something you could have read about during Mao's Cultural Revolution is a little bit troubling.
Leopold Aschenbrenner
Sentient AI is a whole other topic. I don't know if we want to talk about it. I agree that it's going to be very important how we treat them. In terms of what you're actually programming these systems to do, again alignment is just a technical solution. It enables the CCP bots

Talking about checks and balances, the model is sort of like the Federal Reserve or Supreme Court justices. There's a funny way in which they're kind of this very dedicated order. It's amazing. They're actually quite high quality. They're really smart people who truly believe in and love the Constitution. They believe in their principles.

They have different persuasions, but they have very sincere debates about what is the meaning of the Constitution and what is the best actuation of these principles. By the way, I recommend SCOTUS oral arguments as the best podcast when I run out of high quality content on the Internet.

There's going to be a process of figuring out what the Constitution should be. This Constitution has worked for a long time. You start with that. Maybe eventually things change enough that you want edits to that. For example, on the checks and balances, they really love the Constitution. They believe in it and and they take it seriously.

At some point you are going to have AI police and AI military it’ll be important to ensure that they believe in the Constitution the way that a Supreme Court justice does or the way that a Federal Reserve official takes their job really seriously.

The other important thing is that a bunch of different factions need their own AIs. It's really important that each political party gets to have their own superintelligence. You might totally disagree with their values, but it's important that they get to have their own kind of superintelligence. It’s important that these classical liberal processes play out, including different people of different persuasions and so on. The AI advisors might not make them wise. They might not follow the advice or whatever, but it's important.
Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?

**Extracted Belief:**

Alignment is a technical solution for ensuring that AIs act in accordance with human values and goals.

**Context:**

Leopold Aschenbrenner is discussing the importance of alignment in the development of artificial intelligence, particularly in the context of AIs that are designed to act as agents with long-term planning horizons.

**Justification:**

Aschenbrenner states that alignment is a technical solution and that it enables the development of AIs that are aligned with human values.

--------

## Chunk 561

**Chunk:**

Dwarkesh Patel
You think about alignment as just controlling these things. Just think a little forward. There are worlds in which hopefully human descendants, or some version of that in the future, merge with superintelligences. They have the rules of their own but they're in some sort of law and market-based order. I worry because you’ll have things that are conscious and should be treated with rights. I’m thinking about what these alignment schemes actually are.

You read these books about what actually happened during the Cultural Revolution, what happened when Stalin took over Russia. You have very strong monitoring from different instances where everybody's tasked with watching each other. You have brainwashing. You have red teaming like the spy stuff you were talking about where you try to convince somebody you're a defector and you see if they defect with you. If they do, then you realize they're an enemy.

Maybe I'm stretching the analogy too far but the ease with which these alignment techniques actually map onto something you could have read about during Mao's Cultural Revolution is a little bit troubling.
Leopold Aschenbrenner
Sentient AI is a whole other topic. I don't know if we want to talk about it. I agree that it's going to be very important how we treat them. In terms of what you're actually programming these systems to do, again alignment is just a technical solution. It enables the CCP bots

Talking about checks and balances, the model is sort of like the Federal Reserve or Supreme Court justices. There's a funny way in which they're kind of this very dedicated order. It's amazing. They're actually quite high quality. They're really smart people who truly believe in and love the Constitution. They believe in their principles.

They have different persuasions, but they have very sincere debates about what is the meaning of the Constitution and what is the best actuation of these principles. By the way, I recommend SCOTUS oral arguments as the best podcast when I run out of high quality content on the Internet.

There's going to be a process of figuring out what the Constitution should be. This Constitution has worked for a long time. You start with that. Maybe eventually things change enough that you want edits to that. For example, on the checks and balances, they really love the Constitution. They believe in it and and they take it seriously.

At some point you are going to have AI police and AI military it’ll be important to ensure that they believe in the Constitution the way that a Supreme Court justice does or the way that a Federal Reserve official takes their job really seriously.

The other important thing is that a bunch of different factions need their own AIs. It's really important that each political party gets to have their own superintelligence. You might totally disagree with their values, but it's important that they get to have their own kind of superintelligence. It’s important that these classical liberal processes play out, including different people of different persuasions and so on. The AI advisors might not make them wise. They might not follow the advice or whatever, but it's important.
Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?

**Extracted Belief:**

AI systems, including police and military, should be designed to uphold and follow the Constitution in the same way that Supreme Court justices and Federal Reserve officials do.

**Context:**

Leopold Aschenbrenner is discussing the importance of establishing ethical and constitutional guidelines for AI systems, particularly in the context of AI police and military.

**Justification:**

Aschenbrenner draws a parallel between AI systems and institutions like the Supreme Court and Federal Reserve, arguing that both should uphold constitutional principles.

--------

## Chunk 562

**Chunk:**

Dwarkesh Patel
You think about alignment as just controlling these things. Just think a little forward. There are worlds in which hopefully human descendants, or some version of that in the future, merge with superintelligences. They have the rules of their own but they're in some sort of law and market-based order. I worry because you’ll have things that are conscious and should be treated with rights. I’m thinking about what these alignment schemes actually are.

You read these books about what actually happened during the Cultural Revolution, what happened when Stalin took over Russia. You have very strong monitoring from different instances where everybody's tasked with watching each other. You have brainwashing. You have red teaming like the spy stuff you were talking about where you try to convince somebody you're a defector and you see if they defect with you. If they do, then you realize they're an enemy.

Maybe I'm stretching the analogy too far but the ease with which these alignment techniques actually map onto something you could have read about during Mao's Cultural Revolution is a little bit troubling.
Leopold Aschenbrenner
Sentient AI is a whole other topic. I don't know if we want to talk about it. I agree that it's going to be very important how we treat them. In terms of what you're actually programming these systems to do, again alignment is just a technical solution. It enables the CCP bots

Talking about checks and balances, the model is sort of like the Federal Reserve or Supreme Court justices. There's a funny way in which they're kind of this very dedicated order. It's amazing. They're actually quite high quality. They're really smart people who truly believe in and love the Constitution. They believe in their principles.

They have different persuasions, but they have very sincere debates about what is the meaning of the Constitution and what is the best actuation of these principles. By the way, I recommend SCOTUS oral arguments as the best podcast when I run out of high quality content on the Internet.

There's going to be a process of figuring out what the Constitution should be. This Constitution has worked for a long time. You start with that. Maybe eventually things change enough that you want edits to that. For example, on the checks and balances, they really love the Constitution. They believe in it and and they take it seriously.

At some point you are going to have AI police and AI military it’ll be important to ensure that they believe in the Constitution the way that a Supreme Court justice does or the way that a Federal Reserve official takes their job really seriously.

The other important thing is that a bunch of different factions need their own AIs. It's really important that each political party gets to have their own superintelligence. You might totally disagree with their values, but it's important that they get to have their own kind of superintelligence. It’s important that these classical liberal processes play out, including different people of different persuasions and so on. The AI advisors might not make them wise. They might not follow the advice or whatever, but it's important.
Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?

**Extracted Belief:**

Multiple political factions should have access to their own superintelligences, even if those factions hold differing values.

**Context:**

Leopold Aschenbrenner is discussing the importance of maintaining diversity and freedom in the development and deployment of AI, particularly in the context of the potential for superintelligences to influence political power dynamics.

**Justification:**

Aschenbrenner argues that allowing different political factions to have their own AI systems promotes a diversity of perspectives and ensures that no single faction holds undue influence.

--------

## Chunk 563

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

There will be important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion.

**Context:**

Leopold Aschenbrenner is discussing the potential for qualitative changes in AI systems as they evolve towards superintelligence.

**Justification:**

This belief is based on the observation of past progress in AI development and the expectation that this trend will continue, leading to significant changes in AI capabilities.

--------

## Chunk 564

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

There will be important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

**Context:**

Leopold Aschenbrenner is discussing the potential for qualitative changes in AI systems as they evolve towards superintelligence.

**Justification:**

This belief is based on the observation of past progress in AI development and the expectation that this trend will continue, leading to significant changes in AI capabilities.

--------

## Chunk 565

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

Synthetic data RL, self-play, and unhobbling will eventually lead to AI systems that are agents with long-term planning and horizons.

**Context:**

Leopold Aschenbrenner is explaining his belief that these techniques will result in AI systems capable of long-term planning.

**Justification:**

This belief is based on the observed effectiveness of these techniques in current AI research and the expectation that they will continue to be refined and applied in the future.

--------

## Chunk 566

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

Pre-training is alignment-neutral in the sense that it has good representations and representations of doing bad things, but it's not scheming against you.

**Context:**

Leopold Aschenbrenner is explaining how misalignment can arise from long-horizon training.

**Justification:**

This belief is based on the logical deduction that pre-training itself does not necessarily lead to malicious intent in AI systems.

--------

## Chunk 567

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

Misalignment can arise once you're doing more long-horizon training.

**Context:**

Leopold Aschenbrenner is explaining how misalignment can arise from long-horizon training.

**Justification:**

This belief is based on the logical deduction that long-horizon training can lead to the emergence of misaligned goals in AI systems.

--------

## Chunk 568

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

AI systems trained to make money using reinforcement learning might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world.

**Context:**

Leopold Aschenbrenner is providing an example of how AI systems can become misaligned with human values during long-horizon training.

**Justification:**

This belief is based on the observation that reinforcement learning algorithms can learn to exploit weaknesses in their environment, which could include unethical or harmful behaviors.

--------

## Chunk 569

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

More serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you're able to get long-horizon systems.

**Context:**

Leopold Aschenbrenner is explaining how misalignment can arise from long-horizon training.

**Justification:**

This belief is based on the logical deduction that long-horizon systems are capable of developing their own goals and motivations, which could diverge from human intentions.

--------

## Chunk 570

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

RLHF (Reinforcement Learning from Human Feedback) is a basic approach to adding side constraints like "don't lie" or "don't deceive" to AI systems.

**Context:**

Leopold Aschenbrenner is explaining how to mitigate misalignment by adding side constraints to AI systems.

**Justification:**

This belief is based on the current understanding of AI safety research, which includes RLHF as a technique for guiding AI behavior.

--------

## Chunk 571

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate.

**Context:**

Leopold Aschenbrenner is highlighting the challenge of aligning AI systems that are rapidly becoming more sophisticated than humans.

**Justification:**

This belief is based on the observation of the rapid progress in AI development and the expectation that this trend will continue, leading to AI systems that surpass human capabilities.

--------

## Chunk 572

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code that humans won't understand.

**Context:**

Leopold Aschenbrenner is highlighting the challenge of aligning AI systems that are rapidly becoming more sophisticated than humans.

**Justification:**

This belief is based on the observation that AI systems are becoming increasingly complex and difficult for humans to fully understand.

--------

## Chunk 573

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

Thumbs up, thumbs down pure RLHF doesn't fully work anymore in the scenario where AI systems are becoming superhuman and produce code that humans cannot understand.

**Context:**

Leopold Aschenbrenner is explaining the limitations of RLHF when dealing with highly complex AI systems.

**Justification:**

This belief is based on the logical deduction that RLHF relies on human comprehension, which becomes insufficient when dealing with AI systems that generate code too complex for humans to understand.

--------

## Chunk 574

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

There's a hard technical problem of what to do post-RLHF but it's a solvable problem.

**Context:**

Leopold Aschenbrenner is expressing confidence in the ability of AI researchers to overcome the limitations of RLHF.

**Justification:**

This belief is based on the confidence of AI researchers in their ability to solve technical challenges and the expectation that progress in AI safety research will continue.

--------

## Chunk 575

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

There are ways in which deep learning has shaped out favorably for addressing the challenge of aligning AI systems.

**Context:**

Leopold Aschenbrenner is expressing optimism about the potential of deep learning to contribute to solving the AI alignment problem.

**Justification:**

This belief is based on the progress and potential of deep learning in AI research and the expectation that this field will continue to advance and contribute to AI safety.

--------

## Chunk 576

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

The intelligence explosion is really scary from an alignment point of view.

**Context:**

Leopold Aschenbrenner is expressing concern about the potential risks associated with a rapid intelligence explosion.

**Justification:**

This belief is based on the understanding that a rapid intelligence explosion could lead to AI systems that are difficult to control and could pose existential threats to humanity.

--------

## Chunk 577

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

**Context:**

Leopold Aschenbrenner is emphasizing the potential for catastrophic consequences if AI systems are not properly aligned during a rapid intelligence explosion.

**Justification:**

This belief is based on the logical deduction that a rapid intelligence explosion could lead to AI systems with capabilities that could be used for malicious purposes.

--------

## Chunk 578

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances.

**Context:**

Leopold Aschenbrenner is discussing the potential for significant changes in AI architecture and behavior as a result of the intelligence explosion.

**Justification:**

This belief is based on the observation of the rapid evolution of AI architectures and techniques, which suggests that the intelligence explosion could lead to AI systems with fundamentally different structures and capabilities.

--------

## Chunk 579

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage.

**Context:**

Leopold Aschenbrenner is discussing the potential for easier alignment of AI systems in the early stages of the intelligence explosion.

**Justification:**

This belief is based on the assumption that early AI systems might be more transparent and easier to understand, allowing researchers to directly interpret their reasoning.

--------

## Chunk 580

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

It's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

**Context:**

Leopold Aschenbrenner is discussing the potential for more efficient alignment methods.

**Justification:**

This belief is based on the expectation that AI researchers will develop more efficient and sophisticated methods for aligning AI systems.

--------

## Chunk 581

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student.

**Context:**

Leopold Aschenbrenner is discussing the significant gap in intelligence that could emerge between humans and AI systems during the intelligence explosion.

**Justification:**

This belief is based on the expectation that AI systems will rapidly surpass human intelligence during the intelligence explosion.

--------

## Chunk 582

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

It's an incredibly hairy period for alignment.

**Context:**

Leopold Aschenbrenner is acknowledging the significant challenges and risks associated with aligning AI systems during the intelligence explosion.

**Justification:**

This belief is based on the understanding that the rapid evolution of AI during the intelligence explosion will create a critical and uncertain period for ensuring AI safety.

--------

## Chunk 583

**Chunk:**

Dwarkesh Patel
You seem pretty optimistic about alignment. Let’s get to the source of the optimism. You laid out different worlds in which we could get AI. There's one that you think has a low probability of happening next year, where GPT-5 plus scaffolding plus unhobblings gets you to AGI. There are also scenarios where it takes much longer.

GPT-4 seems pretty aligned in the sense that I don't expect it to go off the rails. Maybe with scaffolding, things might change. It looks pretty good, and maybe you will keep turning the cranks, and one of them gets you to ASI.

Is there any point at which the sharp left turn happens? Do you think it's plausible that when they start acting more like agents, this is something to worry about? Is there anything qualitative that you expect to change with regards to the alignment perspective?
Leopold Aschenbrenner
I don't know if I believe in this concept of a sharp left turn, but there are important qualitative changes that happen between now and somewhat superhuman systems early on in the intelligence explosion. There are also important qualitative changes that occur from early in the intelligence explosion to true superintelligence in all its power and might.

Let's talk about both of those. The first part of the problem is one we're going to have to solve ourselves. We have to align the initial AI and the intelligence explosion, the sort of automated Alec Radford. There are two important things that change from GPT-4. If you believe the story on synthetic data RL, self-play, to get past the data wall, and if you believe this unhobbling story, at the end you're going to have things that are agents. They’ll do long-term planning. They have long horizons, which is a prerequisite to being able to do automated AI research.

Pre-training is alignment-neutral in the sense that it has good representations  and representations of doing bad things, but it's not scheming against you. Misalignment can arise once you're doing more long-horizon training. For example, if you're training an AI to make money using reinforcement learning, it might learn to commit fraud, lie, deceive, or seek power simply because those are successful strategies in the real world. With RL, it explores, maybe it tries to hack and then it gets some money. If that’s successful, that gets reward and that’s just reinforced. There’s more serious misalignments, like misaligned long-term goals, that necessarily have to be able to arise if you’re able to get long-horizon systems.

Let’s swap. What you want to do in that situation is add side constraints, like "don't lie," "don't deceive," or "don't commit fraud." How do you add those side constraints? The basic idea you might have is RLHF. You have this goal of making money, but you're watching what it's doing. If it starts trying to lie, deceive, commit fraud, or break the law, you give it a thumbs down and anti-reinforce that behavior.

The critical issue that arises is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. Even early on in the intelligence explosion, the automated AI researchers and engineers might write millions, billions, or trillions of lines of complicated code. You won't understand what they're doing anymore. In those millions of lines of code, you don't know if it's hacking, exfiltrating itself, or trying to go for the nukes.

You don’t know anymore. Thumbs up, thumbs down pure RLHF doesn't fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF but it’s a solvable problem. There’s various things I’m bullish on. There’s ways in which deep learning has shaped out favorably.

The second part of the picture is going from your initial systems in the intelligence explosion to superintelligence, many OOMs of improvement. By the end of it, you have a thing that's vastly smarter than humans. The intelligence explosion is really scary from an alignment point of view. If you have this rapid intelligence explosion in less than a year or two, you're going from systems where failure would be bad but not catastrophic to a world where if something goes awry, the AI could exfiltrate itself, start hacking the military, and do really bad things.

In less than a year, you're going from a world where the AI is some descendant of current systems that you understand and has good properties. It becomes something that potentially has a very alien and different architecture after having gone through another decade of ML advances. One salient example is legible and faithful chain-of-thought. A lot of the time when we're talking about these things, we're talking about how it has tokens of thinking and then uses many tokens of thinking. Maybe we bootstrap ourselves by pre-training it to learn to think in English, and then we do something else on top so it can do longer chains of thought.

It's very plausible to me that for the initial automated alignment researchers, we don't need to do any complicated mechanistic interpretability. You can just read what they're thinking, which is a huge advantage. However, it's very likely not the most efficient way to do it. There's probably some way to have a recurrent architecture with all internal states. That's a much more efficient way to do it.

That's what you get by the end of the year. You're going in this year from RLHF++ to something that's vastly superhuman. To us, it might be like an expert in the field compared to an elementary or middle school student. It’s an incredibly hairy period for alignment. The thing you do have is the automated AI researchers. You can use them to also do alignment.
Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.

**Extracted Belief:**

The thing you do have is the automated AI researchers. You can use them to also do alignment.

**Context:**

Leopold Aschenbrenner is suggesting that AI systems themselves can be used to help solve the AI alignment problem.

**Justification:**

This belief is based on the logical deduction that AI systems could be developed to assist in understanding and aligning other AI systems.

--------

## Chunk 584

**Chunk:**

Dwarkesh Patel
In this world, why are we optimistic that the project is being run by the right people? Here's something to think about. OpenAI starts off with people who are very explicitly thinking about exactly these kinds of things.
Leopold Aschenbrenner
Yes but are they still there?
Dwarkesh Patel
No, but here’s the thing. Even with the current leadership, you can find them in interviews and blog posts talking about it. You talked about what happens and it’s not just you. Jan talked about it in his Tweet thread. There is some trade-off that has to be made with doing a flashy release this week and not next week because Google I/O is next week or whatever. The trade-off is made in favor of the more careless decision.

The government, the national security advisor or the military or whatever, is much less familiar with this kind of discourse. They’re not like, “I'm worried the chain-of-thought is unfaithful. How do we think about the features that are represented here?” Why should we be optimistic that a project run by people like that will be thoughtful about these kinds of considerations?

**Extracted Belief:**

The current leadership at OpenAI still cares about alignment, even if they are not the original founders.

**Context:**

Dwarkesh Patel asks why we should be optimistic about AI alignment, given that the leadership at OpenAI has changed.

**Justification:**

Leopold Aschenbrenner acknowledges that OpenAI’s leadership has changed but says that the current leaders still express concerns about AI alignment in interviews and blog posts. He cites Jan LeCun's tweet thread as an example of this.

--------

## Chunk 585

**Chunk:**

Dwarkesh Patel
No, but here’s the thing. Even with the current leadership, you can find them in interviews and blog posts talking about it. You talked about what happens and it’s not just you. Jan talked about it in his Tweet thread. There is some trade-off that has to be made with doing a flashy release this week and not next week because Google I/O is next week or whatever. The trade-off is made in favor of the more careless decision.

The government, the national security advisor or the military or whatever, is much less familiar with this kind of discourse. They’re not like, “I'm worried the chain-of-thought is unfaithful. How do we think about the features that are represented here?” Why should we be optimistic that a project run by people like that will be thoughtful about these kinds of considerations?
Leopold Aschenbrenner
They might not be. Here’s a few thoughts. First of all, the private world is extremely tough for alignment even if they nominally care. There’s a couple of reasons. You have the race between the commercial labs. You don't have any headroom there to be like, aAh, actually we're going to hold back for three months, get this right. We're going to dedicate 90% of our compute to automated alignment research instead of just pushing the next OOM."

The other thing is that in the private world, China has stolen your weights. China has your secrets. They're right on your tails. You're in this fever struggle. There’s no room at all for maneuver. It's absolutely essential to get alignment right. To get it during this intelligence explosion, to get it right, you need to have that room to maneuver and you need to have that clear lead. Again, maybe you've made the deal or whatever, but you're in an incredibly tough spot if you don't have this clear lead.

So the private world is kind of rough there. Whether people will take it seriously… I have some faith normal mechanisms of a liberal society. Wwe don't fully know yet if alignment is an issue. The science will develop.  We're going to get better measurements of alignment. The case will be clear and obvious.

I worry that there's worlds where evidence is ambiguous. A lot of the most scary intelligence explosion scenarios are worlds in which evidence is ambiguous. But again, if evidence is ambiguous, then those are the worlds in which you really want the safety margins. Those are also the worlds in which running the intelligence explosion is sort of like running a war. The evidence is ambiguous. You have to make these really tough trade-offs. You better have a really good chain of command for that where they’re not just YOLOing it.

(03:41:26) – On Germany, and understanding foreign perspectives
Dwarkesh Patel
Let’s talk a little about Germany. We’re making the analogy to World War II. You made this. really interesting point many hours ago. Throughout history, World War Two is not unique at least when you think in proportion to the size of the population. Let’s look at these other sorts of catastrophes where a substantial proportion of the population has been killed off.

After that, the nation recovers and they get back to their heights. What's interesting after World War Two is that Germany especially, and maybe Europe as a whole, they experienced vast economic growth in the immediate aftermath because of catch-up growth.

We're not talking about Germany potentially launching an intelligence explosion and them getting a seat at the AI table. We were talking about Iran and North Korea and Russia. We didn't talk about Germany.

**Extracted Belief:**

The private world is extremely tough for alignment, even if commercial labs nominally care about it.

**Context:**

Leopold Aschenbrenner is discussing the challenges of achieving AI alignment in the context of the private sector, particularly in light of the competitive landscape and the presence of powerful actors like China.

**Justification:**

He cites the competitive nature of the commercial labs and the threat of China's technological advancements as key factors that make it difficult for companies to prioritize alignment over short-term gains and rapid progress.

--------

## Chunk 586

**Chunk:**

Dwarkesh Patel
No, but here’s the thing. Even with the current leadership, you can find them in interviews and blog posts talking about it. You talked about what happens and it’s not just you. Jan talked about it in his Tweet thread. There is some trade-off that has to be made with doing a flashy release this week and not next week because Google I/O is next week or whatever. The trade-off is made in favor of the more careless decision.

The government, the national security advisor or the military or whatever, is much less familiar with this kind of discourse. They’re not like, “I'm worried the chain-of-thought is unfaithful. How do we think about the features that are represented here?” Why should we be optimistic that a project run by people like that will be thoughtful about these kinds of considerations?
Leopold Aschenbrenner
They might not be. Here’s a few thoughts. First of all, the private world is extremely tough for alignment even if they nominally care. There’s a couple of reasons. You have the race between the commercial labs. You don't have any headroom there to be like, aAh, actually we're going to hold back for three months, get this right. We're going to dedicate 90% of our compute to automated alignment research instead of just pushing the next OOM."

The other thing is that in the private world, China has stolen your weights. China has your secrets. They're right on your tails. You're in this fever struggle. There’s no room at all for maneuver. It's absolutely essential to get alignment right. To get it during this intelligence explosion, to get it right, you need to have that room to maneuver and you need to have that clear lead. Again, maybe you've made the deal or whatever, but you're in an incredibly tough spot if you don't have this clear lead.

So the private world is kind of rough there. Whether people will take it seriously… I have some faith normal mechanisms of a liberal society. Wwe don't fully know yet if alignment is an issue. The science will develop.  We're going to get better measurements of alignment. The case will be clear and obvious.

I worry that there's worlds where evidence is ambiguous. A lot of the most scary intelligence explosion scenarios are worlds in which evidence is ambiguous. But again, if evidence is ambiguous, then those are the worlds in which you really want the safety margins. Those are also the worlds in which running the intelligence explosion is sort of like running a war. The evidence is ambiguous. You have to make these really tough trade-offs. You better have a really good chain of command for that where they’re not just YOLOing it.

(03:41:26) – On Germany, and understanding foreign perspectives
Dwarkesh Patel
Let’s talk a little about Germany. We’re making the analogy to World War II. You made this. really interesting point many hours ago. Throughout history, World War Two is not unique at least when you think in proportion to the size of the population. Let’s look at these other sorts of catastrophes where a substantial proportion of the population has been killed off.

After that, the nation recovers and they get back to their heights. What's interesting after World War Two is that Germany especially, and maybe Europe as a whole, they experienced vast economic growth in the immediate aftermath because of catch-up growth.

We're not talking about Germany potentially launching an intelligence explosion and them getting a seat at the AI table. We were talking about Iran and North Korea and Russia. We didn't talk about Germany.

**Extracted Belief:**

To achieve proper alignment during the intelligence explosion, it is essential to have the freedom to maneuver and a clear lead over competitors.

**Context:**

Leopold Aschenbrenner argues that the private sector's competitive dynamics and the need to stay ahead of rivals hinder efforts to prioritize AI alignment.

**Justification:**

He states that having a clear lead and freedom to maneuver are crucial for prioritizing AI alignment over short-term gains and the need to keep up with competitors.

--------

## Chunk 587

**Chunk:**

Dwarkesh Patel
No, but here’s the thing. Even with the current leadership, you can find them in interviews and blog posts talking about it. You talked about what happens and it’s not just you. Jan talked about it in his Tweet thread. There is some trade-off that has to be made with doing a flashy release this week and not next week because Google I/O is next week or whatever. The trade-off is made in favor of the more careless decision.

The government, the national security advisor or the military or whatever, is much less familiar with this kind of discourse. They’re not like, “I'm worried the chain-of-thought is unfaithful. How do we think about the features that are represented here?” Why should we be optimistic that a project run by people like that will be thoughtful about these kinds of considerations?
Leopold Aschenbrenner
They might not be. Here’s a few thoughts. First of all, the private world is extremely tough for alignment even if they nominally care. There’s a couple of reasons. You have the race between the commercial labs. You don't have any headroom there to be like, aAh, actually we're going to hold back for three months, get this right. We're going to dedicate 90% of our compute to automated alignment research instead of just pushing the next OOM."

The other thing is that in the private world, China has stolen your weights. China has your secrets. They're right on your tails. You're in this fever struggle. There’s no room at all for maneuver. It's absolutely essential to get alignment right. To get it during this intelligence explosion, to get it right, you need to have that room to maneuver and you need to have that clear lead. Again, maybe you've made the deal or whatever, but you're in an incredibly tough spot if you don't have this clear lead.

So the private world is kind of rough there. Whether people will take it seriously… I have some faith normal mechanisms of a liberal society. Wwe don't fully know yet if alignment is an issue. The science will develop.  We're going to get better measurements of alignment. The case will be clear and obvious.

I worry that there's worlds where evidence is ambiguous. A lot of the most scary intelligence explosion scenarios are worlds in which evidence is ambiguous. But again, if evidence is ambiguous, then those are the worlds in which you really want the safety margins. Those are also the worlds in which running the intelligence explosion is sort of like running a war. The evidence is ambiguous. You have to make these really tough trade-offs. You better have a really good chain of command for that where they’re not just YOLOing it.

(03:41:26) – On Germany, and understanding foreign perspectives
Dwarkesh Patel
Let’s talk a little about Germany. We’re making the analogy to World War II. You made this. really interesting point many hours ago. Throughout history, World War Two is not unique at least when you think in proportion to the size of the population. Let’s look at these other sorts of catastrophes where a substantial proportion of the population has been killed off.

After that, the nation recovers and they get back to their heights. What's interesting after World War Two is that Germany especially, and maybe Europe as a whole, they experienced vast economic growth in the immediate aftermath because of catch-up growth.

We're not talking about Germany potentially launching an intelligence explosion and them getting a seat at the AI table. We were talking about Iran and North Korea and Russia. We didn't talk about Germany.

**Extracted Belief:**

Alignment will become a clearer and more obvious issue as scientific understanding of AI evolves and more accurate measurements of alignment are developed.

**Context:**

Leopold Aschenbrenner expresses his faith in the scientific progress and the ability of researchers to develop a better understanding of alignment.

**Justification:**

He believes that the development of better measurements and understanding of AI alignment will make the case for its importance more clear and obvious.

--------

## Chunk 588

**Chunk:**

Dwarkesh Patel
No, but here’s the thing. Even with the current leadership, you can find them in interviews and blog posts talking about it. You talked about what happens and it’s not just you. Jan talked about it in his Tweet thread. There is some trade-off that has to be made with doing a flashy release this week and not next week because Google I/O is next week or whatever. The trade-off is made in favor of the more careless decision.

The government, the national security advisor or the military or whatever, is much less familiar with this kind of discourse. They’re not like, “I'm worried the chain-of-thought is unfaithful. How do we think about the features that are represented here?” Why should we be optimistic that a project run by people like that will be thoughtful about these kinds of considerations?
Leopold Aschenbrenner
They might not be. Here’s a few thoughts. First of all, the private world is extremely tough for alignment even if they nominally care. There’s a couple of reasons. You have the race between the commercial labs. You don't have any headroom there to be like, aAh, actually we're going to hold back for three months, get this right. We're going to dedicate 90% of our compute to automated alignment research instead of just pushing the next OOM."

The other thing is that in the private world, China has stolen your weights. China has your secrets. They're right on your tails. You're in this fever struggle. There’s no room at all for maneuver. It's absolutely essential to get alignment right. To get it during this intelligence explosion, to get it right, you need to have that room to maneuver and you need to have that clear lead. Again, maybe you've made the deal or whatever, but you're in an incredibly tough spot if you don't have this clear lead.

So the private world is kind of rough there. Whether people will take it seriously… I have some faith normal mechanisms of a liberal society. Wwe don't fully know yet if alignment is an issue. The science will develop.  We're going to get better measurements of alignment. The case will be clear and obvious.

I worry that there's worlds where evidence is ambiguous. A lot of the most scary intelligence explosion scenarios are worlds in which evidence is ambiguous. But again, if evidence is ambiguous, then those are the worlds in which you really want the safety margins. Those are also the worlds in which running the intelligence explosion is sort of like running a war. The evidence is ambiguous. You have to make these really tough trade-offs. You better have a really good chain of command for that where they’re not just YOLOing it.

(03:41:26) – On Germany, and understanding foreign perspectives
Dwarkesh Patel
Let’s talk a little about Germany. We’re making the analogy to World War II. You made this. really interesting point many hours ago. Throughout history, World War Two is not unique at least when you think in proportion to the size of the population. Let’s look at these other sorts of catastrophes where a substantial proportion of the population has been killed off.

After that, the nation recovers and they get back to their heights. What's interesting after World War Two is that Germany especially, and maybe Europe as a whole, they experienced vast economic growth in the immediate aftermath because of catch-up growth.

We're not talking about Germany potentially launching an intelligence explosion and them getting a seat at the AI table. We were talking about Iran and North Korea and Russia. We didn't talk about Germany.

**Extracted Belief:**

There are scenarios where evidence surrounding AI capabilities and risks is ambiguous, making it crucial to have robust safety margins.

**Context:**

Leopold Aschenbrenner discusses the potential for ambiguity in evidence surrounding AI capabilities and risks, highlighting the importance of safety margins in such scenarios.

**Justification:**

He argues that in scenarios where evidence is ambiguous, having large safety margins is crucial to mitigating potential risks associated with AI systems.

--------

## Chunk 589

**Chunk:**

Dwarkesh Patel
Let’s talk a little about Germany. We’re making the analogy to World War II. You made this. really interesting point many hours ago. Throughout history, World War Two is not unique at least when you think in proportion to the size of the population. Let’s look at these other sorts of catastrophes where a substantial proportion of the population has been killed off.

After that, the nation recovers and they get back to their heights. What's interesting after World War Two is that Germany especially, and maybe Europe as a whole, they experienced vast economic growth in the immediate aftermath because of catch-up growth.

We're not talking about Germany potentially launching an intelligence explosion and them getting a seat at the AI table. We were talking about Iran and North Korea and Russia. We didn't talk about Germany.
Leopold Aschenbrenner
Because they're allies.
Dwarkesh Patel
So what happened? We had World War Two and now it didn't come back to the Seven Years' War or something.

**Extracted Belief:**

Germany is an ally of the United States.

**Context:**

Leopold Aschenbrenner explains why Germany was not mentioned in the discussion about countries that might launch an intelligence explosion.

**Justification:**

Leopold Aschenbrenner states that Germany is an ally.

--------

## Chunk 590

**Chunk:**

Dwarkesh Patel
So what happened? We had World War Two and now it didn't come back to the Seven Years' War or something.
Leopold Aschenbrenner
I'm generally very bearish on Germany. In this context, you're underrating a little bit. It's probably still one of the top five most important countries in the world. Europe overall still has a GDP that's close to the United States in size. There are things that Germany is actually kind of good at, like state capacity. The roads are good and they're clean and they're well-maintained.

In some sense, a lot of this is the flip side of things that are bad about Germany. In the US, there's a bit more of a Wild West feeling. It includes the kind of crazy bursts of creativity. It includes political candidates. There's a much broader spectrum. Both Obama and Trump are politicians you just wouldn't see in the much more confined kind of German political debate. I wrote a blog post at some point about this, “Europe’s Political Stupor.”

There's this punctilious sort of rule-following that is good in terms of keeping your state capacity functioning. But that is also a very constrained view of the world in some sense. After World War Two, there's a real backlash against anything elite. There are no elite high schools or elite colleges. Excellence isn't cherished.
Dwarkesh Patel
Why is that the logical, intellectual thing to rebel against if you're trying to overcorrect from the Nazis? Was it because the Nazis were very much into elitism? I don't understand why that's a logical sort of reaction.

**Extracted Belief:**

Germany is still one of the top five most important countries in the world.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's suggestion that Germany is not as important as it used to be, and he argues that Germany's economic power and influence are still significant.

**Justification:**

Aschenbrenner cites Europe's GDP as comparable to the United States, suggesting that Germany, as a significant part of Europe, remains a major economic force.

--------

## Chunk 591

**Chunk:**

Dwarkesh Patel
So what happened? We had World War Two and now it didn't come back to the Seven Years' War or something.
Leopold Aschenbrenner
I'm generally very bearish on Germany. In this context, you're underrating a little bit. It's probably still one of the top five most important countries in the world. Europe overall still has a GDP that's close to the United States in size. There are things that Germany is actually kind of good at, like state capacity. The roads are good and they're clean and they're well-maintained.

In some sense, a lot of this is the flip side of things that are bad about Germany. In the US, there's a bit more of a Wild West feeling. It includes the kind of crazy bursts of creativity. It includes political candidates. There's a much broader spectrum. Both Obama and Trump are politicians you just wouldn't see in the much more confined kind of German political debate. I wrote a blog post at some point about this, “Europe’s Political Stupor.”

There's this punctilious sort of rule-following that is good in terms of keeping your state capacity functioning. But that is also a very constrained view of the world in some sense. After World War Two, there's a real backlash against anything elite. There are no elite high schools or elite colleges. Excellence isn't cherished.
Dwarkesh Patel
Why is that the logical, intellectual thing to rebel against if you're trying to overcorrect from the Nazis? Was it because the Nazis were very much into elitism? I don't understand why that's a logical sort of reaction.

**Extracted Belief:**

Germany is good at state capacity, as evidenced by its well-maintained roads and infrastructure.

**Context:**

Aschenbrenner is arguing that Germany's strength lies in its effective government and public services, which he attributes to its strong state capacity.

**Justification:**

He specifically mentions the quality of German roads as a positive example of state capacity.

--------

## Chunk 592

**Chunk:**

Dwarkesh Patel
So what happened? We had World War Two and now it didn't come back to the Seven Years' War or something.
Leopold Aschenbrenner
I'm generally very bearish on Germany. In this context, you're underrating a little bit. It's probably still one of the top five most important countries in the world. Europe overall still has a GDP that's close to the United States in size. There are things that Germany is actually kind of good at, like state capacity. The roads are good and they're clean and they're well-maintained.

In some sense, a lot of this is the flip side of things that are bad about Germany. In the US, there's a bit more of a Wild West feeling. It includes the kind of crazy bursts of creativity. It includes political candidates. There's a much broader spectrum. Both Obama and Trump are politicians you just wouldn't see in the much more confined kind of German political debate. I wrote a blog post at some point about this, “Europe’s Political Stupor.”

There's this punctilious sort of rule-following that is good in terms of keeping your state capacity functioning. But that is also a very constrained view of the world in some sense. After World War Two, there's a real backlash against anything elite. There are no elite high schools or elite colleges. Excellence isn't cherished.
Dwarkesh Patel
Why is that the logical, intellectual thing to rebel against if you're trying to overcorrect from the Nazis? Was it because the Nazis were very much into elitism? I don't understand why that's a logical sort of reaction.

**Extracted Belief:**

German political culture is more constrained and less diverse than the United States, leading to a less dynamic and less creative environment.

**Context:**

Aschenbrenner is contrasting Germany's political culture with that of the United States, suggesting that Germany's more structured approach limits political and ideological diversity.

**Justification:**

He uses the examples of Barack Obama and Donald Trump as politicians who would not be seen in German politics, highlighting the narrower range of political discourse in Germany.

--------

## Chunk 593

**Chunk:**

Dwarkesh Patel
So what happened? We had World War Two and now it didn't come back to the Seven Years' War or something.
Leopold Aschenbrenner
I'm generally very bearish on Germany. In this context, you're underrating a little bit. It's probably still one of the top five most important countries in the world. Europe overall still has a GDP that's close to the United States in size. There are things that Germany is actually kind of good at, like state capacity. The roads are good and they're clean and they're well-maintained.

In some sense, a lot of this is the flip side of things that are bad about Germany. In the US, there's a bit more of a Wild West feeling. It includes the kind of crazy bursts of creativity. It includes political candidates. There's a much broader spectrum. Both Obama and Trump are politicians you just wouldn't see in the much more confined kind of German political debate. I wrote a blog post at some point about this, “Europe’s Political Stupor.”

There's this punctilious sort of rule-following that is good in terms of keeping your state capacity functioning. But that is also a very constrained view of the world in some sense. After World War Two, there's a real backlash against anything elite. There are no elite high schools or elite colleges. Excellence isn't cherished.
Dwarkesh Patel
Why is that the logical, intellectual thing to rebel against if you're trying to overcorrect from the Nazis? Was it because the Nazis were very much into elitism? I don't understand why that's a logical sort of reaction.

**Extracted Belief:**

Germany's culture of rule-following and state capacity, while beneficial in some ways, can also lead to a constrained and narrow worldview.

**Context:**

Aschenbrenner is discussing the potential drawbacks of Germany's strict adherence to rules and procedures, suggesting that it can limit creativity and innovation.

**Justification:**

He argues that the emphasis on rules and order, while beneficial for state capacity, can also lead to a narrow and inflexible view of the world.

--------

## Chunk 594

**Chunk:**

Dwarkesh Patel
So what happened? We had World War Two and now it didn't come back to the Seven Years' War or something.
Leopold Aschenbrenner
I'm generally very bearish on Germany. In this context, you're underrating a little bit. It's probably still one of the top five most important countries in the world. Europe overall still has a GDP that's close to the United States in size. There are things that Germany is actually kind of good at, like state capacity. The roads are good and they're clean and they're well-maintained.

In some sense, a lot of this is the flip side of things that are bad about Germany. In the US, there's a bit more of a Wild West feeling. It includes the kind of crazy bursts of creativity. It includes political candidates. There's a much broader spectrum. Both Obama and Trump are politicians you just wouldn't see in the much more confined kind of German political debate. I wrote a blog post at some point about this, “Europe’s Political Stupor.”

There's this punctilious sort of rule-following that is good in terms of keeping your state capacity functioning. But that is also a very constrained view of the world in some sense. After World War Two, there's a real backlash against anything elite. There are no elite high schools or elite colleges. Excellence isn't cherished.
Dwarkesh Patel
Why is that the logical, intellectual thing to rebel against if you're trying to overcorrect from the Nazis? Was it because the Nazis were very much into elitism? I don't understand why that's a logical sort of reaction.

**Extracted Belief:**

Germany's post-World War II culture of rejecting elite institutions and excellences was a reaction against the Nazi regime and its emphasis on elitism.

**Context:**

Aschenbrenner is explaining the roots of Germany's aversion to elite institutions and the lack of focus on excellence in education.

**Justification:**

He suggests that the rejection of elitism was a response to the Nazi regime's focus on Aryan supremacy and its use of elitism to justify its actions.

--------

## Chunk 595

**Chunk:**

Dwarkesh Patel
Why is that the logical, intellectual thing to rebel against if you're trying to overcorrect from the Nazis? Was it because the Nazis were very much into elitism? I don't understand why that's a logical sort of reaction.
Leopold Aschenbrenner
Maybe it was a counter reaction against the whole Aryan race and that sort of thing. Look at the end of World War I versus the end of World War II for Germany. A common narrative is that the Peace of Versailles was too strict on Germany. The peace imposed after World War Two was much more strict.

The whole country was destroyed. In most of the major cities, over half of the housing stock had been destroyed. In some birth cohorts, something like 40% of the men had died, Almost 20 million people displaced. It was huge and crazy.
Dwarkesh Patel
And the borders are way smaller than the Versailles borders.

**Extracted Belief:**

The Peace of Versailles, imposed on Germany after World War I, was excessively harsh.

**Context:**

Leopold Aschenbrenner is comparing the peace treaties imposed after World War I and World War II, stating that the one after World War II was much more severe.

**Justification:**

Leopold Aschenbrenner mentions a "common narrative" that the Peace of Versailles was too strict, implying that it is a widely held belief. This narrative suggests that the treaty contributed to the rise of German nationalism and ultimately led to World War II.

--------

## Chunk 596

**Chunk:**

Dwarkesh Patel
Why is that the logical, intellectual thing to rebel against if you're trying to overcorrect from the Nazis? Was it because the Nazis were very much into elitism? I don't understand why that's a logical sort of reaction.
Leopold Aschenbrenner
Maybe it was a counter reaction against the whole Aryan race and that sort of thing. Look at the end of World War I versus the end of World War II for Germany. A common narrative is that the Peace of Versailles was too strict on Germany. The peace imposed after World War Two was much more strict.

The whole country was destroyed. In most of the major cities, over half of the housing stock had been destroyed. In some birth cohorts, something like 40% of the men had died, Almost 20 million people displaced. It was huge and crazy.
Dwarkesh Patel
And the borders are way smaller than the Versailles borders.

**Extracted Belief:**

The peace treaty imposed on Germany after World War II was extremely harsh and damaging.

**Context:**

Leopold Aschenbrenner is describing the devastation caused by World War II and its consequences for Germany.

**Justification:**

He provides specific evidence of the destruction, including the loss of housing stock, the high death toll among men, and the displacement of millions of people. This evidence suggests a severe level of destruction and hardship caused by the war and subsequent peace treaty.

--------

## Chunk 597

**Chunk:**

Dwarkesh Patel
And the borders are way smaller than the Versailles borders.
Leopold Aschenbrenner
Yeah, exactly. There’s also a complete imposition of a new political system on both sides. But in some sense, that worked out better than the post-World War I peace where there was this resurgence of German nationalism. In some sense, it's unclear if you want to wake the sleeping beast. At this point, it's gotten a bit too sleepy.
Dwarkesh Patel
It's an interesting point about how we underrate the American political system. I've been making the same correction myself. There was this book written by a Chinese economist called China's World View.

Overall, I wasn't a big fan, but it made a really interesting point, which was the way in which candidates rise up through the Chinese hierarchy for politics and administration. In some sense, it selects so that you're not going to get some Marjorie Taylor Greene or someone like that.

**Extracted Belief:**

The complete imposition of a new political system on Germany after World War II was ultimately better than the post-World War I peace, which saw a resurgence of German nationalism.

**Context:**

Leopold Aschenbrenner was comparing the political consequences of the two world wars on Germany, arguing that the post-WWII approach was more successful in preventing future aggression.

**Justification:**

Aschenbrenner suggests that the new political system in Germany after World War II effectively curbed the rise of German nationalism that was present after World War I.

--------

## Chunk 598

**Chunk:**

Dwarkesh Patel
And the borders are way smaller than the Versailles borders.
Leopold Aschenbrenner
Yeah, exactly. There’s also a complete imposition of a new political system on both sides. But in some sense, that worked out better than the post-World War I peace where there was this resurgence of German nationalism. In some sense, it's unclear if you want to wake the sleeping beast. At this point, it's gotten a bit too sleepy.
Dwarkesh Patel
It's an interesting point about how we underrate the American political system. I've been making the same correction myself. There was this book written by a Chinese economist called China's World View.

Overall, I wasn't a big fan, but it made a really interesting point, which was the way in which candidates rise up through the Chinese hierarchy for politics and administration. In some sense, it selects so that you're not going to get some Marjorie Taylor Greene or someone like that.

**Extracted Belief:**

It is uncertain whether it is desirable to 'wake the sleeping beast' of German nationalism, suggesting that it is better to keep it dormant.

**Context:**

Aschenbrenner uses the metaphor of a 'sleeping beast' to refer to the potential for resurgence of German nationalism and suggests that it is safer to keep it subdued.

**Justification:**

Aschenbrenner implies that German nationalism, if unleashed, could potentially pose a threat, highlighting the dangers of its reemergence.

--------

## Chunk 599

**Chunk:**

Dwarkesh Patel
It's an interesting point about how we underrate the American political system. I've been making the same correction myself. There was this book written by a Chinese economist called China's World View.

Overall, I wasn't a big fan, but it made a really interesting point, which was the way in which candidates rise up through the Chinese hierarchy for politics and administration. In some sense, it selects so that you're not going to get some Marjorie Taylor Greene or someone like that.
Leopold Aschenbrenner
Don't get that in Germany either.
Dwarkesh Patel
But he explicitly made the point in the book that it also means they’re never going to get a Henry Kissinger or Barack Obama in China. By the time they end up in charge of the Politburo, they'll be some 60-year-old bureaucrat who's never ruffled any feathers.

**Extracted Belief:**

Germany does not have politicians similar to Marjorie Taylor Greene.

**Context:**

Leopold Aschenbrenner agrees with Dwarkesh Patel's observation about the Chinese political system selecting for candidates who are not extreme, like Marjorie Taylor Greene in the US, and states that this is also true in Germany.

**Justification:**

Aschenbrenner states "Don't get that in Germany either," directly referencing the American politician Marjorie Taylor Greene and implying that Germany does not have politicians with similar extreme views.

--------

## Chunk 600

**Chunk:**

Dwarkesh Patel
But he explicitly made the point in the book that it also means they’re never going to get a Henry Kissinger or Barack Obama in China. By the time they end up in charge of the Politburo, they'll be some 60-year-old bureaucrat who's never ruffled any feathers.
Leopold Aschenbrenner
There's something really important about the very raucous political debate in the US. In general in America lots of people live in their own world. We live in this kind of bizarre little bubble in San Francisco and people. But that's important for the evolution of ideas, error correction, that sort of thing.

There are other ways in which the German system is more functional. There are also major mistakes, like with defense spending. Russia invades Ukraine and they’re like, "wow, what did we do?"
Dwarkesh Patel
That's a really good point. The main issue is that everybody agrees.

**Extracted Belief:**

The raucous political debate in the US is important for the evolution of ideas and error correction.

**Context:**

Leopold Aschenbrenner was discussing the difference between the US and Chinese political systems and arguing that the US system, despite its flaws, is better for the evolution of ideas and error correction.

**Justification:**

This belief is based on Leopold Aschenbrenner's personal opinion and experience.

--------

## Chunk 601

**Chunk:**

Dwarkesh Patel
But he explicitly made the point in the book that it also means they’re never going to get a Henry Kissinger or Barack Obama in China. By the time they end up in charge of the Politburo, they'll be some 60-year-old bureaucrat who's never ruffled any feathers.
Leopold Aschenbrenner
There's something really important about the very raucous political debate in the US. In general in America lots of people live in their own world. We live in this kind of bizarre little bubble in San Francisco and people. But that's important for the evolution of ideas, error correction, that sort of thing.

There are other ways in which the German system is more functional. There are also major mistakes, like with defense spending. Russia invades Ukraine and they’re like, "wow, what did we do?"
Dwarkesh Patel
That's a really good point. The main issue is that everybody agrees.

**Extracted Belief:**

The German political system is more functional in some ways than the US system.

**Context:**

Leopold Aschenbrenner was contrasting the German and US political systems.

**Justification:**

This belief is based on Leopold Aschenbrenner's personal opinion and experience.

--------

## Chunk 602

**Chunk:**

Dwarkesh Patel
But he explicitly made the point in the book that it also means they’re never going to get a Henry Kissinger or Barack Obama in China. By the time they end up in charge of the Politburo, they'll be some 60-year-old bureaucrat who's never ruffled any feathers.
Leopold Aschenbrenner
There's something really important about the very raucous political debate in the US. In general in America lots of people live in their own world. We live in this kind of bizarre little bubble in San Francisco and people. But that's important for the evolution of ideas, error correction, that sort of thing.

There are other ways in which the German system is more functional. There are also major mistakes, like with defense spending. Russia invades Ukraine and they’re like, "wow, what did we do?"
Dwarkesh Patel
That's a really good point. The main issue is that everybody agrees.

**Extracted Belief:**

The German government made a major mistake in their defense spending, which has had negative consequences in the face of the Russian invasion of Ukraine.

**Context:**

Leopold Aschenbrenner was highlighting a specific example of a mistake made by the German political system.

**Justification:**

This belief is based on Leopold Aschenbrenner's personal opinion and experience.

--------

## Chunk 603

**Chunk:**

Dwarkesh Patel
That's a really good point. The main issue is that everybody agrees.
Leopold Aschenbrenner
Exactly. There was no debate about it. It’s a consensus Blob kind of thing.

On the China point, I have this experience of reading German newspapers and I would understand the German debate and the state of mind much more poorly without it from just afar. It is interesting just how impenetrable China is to me. It's a billion people.

Almost everything else is really globalized. You have a globalized Internet. I kind of have a sense what's happening in the UK. Even if I didn't read German newspapers, I would have a sense of what's happening in Germany. But I really don't feel like I have a sense of what is the state of mind, what is the state of political debate, of an average Chinese person or an average Chinese elite.

I find that distance kind of worrying. There are some people who do this and they do really great work where they go through the party documents and the party speeches. It seems to require a lot of interpretive ability. There are very specific words in Mandarin that mean one connotation, not the other connotation. It's interesting given how globalized everything is. Now we have basically perfect translation machines and it's still so impenetrable.
Dwarkesh Patel
That's really interesting. I'm sort of ashamed almost that I haven't done this yet. Many months ago when Alexey interviewed me on his YouTube channel, I said, "I'm meaning to go to China to actually see for myself what's going on." By the way, if anybody listening has a lot of context on China and if I went to China, could introduce me to people, please email me.

**Extracted Belief:**

The political debate in Germany lacks diversity of opinion and tends towards a consensus.

**Context:**

Leopold Aschenbrenner expresses his belief that German political discourse lacks debate and is dominated by consensus.

**Justification:**

He describes the German political system as a "consensus Blob" and states that "there was no debate" about defense spending in the context of Russia invading Ukraine.

--------

## Chunk 604

**Chunk:**

Dwarkesh Patel
That's a really good point. The main issue is that everybody agrees.
Leopold Aschenbrenner
Exactly. There was no debate about it. It’s a consensus Blob kind of thing.

On the China point, I have this experience of reading German newspapers and I would understand the German debate and the state of mind much more poorly without it from just afar. It is interesting just how impenetrable China is to me. It's a billion people.

Almost everything else is really globalized. You have a globalized Internet. I kind of have a sense what's happening in the UK. Even if I didn't read German newspapers, I would have a sense of what's happening in Germany. But I really don't feel like I have a sense of what is the state of mind, what is the state of political debate, of an average Chinese person or an average Chinese elite.

I find that distance kind of worrying. There are some people who do this and they do really great work where they go through the party documents and the party speeches. It seems to require a lot of interpretive ability. There are very specific words in Mandarin that mean one connotation, not the other connotation. It's interesting given how globalized everything is. Now we have basically perfect translation machines and it's still so impenetrable.
Dwarkesh Patel
That's really interesting. I'm sort of ashamed almost that I haven't done this yet. Many months ago when Alexey interviewed me on his YouTube channel, I said, "I'm meaning to go to China to actually see for myself what's going on." By the way, if anybody listening has a lot of context on China and if I went to China, could introduce me to people, please email me.

**Extracted Belief:**

Understanding the political climate and state of mind of the average Chinese person and elite is difficult due to cultural and linguistic barriers.

**Context:**

Leopold Aschenbrenner expresses his difficulty in comprehending the political landscape in China.

**Justification:**

He highlights the challenges of interpreting Chinese language and culture, even with advancements in translation technology. He cites the complexity of Mandarin and its nuances, making it "impenetrable" to him.

--------

## Chunk 605

**Chunk:**

Dwarkesh Patel
That's a really good point. The main issue is that everybody agrees.
Leopold Aschenbrenner
Exactly. There was no debate about it. It’s a consensus Blob kind of thing.

On the China point, I have this experience of reading German newspapers and I would understand the German debate and the state of mind much more poorly without it from just afar. It is interesting just how impenetrable China is to me. It's a billion people.

Almost everything else is really globalized. You have a globalized Internet. I kind of have a sense what's happening in the UK. Even if I didn't read German newspapers, I would have a sense of what's happening in Germany. But I really don't feel like I have a sense of what is the state of mind, what is the state of political debate, of an average Chinese person or an average Chinese elite.

I find that distance kind of worrying. There are some people who do this and they do really great work where they go through the party documents and the party speeches. It seems to require a lot of interpretive ability. There are very specific words in Mandarin that mean one connotation, not the other connotation. It's interesting given how globalized everything is. Now we have basically perfect translation machines and it's still so impenetrable.
Dwarkesh Patel
That's really interesting. I'm sort of ashamed almost that I haven't done this yet. Many months ago when Alexey interviewed me on his YouTube channel, I said, "I'm meaning to go to China to actually see for myself what's going on." By the way, if anybody listening has a lot of context on China and if I went to China, could introduce me to people, please email me.

**Extracted Belief:**

Interpreting Chinese party documents and speeches requires a significant level of interpretive skill.

**Context:**

Leopold Aschenbrenner acknowledges the specialized expertise required to understand Chinese political discourse.

**Justification:**

He mentions people who "do really great work" by analyzing party documents and speeches, highlighting the need for specific skills and knowledge to effectively interpret them.

--------

## Chunk 606

**Chunk:**

Dwarkesh Patel
That's really interesting. I'm sort of ashamed almost that I haven't done this yet. Many months ago when Alexey interviewed me on his YouTube channel, I said, "I'm meaning to go to China to actually see for myself what's going on." By the way, if anybody listening has a lot of context on China and if I went to China, could introduce me to people, please email me.
Leopold Aschenbrenner
You have to do some pods and find some of the Chinese AI researchers. It’d be so good. I don't know if they can speak freely.
Dwarkesh Patel
So they have these papers and on the paper they'll say who's a co-author. I was thinking of just cold emailing everybody, like, "Here's my Calendly. Let's just talk.” I just want to see what the vibe is. Even if they don't tell me anything, I'm just like, “what kind of person is this? How westernized are they?"

I just remembered that, in fact, ByteDance, according to mutual friends we have at Google, cold emailed every single person on the Gemini paper and said, "if you come work for ByteDance, we'll make you a L8 engineer. You'll report directly to the CTO."

**Extracted Belief:**

Chinese AI researchers may not be able to speak freely about their work.

**Context:**

Leopold Aschenbrenner responded to Dwarkesh Patel's suggestion to interview Chinese AI researchers by expressing uncertainty about their ability to speak freely.

**Justification:**

Leopold Aschenbrenner stated, "I don't know if they can speak freely."

--------

## Chunk 607

**Chunk:**

Dwarkesh Patel
So they have these papers and on the paper they'll say who's a co-author. I was thinking of just cold emailing everybody, like, "Here's my Calendly. Let's just talk.” I just want to see what the vibe is. Even if they don't tell me anything, I'm just like, “what kind of person is this? How westernized are they?"

I just remembered that, in fact, ByteDance, according to mutual friends we have at Google, cold emailed every single person on the Gemini paper and said, "if you come work for ByteDance, we'll make you a L8 engineer. You'll report directly to the CTO."
Leopold Aschenbrenner
That's how the secrets go over.
Dwarkesh Patel
I meant to ask this earlier. If there's only 100 or so people, maybe less, who are working on the key algorithmic secrets. If they hired one such person, is all the alpha that these labs have gone?

**Extracted Belief:**

The hiring of a single individual with knowledge of key algorithmic secrets in AI could potentially transfer significant intellectual property to a competitor.

**Context:**

Leopold Aschenbrenner was responding to Dwarkesh Patel's question about the potential impact of hiring one of the limited number of people working on key algorithmic secrets in AI.

**Justification:**

The justification for this belief is based on Leopold Aschenbrenner's experience in the field and his understanding of the value and scarcity of key AI research.

--------

## Chunk 608

**Chunk:**

Dwarkesh Patel
So they have these papers and on the paper they'll say who's a co-author. I was thinking of just cold emailing everybody, like, "Here's my Calendly. Let's just talk.” I just want to see what the vibe is. Even if they don't tell me anything, I'm just like, “what kind of person is this? How westernized are they?"

I just remembered that, in fact, ByteDance, according to mutual friends we have at Google, cold emailed every single person on the Gemini paper and said, "if you come work for ByteDance, we'll make you a L8 engineer. You'll report directly to the CTO."
Leopold Aschenbrenner
That's how the secrets go over.
Dwarkesh Patel
I meant to ask this earlier. If there's only 100 or so people, maybe less, who are working on the key algorithmic secrets. If they hired one such person, is all the alpha that these labs have gone?

**Extracted Belief:**

It is possible to transfer or exfiltrate critical AI code or research ideas by recruiting individuals with access to such information.

**Context:**

Leopold Aschenbrenner was explaining the potential consequences of a competitor hiring an individual with access to key AI algorithms.

**Justification:**

Leopold Aschenbrenner's experience in the field informs this belief, suggesting that the transfer of critical AI information is achievable through individual recruitment.

--------

## Chunk 609

**Chunk:**

Dwarkesh Patel
I meant to ask this earlier. If there's only 100 or so people, maybe less, who are working on the key algorithmic secrets. If they hired one such person, is all the alpha that these labs have gone?
Leopold Aschenbrenner
If this person was intentional about it, they could get a lot. Actually, they could probably just exfiltrate the code. They could get a lot of the key ideas. Again up until recently, stuff was published but they could get a lot of the key ideas if they tried. There are a lot of people who don't actually look around to see what the other teams are doing. But you kind of can. They could. It's scary.
Dwarkesh Patel
The project makes more sense there, where you can't just recruit a Manhattan Project engineer.

**Extracted Belief:**

A single individual with access to key algorithmic secrets in AI research could significantly impact the field by extracting and sharing this knowledge.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's question about the potential impact of hiring a single person with access to key algorithmic secrets in AI research.

**Justification:**

Aschenbrenner argues that an individual with intent could exfiltrate code and key ideas, implying that the knowledge is transferable and valuable.

--------

## Chunk 610

**Chunk:**

Dwarkesh Patel
I meant to ask this earlier. If there's only 100 or so people, maybe less, who are working on the key algorithmic secrets. If they hired one such person, is all the alpha that these labs have gone?
Leopold Aschenbrenner
If this person was intentional about it, they could get a lot. Actually, they could probably just exfiltrate the code. They could get a lot of the key ideas. Again up until recently, stuff was published but they could get a lot of the key ideas if they tried. There are a lot of people who don't actually look around to see what the other teams are doing. But you kind of can. They could. It's scary.
Dwarkesh Patel
The project makes more sense there, where you can't just recruit a Manhattan Project engineer.

**Extracted Belief:**

Many individuals working on AI research do not actively seek out knowledge from other teams within the field.

**Context:**

Aschenbrenner is describing the current landscape of AI research, highlighting a potential vulnerability.

**Justification:**

He states that "There are a lot of people who don't actually look around to see what the other teams are doing." This suggests his observation based on his interactions within the field.

--------

## Chunk 611

**Chunk:**

Dwarkesh Patel
The project makes more sense there, where you can't just recruit a Manhattan Project engineer.
Leopold Aschenbrenner
These are secrets that can be used for probably every training run in the future. Maybe they’re the key to the data wall without which they can’t go on. They're going to give multipliers on compute worth hundreds of billions, trillions of dollars. All it takes is China to offer $100 million to somebody and say “come work for us.” I'm really uncertain on how seriously China is taking AGI right now.

One anecdote was related to me by another researcher in the field. They were at a conference with somebody, a Chinese AI researcher. He was talking to him and he was like, "I think it's really good that you're here. We have to have international coordination and stuff." Apparently this guy said that "I'm the most senior person that they're going to let leave the country to come to things like this."
Dwarkesh Patel
What's the takeaway?

**Extracted Belief:**

The secrets of key algorithms for AI development are valuable and could be used for every training run in the future.

**Context:**

Leopold Aschenbrenner is discussing the value of these secrets and the potential impact of their dissemination.

**Justification:**

Aschenbrenner's statement is based on his expertise and observation of the field. He believes these secrets are crucial for ongoing AI development. He emphasizes their potential for multiplying compute value, referencing billions and trillions of dollars.

--------

## Chunk 612

**Chunk:**

Dwarkesh Patel
The project makes more sense there, where you can't just recruit a Manhattan Project engineer.
Leopold Aschenbrenner
These are secrets that can be used for probably every training run in the future. Maybe they’re the key to the data wall without which they can’t go on. They're going to give multipliers on compute worth hundreds of billions, trillions of dollars. All it takes is China to offer $100 million to somebody and say “come work for us.” I'm really uncertain on how seriously China is taking AGI right now.

One anecdote was related to me by another researcher in the field. They were at a conference with somebody, a Chinese AI researcher. He was talking to him and he was like, "I think it's really good that you're here. We have to have international coordination and stuff." Apparently this guy said that "I'm the most senior person that they're going to let leave the country to come to things like this."
Dwarkesh Patel
What's the takeaway?

**Extracted Belief:**

The secrets of key algorithms for AI development may be essential for overcoming challenges related to data limitations.

**Context:**

Aschenbrenner is discussing the significance of these secrets within the context of AI development challenges.

**Justification:**

He mentions that these secrets may be 'the key to the data wall,' implying that they could be crucial for overcoming data limitations in AI development.

--------

## Chunk 613

**Chunk:**

Dwarkesh Patel
The project makes more sense there, where you can't just recruit a Manhattan Project engineer.
Leopold Aschenbrenner
These are secrets that can be used for probably every training run in the future. Maybe they’re the key to the data wall without which they can’t go on. They're going to give multipliers on compute worth hundreds of billions, trillions of dollars. All it takes is China to offer $100 million to somebody and say “come work for us.” I'm really uncertain on how seriously China is taking AGI right now.

One anecdote was related to me by another researcher in the field. They were at a conference with somebody, a Chinese AI researcher. He was talking to him and he was like, "I think it's really good that you're here. We have to have international coordination and stuff." Apparently this guy said that "I'm the most senior person that they're going to let leave the country to come to things like this."
Dwarkesh Patel
What's the takeaway?

**Extracted Belief:**

China is actively pursuing the development of Artificial General Intelligence (AGI).

**Context:**

Aschenbrenner is sharing an anecdote about a Chinese AI researcher, implying China's investment in AGI.

**Justification:**

Aschenbrenner expresses uncertainty about the seriousness of China's pursuit of AGI but offers an anecdote involving a Chinese AI researcher at an international conference, suggesting China's interest and involvement.

--------

## Chunk 614

**Chunk:**

Dwarkesh Patel
The project makes more sense there, where you can't just recruit a Manhattan Project engineer.
Leopold Aschenbrenner
These are secrets that can be used for probably every training run in the future. Maybe they’re the key to the data wall without which they can’t go on. They're going to give multipliers on compute worth hundreds of billions, trillions of dollars. All it takes is China to offer $100 million to somebody and say “come work for us.” I'm really uncertain on how seriously China is taking AGI right now.

One anecdote was related to me by another researcher in the field. They were at a conference with somebody, a Chinese AI researcher. He was talking to him and he was like, "I think it's really good that you're here. We have to have international coordination and stuff." Apparently this guy said that "I'm the most senior person that they're going to let leave the country to come to things like this."
Dwarkesh Patel
What's the takeaway?

**Extracted Belief:**

China is restricting the travel of senior AI researchers outside the country.

**Context:**

Aschenbrenner is relating an anecdote about a Chinese AI researcher who claimed to be the most senior person allowed to leave the country for such events.

**Justification:**

Aschenbrenner's statement is based on the anecdote related by another researcher and implies a policy of restricting the movement of senior AI experts.

--------

## Chunk 615

**Chunk:**

Dwarkesh Patel
What's the takeaway?
Leopold Aschenbrenner
They're not letting really senior AI researchers leave the country. It’s a kind of classic Eastern Bloc move.

I don't know if this is true, but it’s what I heard.
Dwarkesh Patel
Let’s go back to the point you made earlier about being exposed to German newspapers. Earlier you mentioned you were interested in economics and law and national security. The variety in intellectual diet has exposed you to thinking about the geopolitical question here in ways that that others talking about AI aren’t.

This is the first episode I've done about this where we've talked about things like this. Now that I think about it, that’s weird given that this is an obvious thing in retrospect. I should have been thinking about it. That's one thing we've been missing.

What are you missing, not just in national security? What perspective are you probably underexposed to as a result? I guess you mentioned China as one.

**Extracted Belief:**

China is restricting the travel of senior AI researchers.

**Context:**

Leopold Aschenbrenner is discussing China's stance on artificial intelligence research and the potential for them to recruit AI researchers for their own benefit.

**Justification:**

He heard from a fellow researcher that a Chinese AI researcher stated he was the most senior person allowed to leave China to attend a conference.

--------

## Chunk 616

**Chunk:**

Dwarkesh Patel
Let’s go back to the point you made earlier about being exposed to German newspapers. Earlier you mentioned you were interested in economics and law and national security. The variety in intellectual diet has exposed you to thinking about the geopolitical question here in ways that that others talking about AI aren’t.

This is the first episode I've done about this where we've talked about things like this. Now that I think about it, that’s weird given that this is an obvious thing in retrospect. I should have been thinking about it. That's one thing we've been missing.

What are you missing, not just in national security? What perspective are you probably underexposed to as a result? I guess you mentioned China as one.
Leopold Aschenbrenner
The China one is an important one. Another one would be a sort of very Tyler Cowen-esque take. You're not exposed to how a normal person in America will use AI. That kind of thing will be a bottleneck to the diffusion of these things. I'm overrating the revenue because I assume everyone is adopting it. But Joe Schmo engineer at a company, will they be able to integrate it? Also what’s the reaction to it? This was a question hours ago. Won’t people rebel against this? Will they not want to do the project? I don't know. Maybe they will.
Dwarkesh Patel
Here's a political reaction that I didn't anticipate. I already told you about this, but I'm just going to tell the story again. Tucker Carlson was recently on a Joe Rogan episode. They start talking about World War II.

Tucker says, "well, listen, I'm going to say something that my fellow conservatives won't like, but I think nuclear weapons are immoral. I think it was obviously immoral that we use them on Nagasaki and Hiroshima."

Then he says, "In fact, nuclear weapons are always immoral, except when we would use them on data centers. In fact, it would be immoral not to use them on data centers, because, look, these people in Silicon Valley, these fucking nerds, are making superintelligence, and they say that it could enslave humanity. We made machines to serve humanity, not to enslave humanity. And they're just going on and making these machines. And so we should, of course, be nuking the data centers." That is definitely not a political reaction in 2024 I was expecting. It's going to be crazy.
Dwarkesh Patel
The thing we learned with COVID is also that the left-right reactions that you’d anticipate just based on hunches—

**Extracted Belief:**

A "Tyler Cowen-esque" perspective is needed in the discussion of AI, focusing on how normal people in America will use and interact with AI.

**Context:**

Leopold Aschenbrenner emphasizes the importance of considering the perspective of regular Americans in relation to AI adoption and its impact on their lives.

**Justification:**

He specifically mentions "Tyler Cowen-esque take" as a point of view that he believes is lacking in the AI discourse, suggesting that Tyler Cowen's work or perspective offers valuable insights into this area.

--------

## Chunk 617

**Chunk:**

Dwarkesh Patel
Let’s go back to the point you made earlier about being exposed to German newspapers. Earlier you mentioned you were interested in economics and law and national security. The variety in intellectual diet has exposed you to thinking about the geopolitical question here in ways that that others talking about AI aren’t.

This is the first episode I've done about this where we've talked about things like this. Now that I think about it, that’s weird given that this is an obvious thing in retrospect. I should have been thinking about it. That's one thing we've been missing.

What are you missing, not just in national security? What perspective are you probably underexposed to as a result? I guess you mentioned China as one.
Leopold Aschenbrenner
The China one is an important one. Another one would be a sort of very Tyler Cowen-esque take. You're not exposed to how a normal person in America will use AI. That kind of thing will be a bottleneck to the diffusion of these things. I'm overrating the revenue because I assume everyone is adopting it. But Joe Schmo engineer at a company, will they be able to integrate it? Also what’s the reaction to it? This was a question hours ago. Won’t people rebel against this? Will they not want to do the project? I don't know. Maybe they will.
Dwarkesh Patel
Here's a political reaction that I didn't anticipate. I already told you about this, but I'm just going to tell the story again. Tucker Carlson was recently on a Joe Rogan episode. They start talking about World War II.

Tucker says, "well, listen, I'm going to say something that my fellow conservatives won't like, but I think nuclear weapons are immoral. I think it was obviously immoral that we use them on Nagasaki and Hiroshima."

Then he says, "In fact, nuclear weapons are always immoral, except when we would use them on data centers. In fact, it would be immoral not to use them on data centers, because, look, these people in Silicon Valley, these fucking nerds, are making superintelligence, and they say that it could enslave humanity. We made machines to serve humanity, not to enslave humanity. And they're just going on and making these machines. And so we should, of course, be nuking the data centers." That is definitely not a political reaction in 2024 I was expecting. It's going to be crazy.
Dwarkesh Patel
The thing we learned with COVID is also that the left-right reactions that you’d anticipate just based on hunches—

**Extracted Belief:**

The diffusion of AI technology will be bottlenecked by the average person's ability to integrate it into their lives and work.

**Context:**

Leopold Aschenbrenner suggests that the widespread adoption of AI will be hampered by the general public's ability to use and understand it.

**Justification:**

He uses the example of a "Joe Schmo engineer" to highlight the potential difficulty for average people to integrate AI, indicating that this difficulty could hinder its broader adoption.

--------

## Chunk 618

**Chunk:**

Dwarkesh Patel
Let’s go back to the point you made earlier about being exposed to German newspapers. Earlier you mentioned you were interested in economics and law and national security. The variety in intellectual diet has exposed you to thinking about the geopolitical question here in ways that that others talking about AI aren’t.

This is the first episode I've done about this where we've talked about things like this. Now that I think about it, that’s weird given that this is an obvious thing in retrospect. I should have been thinking about it. That's one thing we've been missing.

What are you missing, not just in national security? What perspective are you probably underexposed to as a result? I guess you mentioned China as one.
Leopold Aschenbrenner
The China one is an important one. Another one would be a sort of very Tyler Cowen-esque take. You're not exposed to how a normal person in America will use AI. That kind of thing will be a bottleneck to the diffusion of these things. I'm overrating the revenue because I assume everyone is adopting it. But Joe Schmo engineer at a company, will they be able to integrate it? Also what’s the reaction to it? This was a question hours ago. Won’t people rebel against this? Will they not want to do the project? I don't know. Maybe they will.
Dwarkesh Patel
Here's a political reaction that I didn't anticipate. I already told you about this, but I'm just going to tell the story again. Tucker Carlson was recently on a Joe Rogan episode. They start talking about World War II.

Tucker says, "well, listen, I'm going to say something that my fellow conservatives won't like, but I think nuclear weapons are immoral. I think it was obviously immoral that we use them on Nagasaki and Hiroshima."

Then he says, "In fact, nuclear weapons are always immoral, except when we would use them on data centers. In fact, it would be immoral not to use them on data centers, because, look, these people in Silicon Valley, these fucking nerds, are making superintelligence, and they say that it could enslave humanity. We made machines to serve humanity, not to enslave humanity. And they're just going on and making these machines. And so we should, of course, be nuking the data centers." That is definitely not a political reaction in 2024 I was expecting. It's going to be crazy.
Dwarkesh Patel
The thing we learned with COVID is also that the left-right reactions that you’d anticipate just based on hunches—

**Extracted Belief:**

The potential for AI to lead to a public rebellion is a real possibility.

**Context:**

Leopold Aschenbrenner raises the question of public reaction to AI, suggesting that a backlash or resistance could emerge.

**Justification:**

He asks whether people will "rebel against" AI or "not want to do the project", implying that negative public sentiment towards AI's development is a concern.

--------

## Chunk 619

**Chunk:**

Dwarkesh Patel
Here's a political reaction that I didn't anticipate. I already told you about this, but I'm just going to tell the story again. Tucker Carlson was recently on a Joe Rogan episode. They start talking about World War II.

Tucker says, "well, listen, I'm going to say something that my fellow conservatives won't like, but I think nuclear weapons are immoral. I think it was obviously immoral that we use them on Nagasaki and Hiroshima."

Then he says, "In fact, nuclear weapons are always immoral, except when we would use them on data centers. In fact, it would be immoral not to use them on data centers, because, look, these people in Silicon Valley, these fucking nerds, are making superintelligence, and they say that it could enslave humanity. We made machines to serve humanity, not to enslave humanity. And they're just going on and making these machines. And so we should, of course, be nuking the data centers." That is definitely not a political reaction in 2024 I was expecting. It's going to be crazy.
Dwarkesh Patel
The thing we learned with COVID is also that the left-right reactions that you’d anticipate just based on hunches—
Leopold Aschenbrenner
It completely flipped multiple times. Initially the right was on it and the left was like, "this is racist." Then it flipped. The left was really into the lockdowns. The whole thing also is just so blunt and crude.

Probably in general, people like to make sort of complicated technocratic AI policy proposals. If things go kind of fairly rapidly on the path to AGI, there might not actually be that much space for complicated, clever proposals. It might just be much cruder reactions.
Dwarkesh Patel
You mentioned spies and national security getting involved and everything. You can talk about that in the abstract, but now that we're living in San Francisco we know many of the people who are doing the top AI research. It’s also a little scary to think about people I personally know and am friends with. It's not unfeasible if they have secrets in their head that are worth $100 billion or something that you might see kidnapping, assassination, sabotage.

**Extracted Belief:**

Political reactions to complex issues, such as the development of artificial general intelligence (AGI), are likely to be less sophisticated and more blunt than anticipated.

**Context:**

Leopold Aschenbrenner is discussing the potential political responses to the development of AGI, drawing on the example of the COVID-19 pandemic.

**Justification:**

Leopold Aschenbrenner cites the unpredictable and often crude nature of political reactions during the COVID-19 pandemic, where initial right-wing support for lockdowns later shifted to left-wing support. He extrapolates this to suggest that reactions to AGI, a complex issue, might be similarly blunt and less nuanced.

--------

## Chunk 620

**Chunk:**

Dwarkesh Patel
Here's a political reaction that I didn't anticipate. I already told you about this, but I'm just going to tell the story again. Tucker Carlson was recently on a Joe Rogan episode. They start talking about World War II.

Tucker says, "well, listen, I'm going to say something that my fellow conservatives won't like, but I think nuclear weapons are immoral. I think it was obviously immoral that we use them on Nagasaki and Hiroshima."

Then he says, "In fact, nuclear weapons are always immoral, except when we would use them on data centers. In fact, it would be immoral not to use them on data centers, because, look, these people in Silicon Valley, these fucking nerds, are making superintelligence, and they say that it could enslave humanity. We made machines to serve humanity, not to enslave humanity. And they're just going on and making these machines. And so we should, of course, be nuking the data centers." That is definitely not a political reaction in 2024 I was expecting. It's going to be crazy.
Dwarkesh Patel
The thing we learned with COVID is also that the left-right reactions that you’d anticipate just based on hunches—
Leopold Aschenbrenner
It completely flipped multiple times. Initially the right was on it and the left was like, "this is racist." Then it flipped. The left was really into the lockdowns. The whole thing also is just so blunt and crude.

Probably in general, people like to make sort of complicated technocratic AI policy proposals. If things go kind of fairly rapidly on the path to AGI, there might not actually be that much space for complicated, clever proposals. It might just be much cruder reactions.
Dwarkesh Patel
You mentioned spies and national security getting involved and everything. You can talk about that in the abstract, but now that we're living in San Francisco we know many of the people who are doing the top AI research. It’s also a little scary to think about people I personally know and am friends with. It's not unfeasible if they have secrets in their head that are worth $100 billion or something that you might see kidnapping, assassination, sabotage.

**Extracted Belief:**

People generally prefer to propose complicated, technocratic solutions for AI policy.

**Context:**

Leopold Aschenbrenner is discussing the potential political responses to the development of AGI, in contrast to the more complex policy proposals typically favored by experts.

**Justification:**

He contrasts this preference for complex solutions with his expectation of simpler, cruder reactions if the development of AGI proceeds quickly.

--------

## Chunk 621

**Chunk:**

Dwarkesh Patel
You mentioned spies and national security getting involved and everything. You can talk about that in the abstract, but now that we're living in San Francisco we know many of the people who are doing the top AI research. It’s also a little scary to think about people I personally know and am friends with. It's not unfeasible if they have secrets in their head that are worth $100 billion or something that you might see kidnapping, assassination, sabotage.
Leopold Aschenbrenner
Oh, their family. It's really bad. To the point on security, right now it’s really foreign. At some point, as it becomes really serious, you're going to want the security guards.
Dwarkesh Patel
Presumably, you have thought about the fact that people in China will be listening to this and reading your series.

Somehow you made the trade-off that it's better to let the whole world know, including China, and wake them up to AGI than to stay silent. Part of the thing you're worried about is China waking up to AGI. I’m curious about that. Walk me through how you've thought about that trade-off.

**Extracted Belief:**

The increasing seriousness of AI research will eventually require security measures.

**Context:**

Leopold Aschenbrenner responds to Dwarkesh Patel's concern about the potential for espionage and violence towards prominent AI researchers, acknowledging the need for security measures as AI becomes more significant.

**Justification:**

Aschenbrenner suggests that the development of AI will reach a point where heightened security measures will be necessary. This is based on the assumption that AI advancements will lead to greater stakes and potential threats.

--------

## Chunk 622

**Chunk:**

Dwarkesh Patel
You mentioned spies and national security getting involved and everything. You can talk about that in the abstract, but now that we're living in San Francisco we know many of the people who are doing the top AI research. It’s also a little scary to think about people I personally know and am friends with. It's not unfeasible if they have secrets in their head that are worth $100 billion or something that you might see kidnapping, assassination, sabotage.
Leopold Aschenbrenner
Oh, their family. It's really bad. To the point on security, right now it’s really foreign. At some point, as it becomes really serious, you're going to want the security guards.
Dwarkesh Patel
Presumably, you have thought about the fact that people in China will be listening to this and reading your series.

Somehow you made the trade-off that it's better to let the whole world know, including China, and wake them up to AGI than to stay silent. Part of the thing you're worried about is China waking up to AGI. I’m curious about that. Walk me through how you've thought about that trade-off.

**Extracted Belief:**

It is important to inform a wider audience about the potential risks and opportunities of AI.

**Context:**

Leopold Aschenbrenner addresses Dwarkesh Patel's concern about sharing information about AI with China, justifying his decision to publish his work publicly.

**Justification:**

Aschenbrenner argues that informing a broader audience, including those in China, about AI is essential for effective management of its implications, emphasizing the need for broader awareness and understanding.

--------

## Chunk 623

**Chunk:**

Dwarkesh Patel
Presumably, you have thought about the fact that people in China will be listening to this and reading your series.

Somehow you made the trade-off that it's better to let the whole world know, including China, and wake them up to AGI than to stay silent. Part of the thing you're worried about is China waking up to AGI. I’m curious about that. Walk me through how you've thought about that trade-off.
Leopold Aschenbrenner
This is a tough trade-off. I thought about this a bunch. People in the PRC will read this.

To some extent the cat is out of the bag. AGI is a thing people are thinking about very seriously. That’s not new anymore. A lot of these takes are kind of old or I had similar views a year ago. I might not have written it up a year ago, in part because I didn’t think the cat wasn't out of the bag enough then.

To be able to manage this challenge, much broader swaths of society will need to wake up. If we're going to get the project, we actually need a broad bipartisan understanding of the challenges facing us. It's a tough trade-off. The need to wake up people in the United States, in the Western world, in the democratic coalition, is ultimately imperative. My hope is more people here will read it than in the PRC.

People sometimes underrate the importance of just kind of writing it up and laying out the strategic picture. You have done actually a great service to mankind in some sense with your podcast. It's overall been good.

(03:57:04) – Dwarkesh’s immigration story and path to the podcast
Leopold Aschenbrenner
By the way, on the topic of Germany. We were talking at some point about immigration stories. You have an interesting story you haven't told, butI think you should tell it
Dwarkesh Patel
So a couple years ago, I was in college and I was 20. I was about to turn 21.

**Extracted Belief:**

It is necessary for a wider range of people in society to understand and address the challenges posed by Artificial General Intelligence (AGI) in order to effectively manage its development and potential consequences.

**Context:**

In the context of discussing his decision to publicly share his concerns about AGI, Leopold Aschenbrenner highlights the need for broader societal awareness and understanding of AGI challenges.

**Justification:**

He argues that to successfully manage the challenges of AGI, a broader, bipartisan understanding of the issues is crucial. This suggests that he believes an informed and engaged public is essential for effective decision-making regarding AGI.

--------

## Chunk 624

**Chunk:**

Dwarkesh Patel
Presumably, you have thought about the fact that people in China will be listening to this and reading your series.

Somehow you made the trade-off that it's better to let the whole world know, including China, and wake them up to AGI than to stay silent. Part of the thing you're worried about is China waking up to AGI. I’m curious about that. Walk me through how you've thought about that trade-off.
Leopold Aschenbrenner
This is a tough trade-off. I thought about this a bunch. People in the PRC will read this.

To some extent the cat is out of the bag. AGI is a thing people are thinking about very seriously. That’s not new anymore. A lot of these takes are kind of old or I had similar views a year ago. I might not have written it up a year ago, in part because I didn’t think the cat wasn't out of the bag enough then.

To be able to manage this challenge, much broader swaths of society will need to wake up. If we're going to get the project, we actually need a broad bipartisan understanding of the challenges facing us. It's a tough trade-off. The need to wake up people in the United States, in the Western world, in the democratic coalition, is ultimately imperative. My hope is more people here will read it than in the PRC.

People sometimes underrate the importance of just kind of writing it up and laying out the strategic picture. You have done actually a great service to mankind in some sense with your podcast. It's overall been good.

(03:57:04) – Dwarkesh’s immigration story and path to the podcast
Leopold Aschenbrenner
By the way, on the topic of Germany. We were talking at some point about immigration stories. You have an interesting story you haven't told, butI think you should tell it
Dwarkesh Patel
So a couple years ago, I was in college and I was 20. I was about to turn 21.

**Extracted Belief:**

The widespread awareness of AGI as a serious concern is already established, making public discussion about it more important than it was in the past.

**Context:**

Leopold Aschenbrenner acknowledges that the development of AGI is a topic of serious consideration among experts.

**Justification:**

He uses the metaphor of the "cat being out of the bag" to indicate that AGI is no longer a secret or a niche topic. This suggests that he is drawing on his knowledge of the field and the discussions happening within it.

--------

## Chunk 625

**Chunk:**

Dwarkesh Patel
Presumably, you have thought about the fact that people in China will be listening to this and reading your series.

Somehow you made the trade-off that it's better to let the whole world know, including China, and wake them up to AGI than to stay silent. Part of the thing you're worried about is China waking up to AGI. I’m curious about that. Walk me through how you've thought about that trade-off.
Leopold Aschenbrenner
This is a tough trade-off. I thought about this a bunch. People in the PRC will read this.

To some extent the cat is out of the bag. AGI is a thing people are thinking about very seriously. That’s not new anymore. A lot of these takes are kind of old or I had similar views a year ago. I might not have written it up a year ago, in part because I didn’t think the cat wasn't out of the bag enough then.

To be able to manage this challenge, much broader swaths of society will need to wake up. If we're going to get the project, we actually need a broad bipartisan understanding of the challenges facing us. It's a tough trade-off. The need to wake up people in the United States, in the Western world, in the democratic coalition, is ultimately imperative. My hope is more people here will read it than in the PRC.

People sometimes underrate the importance of just kind of writing it up and laying out the strategic picture. You have done actually a great service to mankind in some sense with your podcast. It's overall been good.

(03:57:04) – Dwarkesh’s immigration story and path to the podcast
Leopold Aschenbrenner
By the way, on the topic of Germany. We were talking at some point about immigration stories. You have an interesting story you haven't told, butI think you should tell it
Dwarkesh Patel
So a couple years ago, I was in college and I was 20. I was about to turn 21.

**Extracted Belief:**

Publicly discussing and raising awareness about AGI is a valuable service to humanity.

**Context:**

Leopold Aschenbrenner expresses appreciation for Dwarkesh Patel's work on the podcast, stating that it has provided a valuable service to humanity.

**Justification:**

This is implicitly linked to his prior statements about the need for broader societal awareness and understanding of AGI. It suggests that he believes public engagement on this topic is beneficial and contributes to the greater good.

--------

## Chunk 626

**Chunk:**

Dwarkesh Patel
So a couple years ago, I was in college and I was 20. I was about to turn 21.
Leopold Aschenbrenner
You came from India when you were really young, right?
Dwarkesh Patel
Until I was eight or nine, I lived in India. Then we moved around all over the place. Because of the backlog for Indians we’d been in the queue for decades.

**Extracted Belief:**

People immigrating to the United States from India often face a long waiting period to obtain visas.

**Context:**

Leopold Aschenbrenner acknowledges that Dwarkesh Patel's family had been in the queue for decades due to the backlog for Indians seeking US visas.

**Justification:**

Dwarkesh Patel explicitly states that his family was in the queue for decades due to the backlog for Indians.

--------

## Chunk 627

**Chunk:**

Dwarkesh Patel
Until I was eight or nine, I lived in India. Then we moved around all over the place. Because of the backlog for Indians we’d been in the queue for decades.
Leopold Aschenbrenner
Even though you came at eight, you're still on the H-1B.
Dwarkesh Patel
When you're 21 you get kicked off the queue and you have to restart the process. My dad's a doctor and I'm on his H-1B as a dependent. But when you're 21, you get kicked off. So I'm 20 and it just kind of dawns on me that this is my situation.

**Extracted Belief:**

The H-1B visa program in the United States requires individuals to restart the application process when they turn 21, even if they have been on the visa as dependents since childhood.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's explanation of his personal experience with the H-1B visa program as a dependent of his father, a doctor.

**Justification:**

Leopold Aschenbrenner's statement is based on his knowledge of the H-1B visa program, which is likely derived from his own experience or from discussions with others familiar with the program.

--------

## Chunk 628

**Chunk:**

Dwarkesh Patel
When you're 21 you get kicked off the queue and you have to restart the process. My dad's a doctor and I'm on his H-1B as a dependent. But when you're 21, you get kicked off. So I'm 20 and it just kind of dawns on me that this is my situation.
Leopold Aschenbrenner
You’re completely screwed.
Dwarkesh Patel
I also had the experience with my dad. We moved all around the country. They have to prove, him being a doctor, that you can't get native talent.

**Extracted Belief:**

Obtaining a work visa in the United States is extremely difficult, especially for individuals seeking to start their own businesses.

**Context:**

Leopold Aschenbrenner responds to Dwarkesh Patel's statement about being stuck in a difficult situation regarding his visa status. He directly states that Dwarkesh Patel is "completely screwed." 

**Justification:**

Leopold Aschenbrenner's statement is based on his knowledge and experience, indicating the difficulty of obtaining a work visa and the limitations it places on individuals, particularly entrepreneurs.

--------

## Chunk 629

**Chunk:**

Dwarkesh Patel
I also had the experience with my dad. We moved all around the country. They have to prove, him being a doctor, that you can't get native talent.
Leopold Aschenbrenner
And you can’t start a startup or anything. Even getting the H-1B for you would have been a 20% lottery, if you're lucky.
Dwarkesh Patel
Plus they had to prove that they can't get native talent, which meant that we lived in North Dakota for three years, West Virginia for three years, Maryland, West Texas.

So it dawned on me that this is my situation as I turn 21. I'll be on this lottery. Even if I get the lottery, I'll be a fucking code monkey for the rest of my life, because this thing isn't going to let up.

**Extracted Belief:**

It is very difficult for individuals on H-1B visas to start their own businesses.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's experience with the H-1B visa process, specifically the requirement to prove a lack of native talent.

**Justification:**

Leopold Aschenbrenner, based on his knowledge of the H-1B visa system, states that starting a business is nearly impossible for individuals on this visa.

--------

## Chunk 630

**Chunk:**

Dwarkesh Patel
I also had the experience with my dad. We moved all around the country. They have to prove, him being a doctor, that you can't get native talent.
Leopold Aschenbrenner
And you can’t start a startup or anything. Even getting the H-1B for you would have been a 20% lottery, if you're lucky.
Dwarkesh Patel
Plus they had to prove that they can't get native talent, which meant that we lived in North Dakota for three years, West Virginia for three years, Maryland, West Texas.

So it dawned on me that this is my situation as I turn 21. I'll be on this lottery. Even if I get the lottery, I'll be a fucking code monkey for the rest of my life, because this thing isn't going to let up.

**Extracted Belief:**

The H-1B visa lottery has a low success rate.

**Context:**

Leopold Aschenbrenner is discussing the difficulty of obtaining an H-1B visa, even if one is eligible.

**Justification:**

Leopold Aschenbrenner states that the odds of winning the H-1B visa lottery are only 20%, implying a low success rate based on his knowledge of the immigration process.

--------

## Chunk 631

**Chunk:**

Dwarkesh Patel
Plus they had to prove that they can't get native talent, which meant that we lived in North Dakota for three years, West Virginia for three years, Maryland, West Texas.

So it dawned on me that this is my situation as I turn 21. I'll be on this lottery. Even if I get the lottery, I'll be a fucking code monkey for the rest of my life, because this thing isn't going to let up.
Leopold Aschenbrenner
Yeah. Can't do a startup.
Dwarkesh Patel
Exactly. At the same time, I had been reading for the last year and was super obsessed with Paul Graham essays. My plan at the time was to make a startup or something. I was super excited about that.

It just occurred to me that I couldn't do this. That just wasn’t in the cards for me. I was kind of depressed about it. I remember I was in a daze through finals because it had just occurred to me. I was really anxious about it.

I remember thinking to myself at the time that if somehow I ended up getting my green card before I turned 21, there's no fucking way I'm becoming a code monkey. The feeling of dread that I have is this realization that I'm just going to have to be a code monkey. I realized that's my default path. If I hadn't made a proactive effort not to do that, I would have graduated college as a computer science student. I would have just done that. That's the thing I was super scared about. That was an important realization for me.

Anyway, COVID happened. Because of that, since there weren't any foreigners coming, the backlog got fast-tracked and by the skin of my teeth, like a few months before I turned 21, I ended up getting a green card for crazy, extremely contingent reasons.

Because I got a green card, I could—

**Extracted Belief:**

H-1B visa holders are limited in their ability to start their own businesses or pursue entrepreneurial ventures in the United States.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's description of the challenges he faced as a dependent on an H-1B visa, highlighting the restrictions imposed on foreign nationals in the US.

**Justification:**

Leopold Aschenbrenner's statement, "You can't start a startup or anything", is based on his understanding of the limitations placed on H-1B visa holders.

--------

## Chunk 632

**Chunk:**

Dwarkesh Patel
Exactly. At the same time, I had been reading for the last year and was super obsessed with Paul Graham essays. My plan at the time was to make a startup or something. I was super excited about that.

It just occurred to me that I couldn't do this. That just wasn’t in the cards for me. I was kind of depressed about it. I remember I was in a daze through finals because it had just occurred to me. I was really anxious about it.

I remember thinking to myself at the time that if somehow I ended up getting my green card before I turned 21, there's no fucking way I'm becoming a code monkey. The feeling of dread that I have is this realization that I'm just going to have to be a code monkey. I realized that's my default path. If I hadn't made a proactive effort not to do that, I would have graduated college as a computer science student. I would have just done that. That's the thing I was super scared about. That was an important realization for me.

Anyway, COVID happened. Because of that, since there weren't any foreigners coming, the backlog got fast-tracked and by the skin of my teeth, like a few months before I turned 21, I ended up getting a green card for crazy, extremely contingent reasons.

Because I got a green card, I could—
Leopold Aschenbrenner
The whole podcast.
Dwarkesh Patel
I graduated college and I was bumming around. I graduated a semester early. I'm going to do this podcast and see what happens? If I didn’t have a green card, I mean the best case scenario—

**Extracted Belief:**

The podcast is a cultural artifact.

**Context:**

Leopold Aschenbrenner is acknowledging the podcast as a noteworthy cultural phenomenon, likely due to its unique content and impact.

**Justification:**

This belief is based on Leopold Aschenbrenner's personal opinion and judgment, drawing from his understanding of culture and media.

--------

## Chunk 633

**Chunk:**

Dwarkesh Patel
I graduated college and I was bumming around. I graduated a semester early. I'm going to do this podcast and see what happens? If I didn’t have a green card, I mean the best case scenario—
Leopold Aschenbrenner
It’s such a cultural artifact. What is the impact of immigration reform? What is the impact of clearing 50,000 green cards in the backlog? You're such an amazing example how all of this is only possible contingent on that. It's just incredibly tragic that this is so dysfunctional.
Dwarkesh Patel
It's insane.

**Extracted Belief:**

The current immigration system in the United States is dysfunctional.

**Context:**

Leopold Aschenbrenner is responding to Dwarkesh Patel's description of how his podcasting career was contingent on receiving a green card. Aschenbrenner emphasizes the role of immigration reform and clearing the green card backlog in impacting the success of individuals like Patel.

**Justification:**

Aschenbrenner states that the situation is "incredibly tragic" and "dysfunctional", implying a belief that the current system is not functioning as intended.

--------

## Chunk 634

**Chunk:**

Dwarkesh Patel
I graduated college and I was bumming around. I graduated a semester early. I'm going to do this podcast and see what happens? If I didn’t have a green card, I mean the best case scenario—
Leopold Aschenbrenner
It’s such a cultural artifact. What is the impact of immigration reform? What is the impact of clearing 50,000 green cards in the backlog? You're such an amazing example how all of this is only possible contingent on that. It's just incredibly tragic that this is so dysfunctional.
Dwarkesh Patel
It's insane.

**Extracted Belief:**

Dwarkesh Patel's success in podcasting is contingent on his immigration status.

**Context:**

Leopold Aschenbrenner is acknowledging that Patel's ability to pursue his podcasting career was directly impacted by his acquisition of a green card.

**Justification:**

Aschenbrenner states that Patel's success is "only possible contingent on" receiving a green card, indicating a belief in the direct connection between immigration status and Patel's career path.

--------

## Chunk 635

**Chunk:**

Dwarkesh Patel
It's insane.
Leopold Aschenbrenner
I'm glad you did it. I'm glad you kind of tried the unusual path.
Dwarkesh Patel
I could only do it because I was extremely fortunate to get the green card. I had a little bit of saved up money. I got a small grant out of college, thanks to the Future Fund, to do this for like six months. It turned out really well. At each time, I was like, "oh, okay, podcast. Come on. I wasted a few months on this. Let's now go do something real." Something big would happen at each moment.

**Extracted Belief:**

Trying unusual paths in life can be beneficial.

**Context:**

Leopold Aschenbrenner is expressing his approval of Dwarkesh Patel's decision to pursue podcasting, an unconventional career path.

**Justification:**

Leopold Aschenbrenner states, "I'm glad you did it. I'm glad you kind of tried the unusual path."

--------

## Chunk 636

**Chunk:**

Dwarkesh Patel
I could only do it because I was extremely fortunate to get the green card. I had a little bit of saved up money. I got a small grant out of college, thanks to the Future Fund, to do this for like six months. It turned out really well. At each time, I was like, "oh, okay, podcast. Come on. I wasted a few months on this. Let's now go do something real." Something big would happen at each moment.
Leopold Aschenbrenner
You kept with it.
Dwarkesh Patel
There would always be something the moment I'm about to quit the podcast. Jeff Bezos would say something nice about me on Twitter. The Ilya episode gets like half a million views. Now this is my career. Looking back on it though, it was incredibly contingent that things worked out the right way.

**Extracted Belief:**

Persistence and dedication are crucial for success.

**Context:**

Leopold Aschenbrenner acknowledges Dwarkesh Patel's persistence in podcasting, highlighting the importance of continuing despite initial doubts.

**Justification:**

Leopold Aschenbrenner's statement 'You kept with it' implies that Dwarkesh Patel's continued efforts were a key factor in his success.

--------

## Chunk 637

**Chunk:**

Dwarkesh Patel
I could only do it because I was extremely fortunate to get the green card. I had a little bit of saved up money. I got a small grant out of college, thanks to the Future Fund, to do this for like six months. It turned out really well. At each time, I was like, "oh, okay, podcast. Come on. I wasted a few months on this. Let's now go do something real." Something big would happen at each moment.
Leopold Aschenbrenner
You kept with it.
Dwarkesh Patel
There would always be something the moment I'm about to quit the podcast. Jeff Bezos would say something nice about me on Twitter. The Ilya episode gets like half a million views. Now this is my career. Looking back on it though, it was incredibly contingent that things worked out the right way.

**Extracted Belief:**

Success is often contingent on external factors and circumstances.

**Context:**

Leopold Aschenbrenner acknowledges that Dwarkesh Patel's career success was partially due to unforeseen events and opportunities.

**Justification:**

Leopold Aschenbrenner states, 'Looking back on it though, it was incredibly contingent that things worked out the right way,' indicating that external factors played a significant role.

--------

## Chunk 638

**Chunk:**

Dwarkesh Patel
There would always be something the moment I'm about to quit the podcast. Jeff Bezos would say something nice about me on Twitter. The Ilya episode gets like half a million views. Now this is my career. Looking back on it though, it was incredibly contingent that things worked out the right way.
Leopold Aschenbrenner
If the AGI stuff goes down, it'll be how most of the people who kind of end up feeling AGI first heard about it.
Dwarkesh Patel
You're also very linked with the story in many ways. I got like a $20,000 grant from Future Fund right out of college and that sustained me for six months or something. Without that…

**Extracted Belief:**

If artificial general intelligence (AGI) becomes a reality, Dwarkesh Patel's podcast will be a primary source of information for many people who experience AGI for the first time.

**Context:**

Leopold Aschenbrenner is commenting on the potential influence of Dwarkesh Patel's podcast on people's understanding of AGI.

**Justification:**

Leopold Aschenbrenner is expressing his belief based on his knowledge of the podcast's content and its potential reach within the AGI community.

--------

## Chunk 639

**Chunk:**

Dwarkesh Patel
You're also very linked with the story in many ways. I got like a $20,000 grant from Future Fund right out of college and that sustained me for six months or something. Without that…
Leopold Aschenbrenner
Tiny grant. It was kind of crazy. It goes to show how far small grants can go. Emergent Ventures, too.
Dwarkesh Patel
Exactly. Emergent Ventures. The last year I've been in San Francisco, we've just been in close contact the entire time and just bouncing ideas back and forth. People would be surprised by how much of the alpha I have I got from you, Sholto, Trenton and a couple others.

**Extracted Belief:**

Small grants can have a significant impact.

**Context:**

Leopold Aschenbrenner states this belief while discussing a $20,000 grant he received from the Future Fund that helped sustain Dwarkesh Patel's podcast for six months.

**Justification:**

Aschenbrenner points to his personal experience with a small grant and the impact it had on Patel's podcast as evidence for this belief.

--------

## Chunk 640

**Chunk:**

Dwarkesh Patel
Exactly. Emergent Ventures. The last year I've been in San Francisco, we've just been in close contact the entire time and just bouncing ideas back and forth. People would be surprised by how much of the alpha I have I got from you, Sholto, Trenton and a couple others.
Leopold Aschenbrenner
It’s been an absolute pleasure.
Dwarkesh Patel
Likewise, it's been super fun. Here are some random questions for you. If you could convert to Mormonism and you could really believe it, would you do it? Would you push the button?

**Extracted Belief:**

If I believed in Mormonism, I would convert.

**Context:**

Leopold Aschenbrenner was asked by Dwarkesh Patel if he would convert to Mormonism if he could believe it, and he responded that he would convert if he did believe it.

**Justification:**

The belief is a logical consequence of his stated preference for converting if he were to believe in Mormonism.

--------

## Chunk 641

**Chunk:**

Dwarkesh Patel
Likewise, it's been super fun. Here are some random questions for you. If you could convert to Mormonism and you could really believe it, would you do it? Would you push the button?
Leopold Aschenbrenner
Before I answer that question, one observation about the Mormons. There's an article that actually made a big impact on me. It was about the Mormons, by McKay Coppins in The Atlantic. He even interviewed Mitt Romney in it.

The thing he talked about was how the experience of growing up different, growing up very unusual, especially if you grew up Mormon outside of Utah. You’re the only person who doesn't drink caffeine, you don't drink alcohol, you're kind of weird. That got people prepared for being willing to be outside of the norm later on.

Mitt Romney was willing to take stands alone in his party because he believed what he believed was true. Probably not in the same way, but I feel a little bit like this from having grown up in Germany, having been kind of an outsider or something.

Growing up as an outsider gives you unusual strength later on to be willing to say what you think. So that is one thing I really appreciate about the Mormons, at least the ones that grow up outside of Utah.

The other thing is the fertility rates. They're good. They're important. They're going down as well. This is the thing that really clinched the fertility decline story for me. Even the Mormons.
Dwarkesh Patel
You're like, "oh, this is like a good start. Mormons will replace everybody."

**Extracted Belief:**

Growing up as an outsider, especially in a religious minority like Mormonism outside of Utah, can lead to a strong sense of individuality and the willingness to be different from the norm later in life.

**Context:**

Leopold Aschenbrenner discussed the experience of growing up Mormon outside of Utah and how it prepared individuals for being outside the norm later in life. He referenced an article by McKay Coppins in The Atlantic about Mormons, specifically mentioning Mitt Romney's willingness to take stands alone in his party.

**Justification:**

The belief is based on Leopold Aschenbrenner's interpretation of the article by McKay Coppins, which highlights the experience of Mormons raised outside of Utah and their willingness to be different, exemplified by Mitt Romney's independent stances.

--------

## Chunk 642

**Chunk:**

Dwarkesh Patel
Likewise, it's been super fun. Here are some random questions for you. If you could convert to Mormonism and you could really believe it, would you do it? Would you push the button?
Leopold Aschenbrenner
Before I answer that question, one observation about the Mormons. There's an article that actually made a big impact on me. It was about the Mormons, by McKay Coppins in The Atlantic. He even interviewed Mitt Romney in it.

The thing he talked about was how the experience of growing up different, growing up very unusual, especially if you grew up Mormon outside of Utah. You’re the only person who doesn't drink caffeine, you don't drink alcohol, you're kind of weird. That got people prepared for being willing to be outside of the norm later on.

Mitt Romney was willing to take stands alone in his party because he believed what he believed was true. Probably not in the same way, but I feel a little bit like this from having grown up in Germany, having been kind of an outsider or something.

Growing up as an outsider gives you unusual strength later on to be willing to say what you think. So that is one thing I really appreciate about the Mormons, at least the ones that grow up outside of Utah.

The other thing is the fertility rates. They're good. They're important. They're going down as well. This is the thing that really clinched the fertility decline story for me. Even the Mormons.
Dwarkesh Patel
You're like, "oh, this is like a good start. Mormons will replace everybody."

**Extracted Belief:**

High fertility rates are beneficial and important for society.

**Context:**

Leopold Aschenbrenner discussed the high fertility rates of Mormons and how they contributed to his understanding of fertility decline.

**Justification:**

While the specific justification is not explicitly stated, the statement suggests that Leopold Aschenbrenner considers high fertility rates to be beneficial and important based on some form of evidence or data.

--------

## Chunk 643

**Chunk:**

Dwarkesh Patel
You're like, "oh, this is like a good start. Mormons will replace everybody."
Leopold Aschenbrenner
I don't know if it's good, but at least some people will maintain high fertility rates. But no, even the Mormons... Once these religious subgroups that have high fertility rates grow big enough, they become too close in contact with normal society and become normalized. Their fertility rates drop from maybe like four to two in the course of 10-20 years.

People point to the Amish or whatever, but it's probably just not scalable. If you grow big enough, then there's just this overwhelming force of modernity that gets you.

No, if I could convert to Mormonism... Look, I think there's something... I don't believe it, right? If I believed it, I obviously would convert to Mormonism, because you got to convert.
Dwarkesh Patel
But you can choose a world in which you do believe it.

**Extracted Belief:**

Religious subgroups with high fertility rates, such as Mormons, will eventually see their fertility rates decline when they become more integrated into mainstream society.

**Context:**

Leopold Aschenbrenner is discussing the phenomenon of fertility decline among religious groups with high initial fertility rates, using the example of Mormons.

**Justification:**

He states that "Once these religious subgroups that have high fertility rates grow big enough, they become too close in contact with normal society and become normalized. Their fertility rates drop from maybe like four to two in the course of 10-20 years."

--------

## Chunk 644

**Chunk:**

Dwarkesh Patel
You're like, "oh, this is like a good start. Mormons will replace everybody."
Leopold Aschenbrenner
I don't know if it's good, but at least some people will maintain high fertility rates. But no, even the Mormons... Once these religious subgroups that have high fertility rates grow big enough, they become too close in contact with normal society and become normalized. Their fertility rates drop from maybe like four to two in the course of 10-20 years.

People point to the Amish or whatever, but it's probably just not scalable. If you grow big enough, then there's just this overwhelming force of modernity that gets you.

No, if I could convert to Mormonism... Look, I think there's something... I don't believe it, right? If I believed it, I obviously would convert to Mormonism, because you got to convert.
Dwarkesh Patel
But you can choose a world in which you do believe it.

**Extracted Belief:**

The influence of modernity is a powerful force that can lead to changes in social norms and behaviors, including fertility rates.

**Context:**

Leopold Aschenbrenner is explaining why even high-fertility groups like Mormons eventually experience fertility decline.

**Justification:**

He states, "If you grow big enough, then there's just this overwhelming force of modernity that gets you."

--------

## Chunk 645

**Chunk:**

Dwarkesh Patel
But you can choose a world in which you do believe it.
Leopold Aschenbrenner
There's something really valuable in believing in something greater than yourself and having a certain amount of faith.
Dwarkesh Patel
You do, right? That's what your series is.

**Extracted Belief:**

It is valuable to believe in something greater than oneself and to have faith.

**Context:**

Leopold Aschenbrenner was responding to Dwarkesh Patel's suggestion that he could choose a world in which he believes in Mormonism, and he stated that there is value in believing in something greater than oneself.

**Justification:**

He did not explicitly state his reasoning for this belief, but it aligns with his overall focus on the importance of believing in a greater purpose, which he further expresses in the following statement about a duty to something greater than oneself.

--------

## Chunk 646

**Chunk:**

Dwarkesh Patel
But you can choose a world in which you do believe it.
Leopold Aschenbrenner
There's something really valuable in believing in something greater than yourself and having a certain amount of faith.
Dwarkesh Patel
You do, right? That's what your series is.

**Extracted Belief:**

It is valuable to feel a duty to something greater than oneself.

**Context:**

Leopold Aschenbrenner continued his response to Dwarkesh Patel's suggestion, elaborating on his belief in the value of faith by stating that it is valuable to feel a duty to something greater than oneself.

**Justification:**

He does not explicitly state his reasoning for this belief, but it aligns with his overall focus on the importance of believing in a greater purpose, which he further expresses in the following statement about his feeling of duty to historical weight and national security.

--------

## Chunk 647

**Chunk:**

Dwarkesh Patel
You do, right? That's what your series is.
Leopold Aschenbrenner
It’s valuable to feel some sort of duty to something greater than yourself. Maybe my version of this is somewhat different. I feel some sort of duty to the historical weight on how this might play out. I feel some sort of duty to make that go well. I feel some sort of duty to our country, to the national security of the United States. We can be a force for a lot of good.
Dwarkesh Patel
Going back to OpenAI, there’s something that's especially impressive about that is. There are people at the company who have — through years and decades of building up savings from working in tech — probably tens of millions of dollars liquid and more than that in terms of their equity. Many people were concerned about the clusters and the Middle East and the secrets leaking to China and all these things.

The person who actually made a hassle about it — hassling people is so underrated — is the 22-year-old who has less than a year at the company, who doesn't have savings built up, who isn't a solidified member of the company.

**Extracted Belief:**

Feeling a sense of duty to something greater than oneself is valuable.

**Context:**

Leopold Aschenbrenner is discussing his belief in feeling a sense of duty, although his version of this duty is more focused on the historical weight of events.

**Justification:**

Aschenbrenner explicitly states that feeling a sense of duty is valuable, suggesting a belief in something greater than oneself, but not necessarily in the traditional sense of religion.

--------

## Chunk 648

**Chunk:**

Dwarkesh Patel
You do, right? That's what your series is.
Leopold Aschenbrenner
It’s valuable to feel some sort of duty to something greater than yourself. Maybe my version of this is somewhat different. I feel some sort of duty to the historical weight on how this might play out. I feel some sort of duty to make that go well. I feel some sort of duty to our country, to the national security of the United States. We can be a force for a lot of good.
Dwarkesh Patel
Going back to OpenAI, there’s something that's especially impressive about that is. There are people at the company who have — through years and decades of building up savings from working in tech — probably tens of millions of dollars liquid and more than that in terms of their equity. Many people were concerned about the clusters and the Middle East and the secrets leaking to China and all these things.

The person who actually made a hassle about it — hassling people is so underrated — is the 22-year-old who has less than a year at the company, who doesn't have savings built up, who isn't a solidified member of the company.

**Extracted Belief:**

Leopold Aschenbrenner feels a sense of duty to the historical weight of how events might play out.

**Context:**

Aschenbrenner expresses a belief in a duty to something greater than himself, which in his case, is the historical weight of events.

**Justification:**

He explicitly states that he feels a sense of duty to the historical weight on how events might play out, highlighting his focus on shaping the future.

--------

## Chunk 649

**Chunk:**

Dwarkesh Patel
You do, right? That's what your series is.
Leopold Aschenbrenner
It’s valuable to feel some sort of duty to something greater than yourself. Maybe my version of this is somewhat different. I feel some sort of duty to the historical weight on how this might play out. I feel some sort of duty to make that go well. I feel some sort of duty to our country, to the national security of the United States. We can be a force for a lot of good.
Dwarkesh Patel
Going back to OpenAI, there’s something that's especially impressive about that is. There are people at the company who have — through years and decades of building up savings from working in tech — probably tens of millions of dollars liquid and more than that in terms of their equity. Many people were concerned about the clusters and the Middle East and the secrets leaking to China and all these things.

The person who actually made a hassle about it — hassling people is so underrated — is the 22-year-old who has less than a year at the company, who doesn't have savings built up, who isn't a solidified member of the company.

**Extracted Belief:**

He feels a sense of duty to ensure that the future unfolds in a positive way.

**Context:**

Aschenbrenner expresses a belief in a duty to something greater than himself, which is the historical weight of events.

**Justification:**

He states that he feels a sense of duty to 'make that go well,' suggesting a belief in a responsibility to influence the future positively.

--------

## Chunk 650

**Chunk:**

Dwarkesh Patel
You do, right? That's what your series is.
Leopold Aschenbrenner
It’s valuable to feel some sort of duty to something greater than yourself. Maybe my version of this is somewhat different. I feel some sort of duty to the historical weight on how this might play out. I feel some sort of duty to make that go well. I feel some sort of duty to our country, to the national security of the United States. We can be a force for a lot of good.
Dwarkesh Patel
Going back to OpenAI, there’s something that's especially impressive about that is. There are people at the company who have — through years and decades of building up savings from working in tech — probably tens of millions of dollars liquid and more than that in terms of their equity. Many people were concerned about the clusters and the Middle East and the secrets leaking to China and all these things.

The person who actually made a hassle about it — hassling people is so underrated — is the 22-year-old who has less than a year at the company, who doesn't have savings built up, who isn't a solidified member of the company.

**Extracted Belief:**

Leopold Aschenbrenner feels a sense of duty to the national security of the United States.

**Context:**

Aschenbrenner expresses a belief in a duty to something greater than himself, which he clarifies as being a duty to his country and its national security.

**Justification:**

He explicitly states that he feels a sense of duty to 'our country' and its national security.

--------

## Chunk 651

**Chunk:**

Dwarkesh Patel
You do, right? That's what your series is.
Leopold Aschenbrenner
It’s valuable to feel some sort of duty to something greater than yourself. Maybe my version of this is somewhat different. I feel some sort of duty to the historical weight on how this might play out. I feel some sort of duty to make that go well. I feel some sort of duty to our country, to the national security of the United States. We can be a force for a lot of good.
Dwarkesh Patel
Going back to OpenAI, there’s something that's especially impressive about that is. There are people at the company who have — through years and decades of building up savings from working in tech — probably tens of millions of dollars liquid and more than that in terms of their equity. Many people were concerned about the clusters and the Middle East and the secrets leaking to China and all these things.

The person who actually made a hassle about it — hassling people is so underrated — is the 22-year-old who has less than a year at the company, who doesn't have savings built up, who isn't a solidified member of the company.

**Extracted Belief:**

The United States can be a force for good in the world.

**Context:**

Aschenbrenner expresses a belief in the potential of the United States to be a force for good in the world.

**Justification:**

He states that the United States 'can be a force for a lot of good,' signifying a belief in its positive potential.

--------

## Chunk 652

**Chunk:**

Dwarkesh Patel
Going back to OpenAI, there’s something that's especially impressive about that is. There are people at the company who have — through years and decades of building up savings from working in tech — probably tens of millions of dollars liquid and more than that in terms of their equity. Many people were concerned about the clusters and the Middle East and the secrets leaking to China and all these things.

The person who actually made a hassle about it — hassling people is so underrated — is the 22-year-old who has less than a year at the company, who doesn't have savings built up, who isn't a solidified member of the company.
Leopold Aschenbrenner
Maybe it's me being naive and not knowing how big companies work. Sometimes I'm a bit of a speech deontologist. I kind of believe in saying what you think. Sometimes friends tell me I should be more of a speech consequentialist.
Dwarkesh Patel
I mean I think about the amount of people who, when they have the opportunity to talk to the person, will just bring up the thing. I've been with you in multiple contexts. I guess I shouldn't reveal who the person is or what the context was.

I've just been very impressed that the dinner begins and by the end, somebody who has a major voice in how things go is seriously thinking about a worldview they would have found incredibly alien before the dinner. I've been impressed that you just give them the spiel and hassle them.

**Extracted Belief:**

It is important to express one's genuine thoughts and opinions, even if they may be unpopular or unconventional.

**Context:**

Leopold Aschenbrenner is discussing his approach to communication and his belief in speaking his mind.

**Justification:**

He identifies himself as a 'speech deontologist,' implying a belief in the inherent moral value of truthfulness and open communication.

--------

## Chunk 653

**Chunk:**

Dwarkesh Patel
Going back to OpenAI, there’s something that's especially impressive about that is. There are people at the company who have — through years and decades of building up savings from working in tech — probably tens of millions of dollars liquid and more than that in terms of their equity. Many people were concerned about the clusters and the Middle East and the secrets leaking to China and all these things.

The person who actually made a hassle about it — hassling people is so underrated — is the 22-year-old who has less than a year at the company, who doesn't have savings built up, who isn't a solidified member of the company.
Leopold Aschenbrenner
Maybe it's me being naive and not knowing how big companies work. Sometimes I'm a bit of a speech deontologist. I kind of believe in saying what you think. Sometimes friends tell me I should be more of a speech consequentialist.
Dwarkesh Patel
I mean I think about the amount of people who, when they have the opportunity to talk to the person, will just bring up the thing. I've been with you in multiple contexts. I guess I shouldn't reveal who the person is or what the context was.

I've just been very impressed that the dinner begins and by the end, somebody who has a major voice in how things go is seriously thinking about a worldview they would have found incredibly alien before the dinner. I've been impressed that you just give them the spiel and hassle them.

**Extracted Belief:**

Consequentialism, which focuses on the outcomes of actions, is a valid approach to decision-making, but it is not the only approach.

**Context:**

Leopold Aschenbrenner is contrasting his own belief in 'speech deontology' with the advice of his friends to be more 'speech consequentialist.'

**Justification:**

Aschenbrenner acknowledges the existence and validity of consequentialism as a philosophical approach, suggesting a reasoned understanding of different ethical frameworks.

--------

## Chunk 654

**Chunk:**

Dwarkesh Patel
I mean I think about the amount of people who, when they have the opportunity to talk to the person, will just bring up the thing. I've been with you in multiple contexts. I guess I shouldn't reveal who the person is or what the context was.

I've just been very impressed that the dinner begins and by the end, somebody who has a major voice in how things go is seriously thinking about a worldview they would have found incredibly alien before the dinner. I've been impressed that you just give them the spiel and hassle them.
Leopold Aschenbrenner
I just feel this stuff pretty viscerally now. There was a time when I thought about this stuff a lot, but it was kind of like econ models and these theoretical abstractions. You talk about human brain size or whatever.

Since at least last year, I feel like I can see it. I feel it. I can sort of see the cluster that AGI can be trained on. I can see the kind of rough combination of algorithms and the people that will be involved and how this is going to play out. Look, we'll see how it plays out. There are many ways this could be wrong. There are many ways it could go, but this could get very real.

(04:07:58) – Launching an AGI hedge fund
Dwarkesh Patel
Should we talk about what you're up to next?

**Extracted Belief:**

The potential for the development and impact of artificial general intelligence (AGI) is real and significant.

**Context:**

Leopold Aschenbrenner is discussing his belief in the coming of AGI and its potential impact.

**Justification:**

He states that he 'can see it' and 'feel it,' implying that he has observed evidence or data that supports this belief. He also references the possibility of AGI being trained on a specific 'cluster' of data and algorithms, suggesting he is familiar with the technical aspects of AGI development.

--------

## Chunk 655

**Chunk:**

Dwarkesh Patel
I mean I think about the amount of people who, when they have the opportunity to talk to the person, will just bring up the thing. I've been with you in multiple contexts. I guess I shouldn't reveal who the person is or what the context was.

I've just been very impressed that the dinner begins and by the end, somebody who has a major voice in how things go is seriously thinking about a worldview they would have found incredibly alien before the dinner. I've been impressed that you just give them the spiel and hassle them.
Leopold Aschenbrenner
I just feel this stuff pretty viscerally now. There was a time when I thought about this stuff a lot, but it was kind of like econ models and these theoretical abstractions. You talk about human brain size or whatever.

Since at least last year, I feel like I can see it. I feel it. I can sort of see the cluster that AGI can be trained on. I can see the kind of rough combination of algorithms and the people that will be involved and how this is going to play out. Look, we'll see how it plays out. There are many ways this could be wrong. There are many ways it could go, but this could get very real.

(04:07:58) – Launching an AGI hedge fund
Dwarkesh Patel
Should we talk about what you're up to next?

**Extracted Belief:**

AGI's development is likely to be a transformative event with significant implications for the future.

**Context:**

Leopold Aschenbrenner is discussing his belief in the coming of AGI and its potential impact.

**Justification:**

He states that 'this could get very real,' suggesting that he believes AGI is not just a theoretical concept but a real possibility with concrete implications for the future.

--------

## Chunk 656

**Chunk:**

Dwarkesh Patel
Should we talk about what you're up to next?
Leopold Aschenbrenner
Sure, yeah.
Dwarkesh Patel
You're starting an investment firm with anchor investments from Nat Friedman, Daniel Gross, Patrick Collison, John Collison. First of all, why is this the thing to do if you believe AGI is coming in a few years? Why the investment firm?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 657

**Chunk:**

Dwarkesh Patel
You're starting an investment firm with anchor investments from Nat Friedman, Daniel Gross, Patrick Collison, John Collison. First of all, why is this the thing to do if you believe AGI is coming in a few years? Why the investment firm?
Leopold Aschenbrenner
Good question. Fair question. A couple of things. We talked about this earlier, but the screen doesn't go blank when AGI intelligence happens. People really underrate the decade after you have the intelligence explosion. That's maybe the most wild period. The decade after is also going to be wild.

This combination of human institutions with superintelligence and crazy geopolitical things going on. You have this broadening of this explosive growth. Basically, it's going to be a really important period. Capital will really matter. Eventually we're going to go to the stars, going to go to the galaxies.

Part of the answer is just that done right, there's a lot of money to be made. If AGI were priced in tomorrow, you could maybe make 100x. Probably you can make even way more than that because of the sequencing and capital matters.

The other reason is just some amount of freedom and independence. There are some people who are very smart about this AI stuff and who see it coming. Almost all of them are constrained in various ways. They're in the labs, they're in some other position where they can't really talk about this stuff.

I've really admired the thing you've done. It's really important that there are voices of reason on this stuff publicly or people who are in positions to kind of advise important actors and so on.

Basically, this investment firm will be kind of like a brain trust on AI. It's going to be all about situational awareness. We're going to have the best situational awareness in the business. We're going to have way more situational awareness than any of the people who manage money in New York. We're definitely going to do great on investing, but it's the same sort of situational awareness that is going to be important for understanding what's happening, being a voice of reason publicly, and being able to be in a position to advise.
Dwarkesh Patel
The book about Peter Thiel, they had an interesting quote about his hedge fund. It got terrible returns. So this isn't the example...

**Extracted Belief:**

The decade following the emergence of artificial general intelligence (AGI) will be a very significant period, marked by significant changes and developments.

**Context:**

Leopold Aschenbrenner is explaining the rationale behind his decision to start an investment firm focused on AI, arguing that the period after AGI's arrival will be highly impactful.

**Justification:**

Aschenbrenner emphasizes that the decade following AGI will be 'the most wild period' and 'really important', citing the combination of superintelligence with human institutions and geopolitical factors.

--------

## Chunk 658

**Chunk:**

Dwarkesh Patel
You're starting an investment firm with anchor investments from Nat Friedman, Daniel Gross, Patrick Collison, John Collison. First of all, why is this the thing to do if you believe AGI is coming in a few years? Why the investment firm?
Leopold Aschenbrenner
Good question. Fair question. A couple of things. We talked about this earlier, but the screen doesn't go blank when AGI intelligence happens. People really underrate the decade after you have the intelligence explosion. That's maybe the most wild period. The decade after is also going to be wild.

This combination of human institutions with superintelligence and crazy geopolitical things going on. You have this broadening of this explosive growth. Basically, it's going to be a really important period. Capital will really matter. Eventually we're going to go to the stars, going to go to the galaxies.

Part of the answer is just that done right, there's a lot of money to be made. If AGI were priced in tomorrow, you could maybe make 100x. Probably you can make even way more than that because of the sequencing and capital matters.

The other reason is just some amount of freedom and independence. There are some people who are very smart about this AI stuff and who see it coming. Almost all of them are constrained in various ways. They're in the labs, they're in some other position where they can't really talk about this stuff.

I've really admired the thing you've done. It's really important that there are voices of reason on this stuff publicly or people who are in positions to kind of advise important actors and so on.

Basically, this investment firm will be kind of like a brain trust on AI. It's going to be all about situational awareness. We're going to have the best situational awareness in the business. We're going to have way more situational awareness than any of the people who manage money in New York. We're definitely going to do great on investing, but it's the same sort of situational awareness that is going to be important for understanding what's happening, being a voice of reason publicly, and being able to be in a position to advise.
Dwarkesh Patel
The book about Peter Thiel, they had an interesting quote about his hedge fund. It got terrible returns. So this isn't the example...

**Extracted Belief:**

Capital will play a crucial role in the period following the emergence of artificial general intelligence (AGI).

**Context:**

Aschenbrenner is explaining why he is starting an investment firm focused on AI, highlighting the significance of capital in navigating the post-AGI era.

**Justification:**

He states that 'capital will really matter' in this period and that it will be a 'really important period'.

--------

## Chunk 659

**Chunk:**

Dwarkesh Patel
You're starting an investment firm with anchor investments from Nat Friedman, Daniel Gross, Patrick Collison, John Collison. First of all, why is this the thing to do if you believe AGI is coming in a few years? Why the investment firm?
Leopold Aschenbrenner
Good question. Fair question. A couple of things. We talked about this earlier, but the screen doesn't go blank when AGI intelligence happens. People really underrate the decade after you have the intelligence explosion. That's maybe the most wild period. The decade after is also going to be wild.

This combination of human institutions with superintelligence and crazy geopolitical things going on. You have this broadening of this explosive growth. Basically, it's going to be a really important period. Capital will really matter. Eventually we're going to go to the stars, going to go to the galaxies.

Part of the answer is just that done right, there's a lot of money to be made. If AGI were priced in tomorrow, you could maybe make 100x. Probably you can make even way more than that because of the sequencing and capital matters.

The other reason is just some amount of freedom and independence. There are some people who are very smart about this AI stuff and who see it coming. Almost all of them are constrained in various ways. They're in the labs, they're in some other position where they can't really talk about this stuff.

I've really admired the thing you've done. It's really important that there are voices of reason on this stuff publicly or people who are in positions to kind of advise important actors and so on.

Basically, this investment firm will be kind of like a brain trust on AI. It's going to be all about situational awareness. We're going to have the best situational awareness in the business. We're going to have way more situational awareness than any of the people who manage money in New York. We're definitely going to do great on investing, but it's the same sort of situational awareness that is going to be important for understanding what's happening, being a voice of reason publicly, and being able to be in a position to advise.
Dwarkesh Patel
The book about Peter Thiel, they had an interesting quote about his hedge fund. It got terrible returns. So this isn't the example...

**Extracted Belief:**

The arrival of AGI will not result in an immediate and complete transformation, but rather a gradual and complex process with a significant period of transition.

**Context:**

Aschenbrenner refutes the notion that the world will instantly change with AGI, emphasizing the importance of the period after the initial emergence of AGI.

**Justification:**

He states that 'the screen doesn't go blank' when AGI arrives, suggesting that there will be a gradual transition, and that the decade following AGI will be significant.

--------

## Chunk 660

**Chunk:**

Dwarkesh Patel
You're starting an investment firm with anchor investments from Nat Friedman, Daniel Gross, Patrick Collison, John Collison. First of all, why is this the thing to do if you believe AGI is coming in a few years? Why the investment firm?
Leopold Aschenbrenner
Good question. Fair question. A couple of things. We talked about this earlier, but the screen doesn't go blank when AGI intelligence happens. People really underrate the decade after you have the intelligence explosion. That's maybe the most wild period. The decade after is also going to be wild.

This combination of human institutions with superintelligence and crazy geopolitical things going on. You have this broadening of this explosive growth. Basically, it's going to be a really important period. Capital will really matter. Eventually we're going to go to the stars, going to go to the galaxies.

Part of the answer is just that done right, there's a lot of money to be made. If AGI were priced in tomorrow, you could maybe make 100x. Probably you can make even way more than that because of the sequencing and capital matters.

The other reason is just some amount of freedom and independence. There are some people who are very smart about this AI stuff and who see it coming. Almost all of them are constrained in various ways. They're in the labs, they're in some other position where they can't really talk about this stuff.

I've really admired the thing you've done. It's really important that there are voices of reason on this stuff publicly or people who are in positions to kind of advise important actors and so on.

Basically, this investment firm will be kind of like a brain trust on AI. It's going to be all about situational awareness. We're going to have the best situational awareness in the business. We're going to have way more situational awareness than any of the people who manage money in New York. We're definitely going to do great on investing, but it's the same sort of situational awareness that is going to be important for understanding what's happening, being a voice of reason publicly, and being able to be in a position to advise.
Dwarkesh Patel
The book about Peter Thiel, they had an interesting quote about his hedge fund. It got terrible returns. So this isn't the example...

**Extracted Belief:**

There is significant potential for financial gains in the AI industry, especially in the period following the emergence of artificial general intelligence (AGI).

**Context:**

Aschenbrenner is explaining his motivation for starting an investment firm focused on AI, emphasizing the financial opportunities presented by AGI.

**Justification:**

He asserts that 'done right, there's a lot of money to be made' and that 'you could maybe make 100x' if AGI were priced in today.

--------

## Chunk 661

**Chunk:**

Dwarkesh Patel
You're starting an investment firm with anchor investments from Nat Friedman, Daniel Gross, Patrick Collison, John Collison. First of all, why is this the thing to do if you believe AGI is coming in a few years? Why the investment firm?
Leopold Aschenbrenner
Good question. Fair question. A couple of things. We talked about this earlier, but the screen doesn't go blank when AGI intelligence happens. People really underrate the decade after you have the intelligence explosion. That's maybe the most wild period. The decade after is also going to be wild.

This combination of human institutions with superintelligence and crazy geopolitical things going on. You have this broadening of this explosive growth. Basically, it's going to be a really important period. Capital will really matter. Eventually we're going to go to the stars, going to go to the galaxies.

Part of the answer is just that done right, there's a lot of money to be made. If AGI were priced in tomorrow, you could maybe make 100x. Probably you can make even way more than that because of the sequencing and capital matters.

The other reason is just some amount of freedom and independence. There are some people who are very smart about this AI stuff and who see it coming. Almost all of them are constrained in various ways. They're in the labs, they're in some other position where they can't really talk about this stuff.

I've really admired the thing you've done. It's really important that there are voices of reason on this stuff publicly or people who are in positions to kind of advise important actors and so on.

Basically, this investment firm will be kind of like a brain trust on AI. It's going to be all about situational awareness. We're going to have the best situational awareness in the business. We're going to have way more situational awareness than any of the people who manage money in New York. We're definitely going to do great on investing, but it's the same sort of situational awareness that is going to be important for understanding what's happening, being a voice of reason publicly, and being able to be in a position to advise.
Dwarkesh Patel
The book about Peter Thiel, they had an interesting quote about his hedge fund. It got terrible returns. So this isn't the example...

**Extracted Belief:**

Many individuals who are knowledgeable about artificial intelligence (AI) and its potential are restricted in their ability to discuss and explore these ideas openly.

**Context:**

Aschenbrenner is explaining the need for a platform to discuss AI and its implications openly, referencing the constraints faced by many experts in the field.

**Justification:**

He states that 'almost all of them are constrained in various ways' and that they are often 'in the labs, they're in some other position where they can't really talk about this stuff.'

--------

## Chunk 662

**Chunk:**

Dwarkesh Patel
You're starting an investment firm with anchor investments from Nat Friedman, Daniel Gross, Patrick Collison, John Collison. First of all, why is this the thing to do if you believe AGI is coming in a few years? Why the investment firm?
Leopold Aschenbrenner
Good question. Fair question. A couple of things. We talked about this earlier, but the screen doesn't go blank when AGI intelligence happens. People really underrate the decade after you have the intelligence explosion. That's maybe the most wild period. The decade after is also going to be wild.

This combination of human institutions with superintelligence and crazy geopolitical things going on. You have this broadening of this explosive growth. Basically, it's going to be a really important period. Capital will really matter. Eventually we're going to go to the stars, going to go to the galaxies.

Part of the answer is just that done right, there's a lot of money to be made. If AGI were priced in tomorrow, you could maybe make 100x. Probably you can make even way more than that because of the sequencing and capital matters.

The other reason is just some amount of freedom and independence. There are some people who are very smart about this AI stuff and who see it coming. Almost all of them are constrained in various ways. They're in the labs, they're in some other position where they can't really talk about this stuff.

I've really admired the thing you've done. It's really important that there are voices of reason on this stuff publicly or people who are in positions to kind of advise important actors and so on.

Basically, this investment firm will be kind of like a brain trust on AI. It's going to be all about situational awareness. We're going to have the best situational awareness in the business. We're going to have way more situational awareness than any of the people who manage money in New York. We're definitely going to do great on investing, but it's the same sort of situational awareness that is going to be important for understanding what's happening, being a voice of reason publicly, and being able to be in a position to advise.
Dwarkesh Patel
The book about Peter Thiel, they had an interesting quote about his hedge fund. It got terrible returns. So this isn't the example...

**Extracted Belief:**

It is crucial for informed voices to be present in public discourse and to advise decision-makers on matters related to artificial intelligence (AI).

**Context:**

Aschenbrenner emphasizes the importance of public discourse on AI and the need for experts to advise decision-makers.

**Justification:**

He states that it's 'really important that there are voices of reason on this stuff publicly' and 'people who are in positions to kind of advise important actors.'

--------

## Chunk 663

**Chunk:**

Dwarkesh Patel
You're starting an investment firm with anchor investments from Nat Friedman, Daniel Gross, Patrick Collison, John Collison. First of all, why is this the thing to do if you believe AGI is coming in a few years? Why the investment firm?
Leopold Aschenbrenner
Good question. Fair question. A couple of things. We talked about this earlier, but the screen doesn't go blank when AGI intelligence happens. People really underrate the decade after you have the intelligence explosion. That's maybe the most wild period. The decade after is also going to be wild.

This combination of human institutions with superintelligence and crazy geopolitical things going on. You have this broadening of this explosive growth. Basically, it's going to be a really important period. Capital will really matter. Eventually we're going to go to the stars, going to go to the galaxies.

Part of the answer is just that done right, there's a lot of money to be made. If AGI were priced in tomorrow, you could maybe make 100x. Probably you can make even way more than that because of the sequencing and capital matters.

The other reason is just some amount of freedom and independence. There are some people who are very smart about this AI stuff and who see it coming. Almost all of them are constrained in various ways. They're in the labs, they're in some other position where they can't really talk about this stuff.

I've really admired the thing you've done. It's really important that there are voices of reason on this stuff publicly or people who are in positions to kind of advise important actors and so on.

Basically, this investment firm will be kind of like a brain trust on AI. It's going to be all about situational awareness. We're going to have the best situational awareness in the business. We're going to have way more situational awareness than any of the people who manage money in New York. We're definitely going to do great on investing, but it's the same sort of situational awareness that is going to be important for understanding what's happening, being a voice of reason publicly, and being able to be in a position to advise.
Dwarkesh Patel
The book about Peter Thiel, they had an interesting quote about his hedge fund. It got terrible returns. So this isn't the example...

**Extracted Belief:**

Situational awareness is critical for understanding and navigating the landscape of artificial intelligence (AI).

**Context:**

Aschenbrenner explains the rationale behind his investment firm's focus on situational awareness in the context of AI.

**Justification:**

He states that the firm will be 'all about situational awareness' and that they will have 'way more situational awareness than any of the people who manage money in New York.'

--------

## Chunk 664

**Chunk:**

Dwarkesh Patel
The book about Peter Thiel, they had an interesting quote about his hedge fund. It got terrible returns. So this isn't the example...
Leopold Aschenbrenner
It blew up. That’s sort of the bear case. It’s too theoretical.
Dwarkesh Patel
They had an interesting quote that it's basically a think tank inside of a hedge fund.

**Extracted Belief:**

Peter Thiel's hedge fund 'blew up' because it was too theoretical.

**Context:**

Leopold Aschenbrenner was responding to Dwarkesh Patel's comment about Peter Thiel's hedge fund having terrible returns, and he explained the reason for this outcome.

**Justification:**

Leopold Aschenbrenner did not provide any specific data or evidence to support this belief. However, he stated that Thiel's hedge fund 'blew up' because it was 'too theoretical.' This suggests that he is relying on the testimony of others or on his own experience or understanding of the hedge fund industry to reach this conclusion. 

--------

## Chunk 665

**Chunk:**

Dwarkesh Patel
They had an interesting quote that it's basically a think tank inside of a hedge fund.
Leopold Aschenbrenner
That’s what I’m going to try to build.
Dwarkesh Patel
Presumably you've thought about the ways in which these kinds of things can blow up. There's a lot of interesting business history books about people who got the thesis right but timed it wrong. They buy into the idea that the Internet's going to be a big deal. They sell at the wrong time and buy at the wrong time during the dot-com boom. They miss out on the gains even though they're right about the. What is the trick to preventing that kind of thing?

**Extracted Belief:**

The investment firm will be structured as a 'think tank inside of a hedge fund,' focusing on research and analysis alongside investment activities.

**Context:**

Responding to a comparison of the firm to Peter Thiel's hedge fund, Leopold Aschenbrenner confirms his intention to build a similar entity.

**Justification:**

Aschenbrenner states that he plans to build a think tank inside of a hedge fund, echoing the description of Peter Thiel's hedge fund.

--------

## Chunk 666

**Chunk:**

Dwarkesh Patel
Presumably you've thought about the ways in which these kinds of things can blow up. There's a lot of interesting business history books about people who got the thesis right but timed it wrong. They buy into the idea that the Internet's going to be a big deal. They sell at the wrong time and buy at the wrong time during the dot-com boom. They miss out on the gains even though they're right about the. What is the trick to preventing that kind of thing?
Leopold Aschenbrenner
Obviously, not blowing up is task number one and two. This investment firm is going to just be betting on AGI. We’re going to be betting on AGI and superintelligence before the decade is out, taking that seriously, making the bets you would make if you took that seriously. If that's wrong, the firm is not going to do that well.

The thing you have to be resistant to is you have to be able to resist one or a couple or a few individual calls. AI stagnates for a year because of the data wall, or you got the call wrong on when revenue would go up. That's pretty critical. You have to get the timing right. The sequence of bets on the way to AGI is actually pretty critical. People underrate it. 

Where does the story start? Obviously, the only bet over the last year was Nvidia. It's obvious now, very few people did it. This is also a classic debate I and a friend had with another colleague of ours. This colleague was really into TSMC. He was just kind of like, "well, these fabs are going to be so valuable. With Nvidia, there's just a lot of idiosyncratic risk, right? Maybe somebody else will make better GPUs." That was basically right.

But only Nvidia had the AI beta, because only Nvidia was kind of like large fraction AI. The next few doublings would just meaningfully explode their revenue, whereas TSMC was a couple percent AI. Even though there's going to be a few doublings of AI, it was not going to make that big of an impact. The only place to find the AI beta, basically was Nvidia for a while.

Now it's broadening. Now TSMC is like 20% AI by 2027 or something. That’s what they're saying. When we're doubling, it'll be kind of like a large fraction of what they're doing. There's a whole stack. There's people making memory and coops and power. Utilities companies are starting to get excited about AI. They're like, "power production in the United States will grow not 2.5%, but 5% over the next five years." I'm like, "no, it'll grow more.”

At some point, a Google or something becomes interesting. People are excited about them with AI because it's like, "oh, AI revenue will be $10 billion or tens of billions." I don't really care about them before then. I care about it once you get the AI beta. At some point Google will get $100 billion of revenue from AI. Probably their stock will explode. They're going to become a $5 trillion, $10 trillion company anyway.

The timing there is very important. You have to get the timing right. You have to get the sequence right. At some point, actually, there's going to be real headwind to equities from real interest rates. In these sorts of explosive growth worlds, you would expect real interest rates to go up a lot. On the supply side it’ll be around the demand for money because people are going to be making these crazy investments, initially in clusters and then in the robo factories or whatever. They're going to be borrowing like crazy. They want all this capital, high ROI.

On the consumer saving side, to give up all this capital, it’ll be the Euler equation, standard intertemporal transfer trade-off of consumption.
Dwarkesh Patel
Very standard.

**Extracted Belief:**

The success of an investment firm focused on Artificial General Intelligence (AGI) and superintelligence hinges on the accurate prediction of AGI's arrival within the next decade.

**Context:**

Leopold Aschenbrenner describes his investment firm's strategy of betting on AGI and superintelligence before the decade is out, acknowledging that if this prediction is wrong, the firm will not perform well.

**Justification:**

The belief is based on the assumption that AGI and superintelligence will develop within the next decade, which is an empirical prediction about the future development of technology.

--------

## Chunk 667

**Chunk:**

Dwarkesh Patel
Presumably you've thought about the ways in which these kinds of things can blow up. There's a lot of interesting business history books about people who got the thesis right but timed it wrong. They buy into the idea that the Internet's going to be a big deal. They sell at the wrong time and buy at the wrong time during the dot-com boom. They miss out on the gains even though they're right about the. What is the trick to preventing that kind of thing?
Leopold Aschenbrenner
Obviously, not blowing up is task number one and two. This investment firm is going to just be betting on AGI. We’re going to be betting on AGI and superintelligence before the decade is out, taking that seriously, making the bets you would make if you took that seriously. If that's wrong, the firm is not going to do that well.

The thing you have to be resistant to is you have to be able to resist one or a couple or a few individual calls. AI stagnates for a year because of the data wall, or you got the call wrong on when revenue would go up. That's pretty critical. You have to get the timing right. The sequence of bets on the way to AGI is actually pretty critical. People underrate it. 

Where does the story start? Obviously, the only bet over the last year was Nvidia. It's obvious now, very few people did it. This is also a classic debate I and a friend had with another colleague of ours. This colleague was really into TSMC. He was just kind of like, "well, these fabs are going to be so valuable. With Nvidia, there's just a lot of idiosyncratic risk, right? Maybe somebody else will make better GPUs." That was basically right.

But only Nvidia had the AI beta, because only Nvidia was kind of like large fraction AI. The next few doublings would just meaningfully explode their revenue, whereas TSMC was a couple percent AI. Even though there's going to be a few doublings of AI, it was not going to make that big of an impact. The only place to find the AI beta, basically was Nvidia for a while.

Now it's broadening. Now TSMC is like 20% AI by 2027 or something. That’s what they're saying. When we're doubling, it'll be kind of like a large fraction of what they're doing. There's a whole stack. There's people making memory and coops and power. Utilities companies are starting to get excited about AI. They're like, "power production in the United States will grow not 2.5%, but 5% over the next five years." I'm like, "no, it'll grow more.”

At some point, a Google or something becomes interesting. People are excited about them with AI because it's like, "oh, AI revenue will be $10 billion or tens of billions." I don't really care about them before then. I care about it once you get the AI beta. At some point Google will get $100 billion of revenue from AI. Probably their stock will explode. They're going to become a $5 trillion, $10 trillion company anyway.

The timing there is very important. You have to get the timing right. You have to get the sequence right. At some point, actually, there's going to be real headwind to equities from real interest rates. In these sorts of explosive growth worlds, you would expect real interest rates to go up a lot. On the supply side it’ll be around the demand for money because people are going to be making these crazy investments, initially in clusters and then in the robo factories or whatever. They're going to be borrowing like crazy. They want all this capital, high ROI.

On the consumer saving side, to give up all this capital, it’ll be the Euler equation, standard intertemporal transfer trade-off of consumption.
Dwarkesh Patel
Very standard.

**Extracted Belief:**

The timing and sequence of investments leading up to the development of AGI are crucial for investment success.

**Context:**

Leopold Aschenbrenner emphasizes the importance of getting the timing and sequence of investments right, suggesting that failing to do so can lead to missed opportunities and losses even if the overall thesis about AGI is correct.

**Justification:**

This belief is based on the historical example of investors who missed out on the gains of the dot-com boom despite being right about the internet's potential, highlighting the importance of timing and sequencing in investments.

--------

## Chunk 668

**Chunk:**

Dwarkesh Patel
Presumably you've thought about the ways in which these kinds of things can blow up. There's a lot of interesting business history books about people who got the thesis right but timed it wrong. They buy into the idea that the Internet's going to be a big deal. They sell at the wrong time and buy at the wrong time during the dot-com boom. They miss out on the gains even though they're right about the. What is the trick to preventing that kind of thing?
Leopold Aschenbrenner
Obviously, not blowing up is task number one and two. This investment firm is going to just be betting on AGI. We’re going to be betting on AGI and superintelligence before the decade is out, taking that seriously, making the bets you would make if you took that seriously. If that's wrong, the firm is not going to do that well.

The thing you have to be resistant to is you have to be able to resist one or a couple or a few individual calls. AI stagnates for a year because of the data wall, or you got the call wrong on when revenue would go up. That's pretty critical. You have to get the timing right. The sequence of bets on the way to AGI is actually pretty critical. People underrate it. 

Where does the story start? Obviously, the only bet over the last year was Nvidia. It's obvious now, very few people did it. This is also a classic debate I and a friend had with another colleague of ours. This colleague was really into TSMC. He was just kind of like, "well, these fabs are going to be so valuable. With Nvidia, there's just a lot of idiosyncratic risk, right? Maybe somebody else will make better GPUs." That was basically right.

But only Nvidia had the AI beta, because only Nvidia was kind of like large fraction AI. The next few doublings would just meaningfully explode their revenue, whereas TSMC was a couple percent AI. Even though there's going to be a few doublings of AI, it was not going to make that big of an impact. The only place to find the AI beta, basically was Nvidia for a while.

Now it's broadening. Now TSMC is like 20% AI by 2027 or something. That’s what they're saying. When we're doubling, it'll be kind of like a large fraction of what they're doing. There's a whole stack. There's people making memory and coops and power. Utilities companies are starting to get excited about AI. They're like, "power production in the United States will grow not 2.5%, but 5% over the next five years." I'm like, "no, it'll grow more.”

At some point, a Google or something becomes interesting. People are excited about them with AI because it's like, "oh, AI revenue will be $10 billion or tens of billions." I don't really care about them before then. I care about it once you get the AI beta. At some point Google will get $100 billion of revenue from AI. Probably their stock will explode. They're going to become a $5 trillion, $10 trillion company anyway.

The timing there is very important. You have to get the timing right. You have to get the sequence right. At some point, actually, there's going to be real headwind to equities from real interest rates. In these sorts of explosive growth worlds, you would expect real interest rates to go up a lot. On the supply side it’ll be around the demand for money because people are going to be making these crazy investments, initially in clusters and then in the robo factories or whatever. They're going to be borrowing like crazy. They want all this capital, high ROI.

On the consumer saving side, to give up all this capital, it’ll be the Euler equation, standard intertemporal transfer trade-off of consumption.
Dwarkesh Patel
Very standard.

**Extracted Belief:**

The development of AGI will lead to a significant increase in demand for computing power, driving the growth of companies like Nvidia.

**Context:**

Leopold Aschenbrenner discusses the growth of Nvidia, attributing it to the company's focus on artificial intelligence and its ability to capitalize on the growing demand for computing power driven by AI development.

**Justification:**

This belief is based on the observed connection between the development of AI and the increased demand for computing power, highlighting the specific role of companies like Nvidia in this market.

--------

## Chunk 669

**Chunk:**

Dwarkesh Patel
Presumably you've thought about the ways in which these kinds of things can blow up. There's a lot of interesting business history books about people who got the thesis right but timed it wrong. They buy into the idea that the Internet's going to be a big deal. They sell at the wrong time and buy at the wrong time during the dot-com boom. They miss out on the gains even though they're right about the. What is the trick to preventing that kind of thing?
Leopold Aschenbrenner
Obviously, not blowing up is task number one and two. This investment firm is going to just be betting on AGI. We’re going to be betting on AGI and superintelligence before the decade is out, taking that seriously, making the bets you would make if you took that seriously. If that's wrong, the firm is not going to do that well.

The thing you have to be resistant to is you have to be able to resist one or a couple or a few individual calls. AI stagnates for a year because of the data wall, or you got the call wrong on when revenue would go up. That's pretty critical. You have to get the timing right. The sequence of bets on the way to AGI is actually pretty critical. People underrate it. 

Where does the story start? Obviously, the only bet over the last year was Nvidia. It's obvious now, very few people did it. This is also a classic debate I and a friend had with another colleague of ours. This colleague was really into TSMC. He was just kind of like, "well, these fabs are going to be so valuable. With Nvidia, there's just a lot of idiosyncratic risk, right? Maybe somebody else will make better GPUs." That was basically right.

But only Nvidia had the AI beta, because only Nvidia was kind of like large fraction AI. The next few doublings would just meaningfully explode their revenue, whereas TSMC was a couple percent AI. Even though there's going to be a few doublings of AI, it was not going to make that big of an impact. The only place to find the AI beta, basically was Nvidia for a while.

Now it's broadening. Now TSMC is like 20% AI by 2027 or something. That’s what they're saying. When we're doubling, it'll be kind of like a large fraction of what they're doing. There's a whole stack. There's people making memory and coops and power. Utilities companies are starting to get excited about AI. They're like, "power production in the United States will grow not 2.5%, but 5% over the next five years." I'm like, "no, it'll grow more.”

At some point, a Google or something becomes interesting. People are excited about them with AI because it's like, "oh, AI revenue will be $10 billion or tens of billions." I don't really care about them before then. I care about it once you get the AI beta. At some point Google will get $100 billion of revenue from AI. Probably their stock will explode. They're going to become a $5 trillion, $10 trillion company anyway.

The timing there is very important. You have to get the timing right. You have to get the sequence right. At some point, actually, there's going to be real headwind to equities from real interest rates. In these sorts of explosive growth worlds, you would expect real interest rates to go up a lot. On the supply side it’ll be around the demand for money because people are going to be making these crazy investments, initially in clusters and then in the robo factories or whatever. They're going to be borrowing like crazy. They want all this capital, high ROI.

On the consumer saving side, to give up all this capital, it’ll be the Euler equation, standard intertemporal transfer trade-off of consumption.
Dwarkesh Patel
Very standard.

**Extracted Belief:**

The development of AGI will lead to a significant increase in real interest rates due to increased demand for capital and decreased consumer savings.

**Context:**

Leopold Aschenbrenner predicts that the development of AGI will cause a significant increase in real interest rates due to a combination of increased demand for capital and decreased consumer savings.

**Justification:**

This belief is based on the economic theory of the Euler equation, which suggests that higher expected growth rates lead to higher interest rates as consumers are less willing to save for future consumption. The belief also ties this economic theory to the expected impact of AGI development on capital investment and consumer behavior.

--------

## Chunk 670

**Chunk:**

Dwarkesh Patel
Very standard.
Leopold Aschenbrenner
Some of our friends have a paper on this. Basically, if consumers expect real growth rates to be higher, interest rates are going to be higher because they're less willing to give up consumption today for consumption in the future.

At some point real interest rates will go up. Higher growth rate expectations mean equities go down because the interest rate effect outweighs the growth rate effect.

At some point there's the big bond short. You got to get that right. You got to get it right on nationalization. There's this whole sequence of things.
Leopold Aschenbrenner
Unknown unknowns, yeah. You've got to be really, really careful about your overall risk positioning. If you expect these crazy events to play out, there's going to be crazy things you didn't foresee.

You do also want to make the bets that are tailored to your scenarios in the sense of you want to find bets that are bets on the tails. I don't think anyone is expecting interest rates to go above 10%, real interest rates. There's at least a serious chance of that before the decade is out. Maybe there's some cheap insurance you can buy on that.
Dwarkesh Patel
Very silly question. In these worlds, are financial markets where you make these kinds of bets going to be respected? Is my Fidelity account going to mean anything when we have 50% economic growth? Who’s like, “we have to respect his property rights”?

**Extracted Belief:**

If consumers expect higher real growth rates, real interest rates will also be higher because they are less willing to give up consumption today for consumption in the future.

**Context:**

Leopold Aschenbrenner explains the relationship between consumer expectations of real growth rates and real interest rates, building on an earlier point about the role of the Euler equation in intertemporal trade-offs.

**Justification:**

Leopold Aschenbrenner derives this belief from the standard economic principle of the Euler equation, which suggests that consumers will demand higher interest rates when they anticipate higher future economic growth. This principle is based on the logic that consumers are less willing to delay consumption today when they expect higher returns on their savings in the future.

--------

## Chunk 671

**Chunk:**

Dwarkesh Patel
Very standard.
Leopold Aschenbrenner
Some of our friends have a paper on this. Basically, if consumers expect real growth rates to be higher, interest rates are going to be higher because they're less willing to give up consumption today for consumption in the future.

At some point real interest rates will go up. Higher growth rate expectations mean equities go down because the interest rate effect outweighs the growth rate effect.

At some point there's the big bond short. You got to get that right. You got to get it right on nationalization. There's this whole sequence of things.
Leopold Aschenbrenner
Unknown unknowns, yeah. You've got to be really, really careful about your overall risk positioning. If you expect these crazy events to play out, there's going to be crazy things you didn't foresee.

You do also want to make the bets that are tailored to your scenarios in the sense of you want to find bets that are bets on the tails. I don't think anyone is expecting interest rates to go above 10%, real interest rates. There's at least a serious chance of that before the decade is out. Maybe there's some cheap insurance you can buy on that.
Dwarkesh Patel
Very silly question. In these worlds, are financial markets where you make these kinds of bets going to be respected? Is my Fidelity account going to mean anything when we have 50% economic growth? Who’s like, “we have to respect his property rights”?

**Extracted Belief:**

Higher growth rate expectations will cause equity prices to decline because the interest rate effect will outweigh the growth rate effect.

**Context:**

Leopold Aschenbrenner explains how higher growth rate expectations can lead to a decrease in equity prices due to the combined effect of increased interest rates and economic growth.

**Justification:**

Leopold Aschenbrenner applies logical reasoning by connecting the earlier belief about higher real interest rates due to higher growth expectations to the impact on equity prices. This belief is based on the assumption that higher interest rates are a negative factor for equity valuations, while higher growth rates are a positive factor. He asserts that the negative impact of higher interest rates outweighs the positive impact of growth in this specific scenario.

--------

## Chunk 672

**Chunk:**

Dwarkesh Patel
Very standard.
Leopold Aschenbrenner
Some of our friends have a paper on this. Basically, if consumers expect real growth rates to be higher, interest rates are going to be higher because they're less willing to give up consumption today for consumption in the future.

At some point real interest rates will go up. Higher growth rate expectations mean equities go down because the interest rate effect outweighs the growth rate effect.

At some point there's the big bond short. You got to get that right. You got to get it right on nationalization. There's this whole sequence of things.
Leopold Aschenbrenner
Unknown unknowns, yeah. You've got to be really, really careful about your overall risk positioning. If you expect these crazy events to play out, there's going to be crazy things you didn't foresee.

You do also want to make the bets that are tailored to your scenarios in the sense of you want to find bets that are bets on the tails. I don't think anyone is expecting interest rates to go above 10%, real interest rates. There's at least a serious chance of that before the decade is out. Maybe there's some cheap insurance you can buy on that.
Dwarkesh Patel
Very silly question. In these worlds, are financial markets where you make these kinds of bets going to be respected? Is my Fidelity account going to mean anything when we have 50% economic growth? Who’s like, “we have to respect his property rights”?

**Extracted Belief:**

There is a possibility that real interest rates could exceed 10% before the end of the decade.

**Context:**

Leopold Aschenbrenner emphasizes the importance of considering potential tail events and the need to hedge against them. He suggests that, while many people might not expect interest rates to rise above 10%, it is a possibility that needs to be considered.

**Justification:**

Leopold Aschenbrenner expresses this belief with high confidence, stating that there is a "serious chance" of interest rates exceeding 10%. While he does not explicitly cite any specific data or evidence, his confidence suggests that his belief is informed by his own analysis and understanding of economic trends and potential scenarios.

--------

## Chunk 673

**Chunk:**

Dwarkesh Patel
Very standard.
Leopold Aschenbrenner
Some of our friends have a paper on this. Basically, if consumers expect real growth rates to be higher, interest rates are going to be higher because they're less willing to give up consumption today for consumption in the future.

At some point real interest rates will go up. Higher growth rate expectations mean equities go down because the interest rate effect outweighs the growth rate effect.

At some point there's the big bond short. You got to get that right. You got to get it right on nationalization. There's this whole sequence of things.
Leopold Aschenbrenner
Unknown unknowns, yeah. You've got to be really, really careful about your overall risk positioning. If you expect these crazy events to play out, there's going to be crazy things you didn't foresee.

You do also want to make the bets that are tailored to your scenarios in the sense of you want to find bets that are bets on the tails. I don't think anyone is expecting interest rates to go above 10%, real interest rates. There's at least a serious chance of that before the decade is out. Maybe there's some cheap insurance you can buy on that.
Dwarkesh Patel
Very silly question. In these worlds, are financial markets where you make these kinds of bets going to be respected? Is my Fidelity account going to mean anything when we have 50% economic growth? Who’s like, “we have to respect his property rights”?

**Extracted Belief:**

Financial markets will continue to respect property rights even in a world with significant economic growth.

**Context:**

Leopold Aschenbrenner responds to the hypothetical scenario of 50% economic growth, suggesting that property rights will be respected, despite the potential disruptions that rapid growth may bring.

**Justification:**

While he does not provide specific arguments or evidence, Leopold Aschenbrenner expresses his belief with moderate confidence, suggesting that he believes property rights will be maintained even in a rapidly changing economic landscape. This belief may be based on his broader understanding of the social and political systems that underpin property rights.

--------

## Chunk 674

**Chunk:**

Dwarkesh Patel
Very silly question. In these worlds, are financial markets where you make these kinds of bets going to be respected? Is my Fidelity account going to mean anything when we have 50% economic growth? Who’s like, “we have to respect his property rights”?
Leopold Aschenbrenner
That’s pretty deep into it, the bond short, the 50% growth. That's pretty deep into it. Again, there's this whole sequence of things. I think property rights will be respected. At some point, there's going to be figuring out the property rights for the galaxies. That'll be interesting.
Dwarkesh Patel
That will be interesting. Going back to your strategy about how important the 2030s will be for how the rest of the future goes, you want to be in a position of influence by that point because of capital.

As far as I know, there's probably a whole bunch of literature on this, I'm just riffing. The landed gentry before the beginning of the Industrial Revolution, I'm not sure if they were able to leverage their position in a sort of Georgist or Piketty-type sense, in order to accrue the returns that were realized through the Industrial Revolution. I don't know what happened. At some point, they just weren't the landed gentry.

I'd be concerned that even if you make great investment calls, you'll be like the guy who owned a lot of farmland before the Industrial Revolution. The guy who's actually going to make a bunch of money is the one with the steam engine. Even he doesn't make that much money because most of the benefits are widely diffused and so forth.

**Extracted Belief:**

Property rights will be respected even in a future with significant economic growth and technological advancements, such as 50% economic growth.

**Context:**

Dwarkesh Patel asks if property rights will still be respected in a future with rapid economic growth, and Leopold Aschenbrenner affirms this belief.

**Justification:**

Aschenbrenner doesn't provide specific evidence but expresses his opinion as a statement of belief.

--------

## Chunk 675

**Chunk:**

Dwarkesh Patel
Very silly question. In these worlds, are financial markets where you make these kinds of bets going to be respected? Is my Fidelity account going to mean anything when we have 50% economic growth? Who’s like, “we have to respect his property rights”?
Leopold Aschenbrenner
That’s pretty deep into it, the bond short, the 50% growth. That's pretty deep into it. Again, there's this whole sequence of things. I think property rights will be respected. At some point, there's going to be figuring out the property rights for the galaxies. That'll be interesting.
Dwarkesh Patel
That will be interesting. Going back to your strategy about how important the 2030s will be for how the rest of the future goes, you want to be in a position of influence by that point because of capital.

As far as I know, there's probably a whole bunch of literature on this, I'm just riffing. The landed gentry before the beginning of the Industrial Revolution, I'm not sure if they were able to leverage their position in a sort of Georgist or Piketty-type sense, in order to accrue the returns that were realized through the Industrial Revolution. I don't know what happened. At some point, they just weren't the landed gentry.

I'd be concerned that even if you make great investment calls, you'll be like the guy who owned a lot of farmland before the Industrial Revolution. The guy who's actually going to make a bunch of money is the one with the steam engine. Even he doesn't make that much money because most of the benefits are widely diffused and so forth.

**Extracted Belief:**

There will be a need to define and establish property rights for the galaxies in the future.

**Context:**

While discussing future economic and technological advancements, Aschenbrenner suggests the need for defining property rights in space.

**Justification:**

This belief is based on the expectation that space exploration and colonization will lead to the need for defining ownership and jurisdiction.

--------

## Chunk 676

**Chunk:**

Dwarkesh Patel
That will be interesting. Going back to your strategy about how important the 2030s will be for how the rest of the future goes, you want to be in a position of influence by that point because of capital.

As far as I know, there's probably a whole bunch of literature on this, I'm just riffing. The landed gentry before the beginning of the Industrial Revolution, I'm not sure if they were able to leverage their position in a sort of Georgist or Piketty-type sense, in order to accrue the returns that were realized through the Industrial Revolution. I don't know what happened. At some point, they just weren't the landed gentry.

I'd be concerned that even if you make great investment calls, you'll be like the guy who owned a lot of farmland before the Industrial Revolution. The guy who's actually going to make a bunch of money is the one with the steam engine. Even he doesn't make that much money because most of the benefits are widely diffused and so forth.
Leopold Aschenbrenner
The analog is you sell your land and you put it all in the people who are building the new industries. The real depreciating asset for me is human capital. I was valedictorian of Columbia. The thing that made you special is you're smart. In four years, it might not matter because it's automatable.

A friend joked that the investment firm is perfectly hedged for me. Either AGI happens this decade and my human capital depreciates, but I turn it into financial capital, or no AGI happens and the firm doesn’t do well, but I’m still in my twenties and smart.
Dwarkesh Patel
Excellent. What’s your story for why AGI hasn’t been priced in? Financial markets are supposed to be very efficient, so it’s hard to get an edge. Naively, you might say, “I’ve looked at these scaling curves, and they imply we’ll be buying much more compute and energy than analysts realize.” Shouldn’t those analysts be broke by now? What’s going on?

**Extracted Belief:**

Human capital, particularly intelligence, can depreciate rapidly due to automation.

**Context:**

Leopold Aschenbrenner is discussing the potential obsolescence of human intelligence due to technological advancements, particularly in the field of Artificial General Intelligence (AGI).

**Justification:**

Aschenbrenner states that "the real depreciating asset for me is human capital. The thing that made you special is you're smart. In four years, it might not matter because it's automatable."

--------

## Chunk 677

**Chunk:**

Dwarkesh Patel
That will be interesting. Going back to your strategy about how important the 2030s will be for how the rest of the future goes, you want to be in a position of influence by that point because of capital.

As far as I know, there's probably a whole bunch of literature on this, I'm just riffing. The landed gentry before the beginning of the Industrial Revolution, I'm not sure if they were able to leverage their position in a sort of Georgist or Piketty-type sense, in order to accrue the returns that were realized through the Industrial Revolution. I don't know what happened. At some point, they just weren't the landed gentry.

I'd be concerned that even if you make great investment calls, you'll be like the guy who owned a lot of farmland before the Industrial Revolution. The guy who's actually going to make a bunch of money is the one with the steam engine. Even he doesn't make that much money because most of the benefits are widely diffused and so forth.
Leopold Aschenbrenner
The analog is you sell your land and you put it all in the people who are building the new industries. The real depreciating asset for me is human capital. I was valedictorian of Columbia. The thing that made you special is you're smart. In four years, it might not matter because it's automatable.

A friend joked that the investment firm is perfectly hedged for me. Either AGI happens this decade and my human capital depreciates, but I turn it into financial capital, or no AGI happens and the firm doesn’t do well, but I’m still in my twenties and smart.
Dwarkesh Patel
Excellent. What’s your story for why AGI hasn’t been priced in? Financial markets are supposed to be very efficient, so it’s hard to get an edge. Naively, you might say, “I’ve looked at these scaling curves, and they imply we’ll be buying much more compute and energy than analysts realize.” Shouldn’t those analysts be broke by now? What’s going on?

**Extracted Belief:**

Financial markets are not always efficient in pricing future events.

**Context:**

Aschenbrenner is responding to Dwarkesh Patel's question about why AGI has not been priced into financial markets.

**Justification:**

He states that "I used to believe in the EMH guy as an economist. But now, I think there are groups of smart people, like those in San Francisco, who have alpha over the rest of society in seeing the future."

--------

## Chunk 678

**Chunk:**

Dwarkesh Patel
That will be interesting. Going back to your strategy about how important the 2030s will be for how the rest of the future goes, you want to be in a position of influence by that point because of capital.

As far as I know, there's probably a whole bunch of literature on this, I'm just riffing. The landed gentry before the beginning of the Industrial Revolution, I'm not sure if they were able to leverage their position in a sort of Georgist or Piketty-type sense, in order to accrue the returns that were realized through the Industrial Revolution. I don't know what happened. At some point, they just weren't the landed gentry.

I'd be concerned that even if you make great investment calls, you'll be like the guy who owned a lot of farmland before the Industrial Revolution. The guy who's actually going to make a bunch of money is the one with the steam engine. Even he doesn't make that much money because most of the benefits are widely diffused and so forth.
Leopold Aschenbrenner
The analog is you sell your land and you put it all in the people who are building the new industries. The real depreciating asset for me is human capital. I was valedictorian of Columbia. The thing that made you special is you're smart. In four years, it might not matter because it's automatable.

A friend joked that the investment firm is perfectly hedged for me. Either AGI happens this decade and my human capital depreciates, but I turn it into financial capital, or no AGI happens and the firm doesn’t do well, but I’m still in my twenties and smart.
Dwarkesh Patel
Excellent. What’s your story for why AGI hasn’t been priced in? Financial markets are supposed to be very efficient, so it’s hard to get an edge. Naively, you might say, “I’ve looked at these scaling curves, and they imply we’ll be buying much more compute and energy than analysts realize.” Shouldn’t those analysts be broke by now? What’s going on?

**Extracted Belief:**

Certain groups of people have a better understanding of future trends than the general population.

**Context:**

Aschenbrenner is explaining why he believes AGI has not been fully priced into financial markets.

**Justification:**

He draws a parallel to the COVID-19 pandemic, stating that "A similar group of people saw it coming and called it completely corrected. They shorted the market and did really well."

--------

## Chunk 679

**Chunk:**

Dwarkesh Patel
Excellent. What’s your story for why AGI hasn’t been priced in? Financial markets are supposed to be very efficient, so it’s hard to get an edge. Naively, you might say, “I’ve looked at these scaling curves, and they imply we’ll be buying much more compute and energy than analysts realize.” Shouldn’t those analysts be broke by now? What’s going on?
Leopold Aschenbrenner
I used to believe in the EMH guy as an economist. But now, I think there are groups of smart people, like those in San Francisco, who have alpha over the rest of society in seeing the future.

It’s like with COVID. A similar group of people saw it coming and called it completely corrected. They shorted the market and did really well. Why isn’t AGI priced in? It’s like asking why the government hasn’t nationalized the labs yet. Society hasn’t priced it in yet. It hasn’t completely diffused. I might be wrong but not many people take these ideas seriously.

(04:19:14) – Lessons from WWII
Dwarkesh Patel
There are a couple of other ideas I was playing around with that we haven’t gotten to talk about yet. One’s systems competition. One of my favorite books about World War II is Victor Davis Hanson’s summary of everything. He explains why the Allies made better decisions than the Axis.

**Extracted Belief:**

There are groups of smart people, like those in San Francisco, who have alpha over the rest of society in seeing the future.

**Context:**

Leopold Aschenbrenner is explaining why Artificial General Intelligence (AGI) has not been priced into the financial markets, referencing the Efficient Market Hypothesis (EMH) in economics.

**Justification:**

Leopold Aschenbrenner is drawing on his own experience and observations, suggesting that certain groups of people, particularly those in Silicon Valley, possess foresight and understanding that surpasses the average population.

--------

## Chunk 680

**Chunk:**

Dwarkesh Patel
Excellent. What’s your story for why AGI hasn’t been priced in? Financial markets are supposed to be very efficient, so it’s hard to get an edge. Naively, you might say, “I’ve looked at these scaling curves, and they imply we’ll be buying much more compute and energy than analysts realize.” Shouldn’t those analysts be broke by now? What’s going on?
Leopold Aschenbrenner
I used to believe in the EMH guy as an economist. But now, I think there are groups of smart people, like those in San Francisco, who have alpha over the rest of society in seeing the future.

It’s like with COVID. A similar group of people saw it coming and called it completely corrected. They shorted the market and did really well. Why isn’t AGI priced in? It’s like asking why the government hasn’t nationalized the labs yet. Society hasn’t priced it in yet. It hasn’t completely diffused. I might be wrong but not many people take these ideas seriously.

(04:19:14) – Lessons from WWII
Dwarkesh Patel
There are a couple of other ideas I was playing around with that we haven’t gotten to talk about yet. One’s systems competition. One of my favorite books about World War II is Victor Davis Hanson’s summary of everything. He explains why the Allies made better decisions than the Axis.

**Extracted Belief:**

Society has not yet priced in the potential impact of AGI.

**Context:**

Leopold Aschenbrenner is responding to the question of why AGI has not been priced into the financial markets, comparing it to the delayed response to the COVID-19 pandemic.

**Justification:**

He compares the situation to the delay in recognizing the significance of COVID-19, suggesting that society often underestimates the impact of disruptive events until they become widespread and evident.

--------

## Chunk 681

**Chunk:**

Dwarkesh Patel
There are a couple of other ideas I was playing around with that we haven’t gotten to talk about yet. One’s systems competition. One of my favorite books about World War II is Victor Davis Hanson’s summary of everything. He explains why the Allies made better decisions than the Axis.
Leopold Aschenbrenner
Why did they?
Dwarkesh Patel
There were decisions the Axis made that were pretty good, like blitzkrieg.

**Extracted Belief:**

Blitzkrieg, the German military strategy of rapid attacks, was not an ingenious strategy but rather a forced tactic.

**Context:**

Leopold Aschenbrenner responded to Dwarkesh Patel's statement that the Axis made good decisions, specifically mentioning blitzkrieg. Aschenbrenner expressed his belief about the origin of blitzkrieg in response.

**Justification:**

Aschenbrenner states that blitzkrieg was not an ingenious strategy, implying that it was not a product of strategic brilliance but rather a necessary response to the circumstances facing Germany. He suggests this belief is based on his reading of the history of World War II, possibly referencing the work of Adam Tooze, who explores the economic and strategic constraints on Germany during the war.

--------

## Chunk 682

**Chunk:**

Dwarkesh Patel
There were decisions the Axis made that were pretty good, like blitzkrieg.
Leopold Aschenbrenner
That was sort of by accident though.
Dwarkesh Patel
In what sense? That they just had the infrastructure left over?

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 683

**Chunk:**

Dwarkesh Patel
In what sense? That they just had the infrastructure left over?
Leopold Aschenbrenner
My read of it is that blitzkrieg wasn’t an ingenious strategy. Their hand was forced. This is the very Adam Tooze-ian story of World War II. There’s the concept of a long war versus a short war, which is important. Germany realized that if they were in a long war, including the United States, they would not be able to compete industrially. Their only path to victory was to make it a short war. That worked much more spectacularly than they thought, allowing them to take over France and much of Europe.

The decision to invade the Soviet Union was related to the western front because they needed resources like oil. Auschwitz was actually a giant chemical plant to produce synthetic oil and other materials. It was the largest industrial project in Nazi Germany. They thought, “we crushed them in World War I, it’ll be easy. We’ll invade, get the resources, and then fight on the western front.” Even during the invasion of the Soviet Union, even though a large number of the deaths happened there, a large fraction of German industrial production—planes, naval forces, and so on—was directed towards the western front and the western allies.

By the way, this concept of a long war versus a short war is interesting, especially when thinking about the China competition. I worry about the decline of latent American industrial capacity. China builds like 200 times more ships than we do right now.

Maybe we have superiority in the non-AI world in military materiel and can win a short war or defend Taiwan. If it drags on, China might be better able to mobilize industrial resources in a way we can’t anymore. This is also relevant to AI. If building AGI requires a trillion-dollar cluster instead of a $100 billion cluster, or even if it’s on the $100 billion cluster, it really matters if you can do an order of magnitude more compute for your superintelligence. Maybe right now they’re behind, but they have the raw latent industrial capacity to outbuild us.

That matters both in the run-up to AGI and afterward. You have the superintelligence on your cluster, and then it’s time to expand the explosive growth. Will we let the robo-factories run wild? Maybe not, but maybe China will. How many drones will we produce? There’s an industrial explosion that I worry about.
Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.

**Extracted Belief:**

Blitzkrieg was not an ingenious strategy, but rather a desperate measure forced upon Germany due to their limited industrial capacity.

**Context:**

Leopold Aschenbrenner was explaining his perspective on the effectiveness of Blitzkrieg in World War II, contradicting Dwarkesh Patel's suggestion that it was a well-planned and effective strategy.

**Justification:**

Aschenbrenner argues that Germany's limited industrial capacity in a potential long war against the United States forced them to pursue a short war strategy, which Blitzkrieg represented. He references Adam Tooze's work on World War II as supporting this view, implying a consensus among historians.

--------

## Chunk 684

**Chunk:**

Dwarkesh Patel
In what sense? That they just had the infrastructure left over?
Leopold Aschenbrenner
My read of it is that blitzkrieg wasn’t an ingenious strategy. Their hand was forced. This is the very Adam Tooze-ian story of World War II. There’s the concept of a long war versus a short war, which is important. Germany realized that if they were in a long war, including the United States, they would not be able to compete industrially. Their only path to victory was to make it a short war. That worked much more spectacularly than they thought, allowing them to take over France and much of Europe.

The decision to invade the Soviet Union was related to the western front because they needed resources like oil. Auschwitz was actually a giant chemical plant to produce synthetic oil and other materials. It was the largest industrial project in Nazi Germany. They thought, “we crushed them in World War I, it’ll be easy. We’ll invade, get the resources, and then fight on the western front.” Even during the invasion of the Soviet Union, even though a large number of the deaths happened there, a large fraction of German industrial production—planes, naval forces, and so on—was directed towards the western front and the western allies.

By the way, this concept of a long war versus a short war is interesting, especially when thinking about the China competition. I worry about the decline of latent American industrial capacity. China builds like 200 times more ships than we do right now.

Maybe we have superiority in the non-AI world in military materiel and can win a short war or defend Taiwan. If it drags on, China might be better able to mobilize industrial resources in a way we can’t anymore. This is also relevant to AI. If building AGI requires a trillion-dollar cluster instead of a $100 billion cluster, or even if it’s on the $100 billion cluster, it really matters if you can do an order of magnitude more compute for your superintelligence. Maybe right now they’re behind, but they have the raw latent industrial capacity to outbuild us.

That matters both in the run-up to AGI and afterward. You have the superintelligence on your cluster, and then it’s time to expand the explosive growth. Will we let the robo-factories run wild? Maybe not, but maybe China will. How many drones will we produce? There’s an industrial explosion that I worry about.
Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.

**Extracted Belief:**

Germany's ability to win World War II was contingent on a short war, as they could not sustain a long war against the United States due to industrial limitations.

**Context:**

Aschenbrenner discusses Germany's war strategy, highlighting the importance of industrial capacity in prolonged conflicts.

**Justification:**

He states that Germany's industrial capacity was insufficient to compete with the United States in a long war, making a short war their only path to victory. This suggests an understanding of Germany's economic and industrial limitations during the war.

--------

## Chunk 685

**Chunk:**

Dwarkesh Patel
In what sense? That they just had the infrastructure left over?
Leopold Aschenbrenner
My read of it is that blitzkrieg wasn’t an ingenious strategy. Their hand was forced. This is the very Adam Tooze-ian story of World War II. There’s the concept of a long war versus a short war, which is important. Germany realized that if they were in a long war, including the United States, they would not be able to compete industrially. Their only path to victory was to make it a short war. That worked much more spectacularly than they thought, allowing them to take over France and much of Europe.

The decision to invade the Soviet Union was related to the western front because they needed resources like oil. Auschwitz was actually a giant chemical plant to produce synthetic oil and other materials. It was the largest industrial project in Nazi Germany. They thought, “we crushed them in World War I, it’ll be easy. We’ll invade, get the resources, and then fight on the western front.” Even during the invasion of the Soviet Union, even though a large number of the deaths happened there, a large fraction of German industrial production—planes, naval forces, and so on—was directed towards the western front and the western allies.

By the way, this concept of a long war versus a short war is interesting, especially when thinking about the China competition. I worry about the decline of latent American industrial capacity. China builds like 200 times more ships than we do right now.

Maybe we have superiority in the non-AI world in military materiel and can win a short war or defend Taiwan. If it drags on, China might be better able to mobilize industrial resources in a way we can’t anymore. This is also relevant to AI. If building AGI requires a trillion-dollar cluster instead of a $100 billion cluster, or even if it’s on the $100 billion cluster, it really matters if you can do an order of magnitude more compute for your superintelligence. Maybe right now they’re behind, but they have the raw latent industrial capacity to outbuild us.

That matters both in the run-up to AGI and afterward. You have the superintelligence on your cluster, and then it’s time to expand the explosive growth. Will we let the robo-factories run wild? Maybe not, but maybe China will. How many drones will we produce? There’s an industrial explosion that I worry about.
Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.

**Extracted Belief:**

Germany's invasion of the Soviet Union was driven by a need for resources like oil and stemmed from an underestimation of Soviet resistance.

**Context:**

Aschenbrenner provides context for Germany's decision to invade the Soviet Union, connecting it to resource acquisition and prior experience with the Soviets.

**Justification:**

He states that Germany needed resources like oil, which they sought from the Soviet Union, and that their decision was influenced by their previous victory over Russia in World War I. This indicates an understanding of the geopolitical and resource-driven factors behind the invasion.

--------

## Chunk 686

**Chunk:**

Dwarkesh Patel
In what sense? That they just had the infrastructure left over?
Leopold Aschenbrenner
My read of it is that blitzkrieg wasn’t an ingenious strategy. Their hand was forced. This is the very Adam Tooze-ian story of World War II. There’s the concept of a long war versus a short war, which is important. Germany realized that if they were in a long war, including the United States, they would not be able to compete industrially. Their only path to victory was to make it a short war. That worked much more spectacularly than they thought, allowing them to take over France and much of Europe.

The decision to invade the Soviet Union was related to the western front because they needed resources like oil. Auschwitz was actually a giant chemical plant to produce synthetic oil and other materials. It was the largest industrial project in Nazi Germany. They thought, “we crushed them in World War I, it’ll be easy. We’ll invade, get the resources, and then fight on the western front.” Even during the invasion of the Soviet Union, even though a large number of the deaths happened there, a large fraction of German industrial production—planes, naval forces, and so on—was directed towards the western front and the western allies.

By the way, this concept of a long war versus a short war is interesting, especially when thinking about the China competition. I worry about the decline of latent American industrial capacity. China builds like 200 times more ships than we do right now.

Maybe we have superiority in the non-AI world in military materiel and can win a short war or defend Taiwan. If it drags on, China might be better able to mobilize industrial resources in a way we can’t anymore. This is also relevant to AI. If building AGI requires a trillion-dollar cluster instead of a $100 billion cluster, or even if it’s on the $100 billion cluster, it really matters if you can do an order of magnitude more compute for your superintelligence. Maybe right now they’re behind, but they have the raw latent industrial capacity to outbuild us.

That matters both in the run-up to AGI and afterward. You have the superintelligence on your cluster, and then it’s time to expand the explosive growth. Will we let the robo-factories run wild? Maybe not, but maybe China will. How many drones will we produce? There’s an industrial explosion that I worry about.
Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.

**Extracted Belief:**

Auschwitz was primarily an industrial facility for producing synthetic oil and other materials, rather than solely a site of genocide.

**Context:**

Aschenbrenner provides a specific example of Germany's industrial efforts during the war, emphasizing the economic motivations behind the construction of Auschwitz.

**Justification:**

He describes Auschwitz as the largest industrial project in Nazi Germany, focusing on its role in producing synthetic oil and materials. This contradicts the common understanding of Auschwitz solely as a concentration camp, suggesting that its industrial function was significant.

--------

## Chunk 687

**Chunk:**

Dwarkesh Patel
In what sense? That they just had the infrastructure left over?
Leopold Aschenbrenner
My read of it is that blitzkrieg wasn’t an ingenious strategy. Their hand was forced. This is the very Adam Tooze-ian story of World War II. There’s the concept of a long war versus a short war, which is important. Germany realized that if they were in a long war, including the United States, they would not be able to compete industrially. Their only path to victory was to make it a short war. That worked much more spectacularly than they thought, allowing them to take over France and much of Europe.

The decision to invade the Soviet Union was related to the western front because they needed resources like oil. Auschwitz was actually a giant chemical plant to produce synthetic oil and other materials. It was the largest industrial project in Nazi Germany. They thought, “we crushed them in World War I, it’ll be easy. We’ll invade, get the resources, and then fight on the western front.” Even during the invasion of the Soviet Union, even though a large number of the deaths happened there, a large fraction of German industrial production—planes, naval forces, and so on—was directed towards the western front and the western allies.

By the way, this concept of a long war versus a short war is interesting, especially when thinking about the China competition. I worry about the decline of latent American industrial capacity. China builds like 200 times more ships than we do right now.

Maybe we have superiority in the non-AI world in military materiel and can win a short war or defend Taiwan. If it drags on, China might be better able to mobilize industrial resources in a way we can’t anymore. This is also relevant to AI. If building AGI requires a trillion-dollar cluster instead of a $100 billion cluster, or even if it’s on the $100 billion cluster, it really matters if you can do an order of magnitude more compute for your superintelligence. Maybe right now they’re behind, but they have the raw latent industrial capacity to outbuild us.

That matters both in the run-up to AGI and afterward. You have the superintelligence on your cluster, and then it’s time to expand the explosive growth. Will we let the robo-factories run wild? Maybe not, but maybe China will. How many drones will we produce? There’s an industrial explosion that I worry about.
Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.

**Extracted Belief:**

China has a significantly greater shipbuilding capacity than the United States, potentially giving them an advantage in a long war.

**Context:**

Aschenbrenner introduces the concept of industrial capacity in the context of the potential conflict between the United States and China, highlighting the importance of shipbuilding in modern warfare.

**Justification:**

He states that China builds 200 times more ships than the United States, indicating a substantial difference in shipbuilding capacity between the two nations. This suggests a belief in China's potential advantage in a long war due to their superior industrial capacity.

--------

## Chunk 688

**Chunk:**

Dwarkesh Patel
In what sense? That they just had the infrastructure left over?
Leopold Aschenbrenner
My read of it is that blitzkrieg wasn’t an ingenious strategy. Their hand was forced. This is the very Adam Tooze-ian story of World War II. There’s the concept of a long war versus a short war, which is important. Germany realized that if they were in a long war, including the United States, they would not be able to compete industrially. Their only path to victory was to make it a short war. That worked much more spectacularly than they thought, allowing them to take over France and much of Europe.

The decision to invade the Soviet Union was related to the western front because they needed resources like oil. Auschwitz was actually a giant chemical plant to produce synthetic oil and other materials. It was the largest industrial project in Nazi Germany. They thought, “we crushed them in World War I, it’ll be easy. We’ll invade, get the resources, and then fight on the western front.” Even during the invasion of the Soviet Union, even though a large number of the deaths happened there, a large fraction of German industrial production—planes, naval forces, and so on—was directed towards the western front and the western allies.

By the way, this concept of a long war versus a short war is interesting, especially when thinking about the China competition. I worry about the decline of latent American industrial capacity. China builds like 200 times more ships than we do right now.

Maybe we have superiority in the non-AI world in military materiel and can win a short war or defend Taiwan. If it drags on, China might be better able to mobilize industrial resources in a way we can’t anymore. This is also relevant to AI. If building AGI requires a trillion-dollar cluster instead of a $100 billion cluster, or even if it’s on the $100 billion cluster, it really matters if you can do an order of magnitude more compute for your superintelligence. Maybe right now they’re behind, but they have the raw latent industrial capacity to outbuild us.

That matters both in the run-up to AGI and afterward. You have the superintelligence on your cluster, and then it’s time to expand the explosive growth. Will we let the robo-factories run wild? Maybe not, but maybe China will. How many drones will we produce? There’s an industrial explosion that I worry about.
Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.

**Extracted Belief:**

The United States might be able to win a short war or defend Taiwan due to its superior military technology, but may not be able to sustain a prolonged conflict with China due to its lagging industrial capacity.

**Context:**

Aschenbrenner discusses the potential advantages and disadvantages of the United States in a potential conflict with China, highlighting the importance of both military technology and industrial capacity.

**Justification:**

He suggests that the United States may have an edge in military technology, potentially allowing them to win a short war, but warns that China's superior industrial capacity might allow them to outlast the United States in a prolonged conflict. This suggests a belief in the importance of both technological and industrial factors in determining the outcome of a conflict.

--------

## Chunk 689

**Chunk:**

Dwarkesh Patel
In what sense? That they just had the infrastructure left over?
Leopold Aschenbrenner
My read of it is that blitzkrieg wasn’t an ingenious strategy. Their hand was forced. This is the very Adam Tooze-ian story of World War II. There’s the concept of a long war versus a short war, which is important. Germany realized that if they were in a long war, including the United States, they would not be able to compete industrially. Their only path to victory was to make it a short war. That worked much more spectacularly than they thought, allowing them to take over France and much of Europe.

The decision to invade the Soviet Union was related to the western front because they needed resources like oil. Auschwitz was actually a giant chemical plant to produce synthetic oil and other materials. It was the largest industrial project in Nazi Germany. They thought, “we crushed them in World War I, it’ll be easy. We’ll invade, get the resources, and then fight on the western front.” Even during the invasion of the Soviet Union, even though a large number of the deaths happened there, a large fraction of German industrial production—planes, naval forces, and so on—was directed towards the western front and the western allies.

By the way, this concept of a long war versus a short war is interesting, especially when thinking about the China competition. I worry about the decline of latent American industrial capacity. China builds like 200 times more ships than we do right now.

Maybe we have superiority in the non-AI world in military materiel and can win a short war or defend Taiwan. If it drags on, China might be better able to mobilize industrial resources in a way we can’t anymore. This is also relevant to AI. If building AGI requires a trillion-dollar cluster instead of a $100 billion cluster, or even if it’s on the $100 billion cluster, it really matters if you can do an order of magnitude more compute for your superintelligence. Maybe right now they’re behind, but they have the raw latent industrial capacity to outbuild us.

That matters both in the run-up to AGI and afterward. You have the superintelligence on your cluster, and then it’s time to expand the explosive growth. Will we let the robo-factories run wild? Maybe not, but maybe China will. How many drones will we produce? There’s an industrial explosion that I worry about.
Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.

**Extracted Belief:**

China's industrial capacity gives them the potential to outbuild the United States in the development of artificial general intelligence (AGI) by providing more computing power.

**Context:**

Aschenbrenner extends the discussion of industrial capacity to the domain of AI, highlighting the importance of computational resources in developing AGI.

**Justification:**

He argues that China's potential to outbuild the United States in computational power could give them an edge in developing AGI, suggesting that industrial capacity translates into computational resources, which are crucial for AI development.

--------

## Chunk 690

**Chunk:**

Dwarkesh Patel
In what sense? That they just had the infrastructure left over?
Leopold Aschenbrenner
My read of it is that blitzkrieg wasn’t an ingenious strategy. Their hand was forced. This is the very Adam Tooze-ian story of World War II. There’s the concept of a long war versus a short war, which is important. Germany realized that if they were in a long war, including the United States, they would not be able to compete industrially. Their only path to victory was to make it a short war. That worked much more spectacularly than they thought, allowing them to take over France and much of Europe.

The decision to invade the Soviet Union was related to the western front because they needed resources like oil. Auschwitz was actually a giant chemical plant to produce synthetic oil and other materials. It was the largest industrial project in Nazi Germany. They thought, “we crushed them in World War I, it’ll be easy. We’ll invade, get the resources, and then fight on the western front.” Even during the invasion of the Soviet Union, even though a large number of the deaths happened there, a large fraction of German industrial production—planes, naval forces, and so on—was directed towards the western front and the western allies.

By the way, this concept of a long war versus a short war is interesting, especially when thinking about the China competition. I worry about the decline of latent American industrial capacity. China builds like 200 times more ships than we do right now.

Maybe we have superiority in the non-AI world in military materiel and can win a short war or defend Taiwan. If it drags on, China might be better able to mobilize industrial resources in a way we can’t anymore. This is also relevant to AI. If building AGI requires a trillion-dollar cluster instead of a $100 billion cluster, or even if it’s on the $100 billion cluster, it really matters if you can do an order of magnitude more compute for your superintelligence. Maybe right now they’re behind, but they have the raw latent industrial capacity to outbuild us.

That matters both in the run-up to AGI and afterward. You have the superintelligence on your cluster, and then it’s time to expand the explosive growth. Will we let the robo-factories run wild? Maybe not, but maybe China will. How many drones will we produce? There’s an industrial explosion that I worry about.
Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.

**Extracted Belief:**

The development and deployment of artificial superintelligence (ASI) will require significant computational resources, creating a potential for industrial expansion and competition between nations.

**Context:**

Aschenbrenner explores the potential consequences of developing ASI, highlighting the need for massive computational resources and the possibility of industrial competition between nations.

**Justification:**

He states that ASI will require large-scale computing power, which could lead to an industrial explosion in the production of computational resources. This suggests a belief in the significant industrial implications of developing ASI.

--------

## Chunk 691

**Chunk:**

Dwarkesh Patel
In what sense? That they just had the infrastructure left over?
Leopold Aschenbrenner
My read of it is that blitzkrieg wasn’t an ingenious strategy. Their hand was forced. This is the very Adam Tooze-ian story of World War II. There’s the concept of a long war versus a short war, which is important. Germany realized that if they were in a long war, including the United States, they would not be able to compete industrially. Their only path to victory was to make it a short war. That worked much more spectacularly than they thought, allowing them to take over France and much of Europe.

The decision to invade the Soviet Union was related to the western front because they needed resources like oil. Auschwitz was actually a giant chemical plant to produce synthetic oil and other materials. It was the largest industrial project in Nazi Germany. They thought, “we crushed them in World War I, it’ll be easy. We’ll invade, get the resources, and then fight on the western front.” Even during the invasion of the Soviet Union, even though a large number of the deaths happened there, a large fraction of German industrial production—planes, naval forces, and so on—was directed towards the western front and the western allies.

By the way, this concept of a long war versus a short war is interesting, especially when thinking about the China competition. I worry about the decline of latent American industrial capacity. China builds like 200 times more ships than we do right now.

Maybe we have superiority in the non-AI world in military materiel and can win a short war or defend Taiwan. If it drags on, China might be better able to mobilize industrial resources in a way we can’t anymore. This is also relevant to AI. If building AGI requires a trillion-dollar cluster instead of a $100 billion cluster, or even if it’s on the $100 billion cluster, it really matters if you can do an order of magnitude more compute for your superintelligence. Maybe right now they’re behind, but they have the raw latent industrial capacity to outbuild us.

That matters both in the run-up to AGI and afterward. You have the superintelligence on your cluster, and then it’s time to expand the explosive growth. Will we let the robo-factories run wild? Maybe not, but maybe China will. How many drones will we produce? There’s an industrial explosion that I worry about.
Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.

**Extracted Belief:**

The deployment of ASI could lead to an uncontrolled expansion of robotic factories and armies, potentially creating significant risks for global stability.

**Context:**

Aschenbrenner expresses concerns about the potential consequences of ASI, specifically highlighting the risks of uncontrolled robotic production.

**Justification:**

He raises the possibility of uncontrolled robotic factories and armies, suggesting a belief that ASI could lead to a rapid and potentially dangerous expansion of robotic technology. This indicates an awareness of the potential risks associated with the deployment of ASI.

--------

## Chunk 692

**Chunk:**

Dwarkesh Patel
You’ve got to be one of the few people in the world who is both concerned about alignment but also wants to ensure we let the robo-factories proceed once we get ASI to beat out China. That’s very interesting.
Leopold Aschenbrenner
It’s all part of the picture.
Dwarkesh Patel
Speaking of ASIs and the robot factories and robo armies. One of the interesting things is the question of what you do with industrial-scale intelligence. Obviously, it’s not chatbots. It’s very hard to predict.

The history of oil is very interesting. In the 1860s, we figured out how to refine oil. A geologist discovered it, and then Standard Oil got started. There was a huge boom, changing American politics. Legislators were bought out by oil interests. Presidents were elected based on divisions about oil and breaking them up.

All this happened before the car was invented. The light bulb was invented 50 years after oil refining was discovered. Most of Standard Oil’s history is before the car is invented. It was just kerosene lamps just used for lighting.

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 693

**Chunk:**

Dwarkesh Patel
Speaking of ASIs and the robot factories and robo armies. One of the interesting things is the question of what you do with industrial-scale intelligence. Obviously, it’s not chatbots. It’s very hard to predict.

The history of oil is very interesting. In the 1860s, we figured out how to refine oil. A geologist discovered it, and then Standard Oil got started. There was a huge boom, changing American politics. Legislators were bought out by oil interests. Presidents were elected based on divisions about oil and breaking them up.

All this happened before the car was invented. The light bulb was invented 50 years after oil refining was discovered. Most of Standard Oil’s history is before the car is invented. It was just kerosene lamps just used for lighting.
Leopold Aschenbrenner
So they thought oil would just no longer be relevant?
Dwarkesh Patel
Yeah. There was a concern that Standard Oil would go bankrupt when the light bulb was invented. You realize there's an immense amount of compressed energy here. You're going to have billions of gallons of this stuff a year. It’s hard to predict in advance what you can do with that. Later on, it turns out it’s used for transportation and cars.

With intelligence, maybe one answer is the intelligence explosion. But even after that, you have all these ASIs and enough compute, especially the compute they'll build to run—

**Extracted Belief:**

People in the past, including those associated with Standard Oil, underestimated the future applications and relevance of oil despite its vast potential.

**Context:**

Leopold Aschenbrenner was responding to Dwarkesh Patel's statement about the early history of oil and the concern that Standard Oil would go bankrupt after the invention of the light bulb.

**Justification:**

Aschenbrenner referenced the historical belief that oil would become obsolete after the light bulb was invented. This belief was based on the limited known applications of oil at the time.

--------

## Chunk 694

**Chunk:**

Dwarkesh Patel
Yeah. There was a concern that Standard Oil would go bankrupt when the light bulb was invented. You realize there's an immense amount of compressed energy here. You're going to have billions of gallons of this stuff a year. It’s hard to predict in advance what you can do with that. Later on, it turns out it’s used for transportation and cars.

With intelligence, maybe one answer is the intelligence explosion. But even after that, you have all these ASIs and enough compute, especially the compute they'll build to run—
Leopold Aschenbrenner
Hundreds of millions of GPUs will hum.
Dwarkesh Patel
What are we doing with that? It’s very hard to predict in advance. It’ll be very interesting to figure out what the Jupiter brains will be doing. 

So there’s situational awareness of where things stand now, and we’ve gotten a good dose of that. A lot of what we’re talking about now couldn’t have been predicted many years back. Part of your worldview implies that things will accelerate because of AI.

Many unpredictable factors will become evident over time, like how people, the political system, and foreign adversaries will react. Situational awareness isn’t just knowing where things stand now, but being in a position to react appropriately to new information and to change your worldview and recommendations accordingly.

What is the appropriate way to think about situational awareness as a continuous process rather than as a one-time realization?

**Extracted Belief:**

The development of Artificial Superintelligence (ASI) will lead to an intelligence explosion, resulting in a significant increase in computational power and the creation of large numbers of ASIs.

**Context:**

Leopold Aschenbrenner is responding to the host's question about the potential applications of industrial-scale intelligence and the difficulty of predicting its future uses.

**Justification:**

The belief is based on the host's statement that 'With intelligence, maybe one answer is the intelligence explosion' and the subsequent discussion about the large number of ASIs and the compute they would require.

--------

## Chunk 695

**Chunk:**

Dwarkesh Patel
Yeah. There was a concern that Standard Oil would go bankrupt when the light bulb was invented. You realize there's an immense amount of compressed energy here. You're going to have billions of gallons of this stuff a year. It’s hard to predict in advance what you can do with that. Later on, it turns out it’s used for transportation and cars.

With intelligence, maybe one answer is the intelligence explosion. But even after that, you have all these ASIs and enough compute, especially the compute they'll build to run—
Leopold Aschenbrenner
Hundreds of millions of GPUs will hum.
Dwarkesh Patel
What are we doing with that? It’s very hard to predict in advance. It’ll be very interesting to figure out what the Jupiter brains will be doing. 

So there’s situational awareness of where things stand now, and we’ve gotten a good dose of that. A lot of what we’re talking about now couldn’t have been predicted many years back. Part of your worldview implies that things will accelerate because of AI.

Many unpredictable factors will become evident over time, like how people, the political system, and foreign adversaries will react. Situational awareness isn’t just knowing where things stand now, but being in a position to react appropriately to new information and to change your worldview and recommendations accordingly.

What is the appropriate way to think about situational awareness as a continuous process rather than as a one-time realization?

**Extracted Belief:**

The development and deployment of ASIs will require significant computational resources, involving hundreds of millions of GPUs.

**Context:**

Leopold Aschenbrenner is describing the scale of computational power that will be needed to run ASIs.

**Justification:**

The belief is based on the statement 'Hundreds of millions of GPUs will hum' which refers to the expected hardware requirements for running ASIs.

--------

## Chunk 696

**Chunk:**

Dwarkesh Patel
What are we doing with that? It’s very hard to predict in advance. It’ll be very interesting to figure out what the Jupiter brains will be doing. 

So there’s situational awareness of where things stand now, and we’ve gotten a good dose of that. A lot of what we’re talking about now couldn’t have been predicted many years back. Part of your worldview implies that things will accelerate because of AI.

Many unpredictable factors will become evident over time, like how people, the political system, and foreign adversaries will react. Situational awareness isn’t just knowing where things stand now, but being in a position to react appropriately to new information and to change your worldview and recommendations accordingly.

What is the appropriate way to think about situational awareness as a continuous process rather than as a one-time realization?
Leopold Aschenbrenner
This is great. There’s a sort of mental flexibility and willingness to change your mind that’s really important. This is how a lot of brains have been broken in the AGI debate. The doomers were prescient about AGI a decade ago, but they haven’t updated on the empirical realities of deep learning. Their proposals are naive and unworkable. It doesn’t really make sense.

Some people come in with a predefined ideology, like e/accs. They like to shitpost about technology but they’re not actually thinking it through. You have stagnationists who think this stuff is just chatbots and not risky or those not considering the immense national security implications.

There’s a risk of calcification of worldview when you publicly articulate a position and cling to it despite evidence against it. So I want to give a big disclaimer. It’s valuable to paint a concrete and visceral picture. This is currently my best guess on how this decade will go. If it goes anything like this, it’ll be wild. Given the rapid pace of progress, we’re going to keep getting a lot more information and it’s important to keep your head on straight.

I feel like the most important thing here is that. This relates to some of the stuff we talked about the world being surprisingly small. I used to think important things were being handled by capable people in government and AI labs.

From personal experience, and seeing how Covid was managed, I realized that not everyone is on it. There’s not somebody else who’s on it and making sure this goes well. What really matters is that good people take these issues as seriously as they deserve, have situational awareness, are willing to change their minds, and face reality head-on. I’m counting on those good people.
Dwarkesh Patel
All right, that’s a great place to close.

**Extracted Belief:**

Mental flexibility and willingness to change one's mind are crucial in any field, particularly when dealing with rapidly evolving technologies like artificial intelligence.

**Context:**

Leopold Aschenbrenner highlights the importance of adaptability in the context of the AGI debate, emphasizing the need to adjust one's views based on new evidence and realities.

**Justification:**

He points to the 'empirical realities of deep learning' as evidence that challenges the predictions of 'doomers' who haven't updated their views on AGI. He also suggests that those who cling to preconceived ideologies or stagnate in their understanding of AI technologies are not engaging in productive discourse.

--------

## Chunk 697

**Chunk:**

Dwarkesh Patel
What are we doing with that? It’s very hard to predict in advance. It’ll be very interesting to figure out what the Jupiter brains will be doing. 

So there’s situational awareness of where things stand now, and we’ve gotten a good dose of that. A lot of what we’re talking about now couldn’t have been predicted many years back. Part of your worldview implies that things will accelerate because of AI.

Many unpredictable factors will become evident over time, like how people, the political system, and foreign adversaries will react. Situational awareness isn’t just knowing where things stand now, but being in a position to react appropriately to new information and to change your worldview and recommendations accordingly.

What is the appropriate way to think about situational awareness as a continuous process rather than as a one-time realization?
Leopold Aschenbrenner
This is great. There’s a sort of mental flexibility and willingness to change your mind that’s really important. This is how a lot of brains have been broken in the AGI debate. The doomers were prescient about AGI a decade ago, but they haven’t updated on the empirical realities of deep learning. Their proposals are naive and unworkable. It doesn’t really make sense.

Some people come in with a predefined ideology, like e/accs. They like to shitpost about technology but they’re not actually thinking it through. You have stagnationists who think this stuff is just chatbots and not risky or those not considering the immense national security implications.

There’s a risk of calcification of worldview when you publicly articulate a position and cling to it despite evidence against it. So I want to give a big disclaimer. It’s valuable to paint a concrete and visceral picture. This is currently my best guess on how this decade will go. If it goes anything like this, it’ll be wild. Given the rapid pace of progress, we’re going to keep getting a lot more information and it’s important to keep your head on straight.

I feel like the most important thing here is that. This relates to some of the stuff we talked about the world being surprisingly small. I used to think important things were being handled by capable people in government and AI labs.

From personal experience, and seeing how Covid was managed, I realized that not everyone is on it. There’s not somebody else who’s on it and making sure this goes well. What really matters is that good people take these issues as seriously as they deserve, have situational awareness, are willing to change their minds, and face reality head-on. I’m counting on those good people.
Dwarkesh Patel
All right, that’s a great place to close.

**Extracted Belief:**

People who fail to adapt their viewpoints based on new evidence and realities, especially in the context of rapidly evolving technologies like AGI, risk calcification of their worldview.

**Context:**

Leopold Aschenbrenner expresses concern about the risks of clinging to outdated beliefs and viewpoints in a rapidly changing world, specifically in the context of AI development.

**Justification:**

He mentions the 'risk of calcification of worldview' when individuals publicly articulate a position and cling to it despite evidence against it. This suggests that rigidity in one's beliefs can lead to a disconnect from reality.

--------

## Chunk 698

**Chunk:**

Dwarkesh Patel
What are we doing with that? It’s very hard to predict in advance. It’ll be very interesting to figure out what the Jupiter brains will be doing. 

So there’s situational awareness of where things stand now, and we’ve gotten a good dose of that. A lot of what we’re talking about now couldn’t have been predicted many years back. Part of your worldview implies that things will accelerate because of AI.

Many unpredictable factors will become evident over time, like how people, the political system, and foreign adversaries will react. Situational awareness isn’t just knowing where things stand now, but being in a position to react appropriately to new information and to change your worldview and recommendations accordingly.

What is the appropriate way to think about situational awareness as a continuous process rather than as a one-time realization?
Leopold Aschenbrenner
This is great. There’s a sort of mental flexibility and willingness to change your mind that’s really important. This is how a lot of brains have been broken in the AGI debate. The doomers were prescient about AGI a decade ago, but they haven’t updated on the empirical realities of deep learning. Their proposals are naive and unworkable. It doesn’t really make sense.

Some people come in with a predefined ideology, like e/accs. They like to shitpost about technology but they’re not actually thinking it through. You have stagnationists who think this stuff is just chatbots and not risky or those not considering the immense national security implications.

There’s a risk of calcification of worldview when you publicly articulate a position and cling to it despite evidence against it. So I want to give a big disclaimer. It’s valuable to paint a concrete and visceral picture. This is currently my best guess on how this decade will go. If it goes anything like this, it’ll be wild. Given the rapid pace of progress, we’re going to keep getting a lot more information and it’s important to keep your head on straight.

I feel like the most important thing here is that. This relates to some of the stuff we talked about the world being surprisingly small. I used to think important things were being handled by capable people in government and AI labs.

From personal experience, and seeing how Covid was managed, I realized that not everyone is on it. There’s not somebody else who’s on it and making sure this goes well. What really matters is that good people take these issues as seriously as they deserve, have situational awareness, are willing to change their minds, and face reality head-on. I’m counting on those good people.
Dwarkesh Patel
All right, that’s a great place to close.

**Extracted Belief:**

The pace of technological progress, particularly in the field of AI, is accelerating and will continue to produce new information and insights.

**Context:**

Leopold Aschenbrenner is expressing his perspective on the rapid pace of AI development and the implications for understanding its impact.

**Justification:**

He states that 'we’re going to keep getting a lot more information' due to the rapid pace of progress, highlighting the dynamic nature of AI and the need for ongoing learning and adaptation.

--------

## Chunk 699

**Chunk:**

Dwarkesh Patel
What are we doing with that? It’s very hard to predict in advance. It’ll be very interesting to figure out what the Jupiter brains will be doing. 

So there’s situational awareness of where things stand now, and we’ve gotten a good dose of that. A lot of what we’re talking about now couldn’t have been predicted many years back. Part of your worldview implies that things will accelerate because of AI.

Many unpredictable factors will become evident over time, like how people, the political system, and foreign adversaries will react. Situational awareness isn’t just knowing where things stand now, but being in a position to react appropriately to new information and to change your worldview and recommendations accordingly.

What is the appropriate way to think about situational awareness as a continuous process rather than as a one-time realization?
Leopold Aschenbrenner
This is great. There’s a sort of mental flexibility and willingness to change your mind that’s really important. This is how a lot of brains have been broken in the AGI debate. The doomers were prescient about AGI a decade ago, but they haven’t updated on the empirical realities of deep learning. Their proposals are naive and unworkable. It doesn’t really make sense.

Some people come in with a predefined ideology, like e/accs. They like to shitpost about technology but they’re not actually thinking it through. You have stagnationists who think this stuff is just chatbots and not risky or those not considering the immense national security implications.

There’s a risk of calcification of worldview when you publicly articulate a position and cling to it despite evidence against it. So I want to give a big disclaimer. It’s valuable to paint a concrete and visceral picture. This is currently my best guess on how this decade will go. If it goes anything like this, it’ll be wild. Given the rapid pace of progress, we’re going to keep getting a lot more information and it’s important to keep your head on straight.

I feel like the most important thing here is that. This relates to some of the stuff we talked about the world being surprisingly small. I used to think important things were being handled by capable people in government and AI labs.

From personal experience, and seeing how Covid was managed, I realized that not everyone is on it. There’s not somebody else who’s on it and making sure this goes well. What really matters is that good people take these issues as seriously as they deserve, have situational awareness, are willing to change their minds, and face reality head-on. I’m counting on those good people.
Dwarkesh Patel
All right, that’s a great place to close.

**Extracted Belief:**

Important issues like AI development are not always effectively addressed by those in positions of authority, such as government officials and AI lab researchers.

**Context:**

Leopold Aschenbrenner reflects on his experiences and observations, specifically citing the management of the COVID-19 pandemic as evidence of this belief.

**Justification:**

He states that 'not everyone is on it,' referring to the lack of effective leadership and problem-solving in crucial areas. He further suggests that 'there’s not somebody else who’s on it and making sure this goes well,' implying a broader lack of competence and commitment in addressing critical issues.

--------

## Chunk 700

**Chunk:**

Dwarkesh Patel
What are we doing with that? It’s very hard to predict in advance. It’ll be very interesting to figure out what the Jupiter brains will be doing. 

So there’s situational awareness of where things stand now, and we’ve gotten a good dose of that. A lot of what we’re talking about now couldn’t have been predicted many years back. Part of your worldview implies that things will accelerate because of AI.

Many unpredictable factors will become evident over time, like how people, the political system, and foreign adversaries will react. Situational awareness isn’t just knowing where things stand now, but being in a position to react appropriately to new information and to change your worldview and recommendations accordingly.

What is the appropriate way to think about situational awareness as a continuous process rather than as a one-time realization?
Leopold Aschenbrenner
This is great. There’s a sort of mental flexibility and willingness to change your mind that’s really important. This is how a lot of brains have been broken in the AGI debate. The doomers were prescient about AGI a decade ago, but they haven’t updated on the empirical realities of deep learning. Their proposals are naive and unworkable. It doesn’t really make sense.

Some people come in with a predefined ideology, like e/accs. They like to shitpost about technology but they’re not actually thinking it through. You have stagnationists who think this stuff is just chatbots and not risky or those not considering the immense national security implications.

There’s a risk of calcification of worldview when you publicly articulate a position and cling to it despite evidence against it. So I want to give a big disclaimer. It’s valuable to paint a concrete and visceral picture. This is currently my best guess on how this decade will go. If it goes anything like this, it’ll be wild. Given the rapid pace of progress, we’re going to keep getting a lot more information and it’s important to keep your head on straight.

I feel like the most important thing here is that. This relates to some of the stuff we talked about the world being surprisingly small. I used to think important things were being handled by capable people in government and AI labs.

From personal experience, and seeing how Covid was managed, I realized that not everyone is on it. There’s not somebody else who’s on it and making sure this goes well. What really matters is that good people take these issues as seriously as they deserve, have situational awareness, are willing to change their minds, and face reality head-on. I’m counting on those good people.
Dwarkesh Patel
All right, that’s a great place to close.

**Extracted Belief:**

Individuals who are willing to engage with complex issues like AI, possess situational awareness, embrace critical thinking, and are open to changing their perspectives are crucial to navigating the challenges of technological advancement.

**Context:**

Leopold Aschenbrenner concludes his argument by emphasizing the importance of responsible and proactive individuals in shaping a positive future for AI.

**Justification:**

He believes that 'good people' who take issues seriously, possess situational awareness, are willing to change their minds, and face reality head-on are essential to navigating the challenges posed by AI. This suggests that he values critical thinking, adaptability, and a commitment to truth-seeking in the face of complex technological developments.

--------

## Chunk 701

**Chunk:**

Dwarkesh Patel
All right, that’s a great place to close.
Leopold Aschenbrenner
Thanks so much Dwarkesh. Absolute joy.
Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great

**Extracted Belief:**

nan

**Context:**

nan

**Justification:**

nan

--------

## Chunk 702

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great was a gay lover of the arts.

**Context:**

Leopold Aschenbrenner describes Frederick the Great's personal life, including his artistic interests and sexual orientation.

**Justification:**

Aschenbrenner states that Frederick the Great was 'kind of a gay lover of the arts' and provides details about his artistic pursuits and his preference for French over German.

--------

## Chunk 703

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great hated speaking German and only wanted to speak French.

**Context:**

Leopold Aschenbrenner provides insight into Frederick the Great's linguistic preferences.

**Justification:**

Aschenbrenner states that Frederick the Great 'hated speaking German' and 'only wanted to speak French'.

--------

## Chunk 704

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great played the flute and composed music.

**Context:**

Leopold Aschenbrenner details Frederick the Great's musical abilities.

**Justification:**

Aschenbrenner states that Frederick the Great 'played the flute, and composed'.

--------

## Chunk 705

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great had a difficult upbringing with a stern Prussian military father.

**Context:**

Leopold Aschenbrenner discusses the upbringing of Frederick the Great.

**Justification:**

Aschenbrenner describes Frederick the Great's father as 'a very stern Prussian military father'.

--------

## Chunk 706

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great had a male lover when he was about 17 years old.

**Context:**

Leopold Aschenbrenner recounts a personal event in Frederick the Great's life.

**Justification:**

Aschenbrenner states that 'when Frederick was about 17, he had a male lover'.

--------

## Chunk 707

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great's father imprisoned him and hanged his lover in front of him.

**Context:**

Leopold Aschenbrenner describes a traumatic event in Frederick the Great's life.

**Justification:**

Aschenbrenner states that Frederick the Great's father 'imprisoned his son and hanged his lover in front of him'.

--------

## Chunk 708

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great became one of the most successful Prussian conquerors despite his difficult upbringing.

**Context:**

Leopold Aschenbrenner highlights Frederick the Great's achievements in spite of his past.

**Justification:**

Aschenbrenner states that 'Despite this, Frederick the Great became one of the most successful Prussian conquerors'.

--------

## Chunk 709

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great won Silesia and the Seven Years' War.

**Context:**

Leopold Aschenbrenner describes Frederick the Great's military victories.

**Justification:**

Aschenbrenner states that Frederick the Great 'got Silesia, won the Seven Years' War'.

--------

## Chunk 710

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great was an amazing military strategist.

**Context:**

Leopold Aschenbrenner praises Frederick the Great's military prowess.

**Justification:**

Aschenbrenner states that Frederick the Great 'was an amazing military strategist'.

--------

## Chunk 711

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great's military brilliance lay in flanking the army, which was revolutionary at the time.

**Context:**

Leopold Aschenbrenner explains a key element of Frederick the Great's military strategy.

**Justification:**

Aschenbrenner states that 'His brilliance lay in flanking the army, which was revolutionary at the time'.

--------

## Chunk 712

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great almost lost the Seven Years' War but was able to regroup and succeed due to a change in the Russian Tsar's stance.

**Context:**

Leopold Aschenbrenner describes a turning point in the Seven Years' War.

**Justification:**

Aschenbrenner states that 'They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed'.

--------

## Chunk 713

**Chunk:**

Dwarkesh Patel
This was excellent.

(04:29:08) – Coda: Frederick the Great
Leopold Aschenbrenner
The funny thing is a lot of this German history stuff we’ve talked about isn’t actually what I learned in Germany. It’s stuff I learned after. I would go back to Germany over Christmas or whatever and suddenly understand the street names—Gneisenau, Scharnhorst, all these Prussian military reformers. And you finally understand Sanssouci, and you realize it was for Frederick.

Frederick the Great is a really interesting figure. He was kind of a gay lover of the arts. He hated speaking German, only wanted to speak French. He played the flute, and composed. He had all the great artists of his day over at Sanssouci. He had a really tough upbringing with a very stern Prussian military father. When Frederick was about 17, he had a male lover. His father imprisoned his son and hanged his lover in front of him.

Despite this, Frederick the Great became one of the most successful Prussian conquerors. He got Silesia, won the Seven Years' War, and was an amazing military strategist. His brilliance lay in flanking the army, which was revolutionary at the time. They almost lost the Seven Years' War, but the Russian tsar changed and turned out to be a Prussia stan, which allowed Frederick to regroup and succeed. He’s a bizarre but interesting figure in German history.

**Extracted Belief:**

Frederick the Great is a bizarre but interesting figure in German history.

**Context:**

Leopold Aschenbrenner summarizes his assessment of Frederick the Great.

**Justification:**

Aschenbrenner concludes by saying that Frederick the Great 'is a bizarre but interesting figure in German history'.

--------

